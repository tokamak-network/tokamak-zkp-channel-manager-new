Directory structure:
â””â”€â”€ tokamak-network-tokamak-zk-evm/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ dockerfile
    â”œâ”€â”€ entitlements.plist
    â”œâ”€â”€ learna.json
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ LICENSE-APACHE
    â”œâ”€â”€ LICENSE-MIT
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ package.json
    â”œâ”€â”€ tokamak-cli
    â”œâ”€â”€ vercel.json
    â”œâ”€â”€ .dockerignore
    â”œâ”€â”€ .editorconfig
    â”œâ”€â”€ .prettierignore
    â”œâ”€â”€ .prettierrc
    â”œâ”€â”€ config/
    â”‚   â”œâ”€â”€ monorepo-js/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ cspell-md.json
    â”‚   â”‚   â”œâ”€â”€ cspell-ts.json
    â”‚   â”‚   â”œâ”€â”€ E2E_TESTING.md
    â”‚   â”‚   â”œâ”€â”€ eslint.cjs
    â”‚   â”‚   â”œâ”€â”€ MONOREPO.md
    â”‚   â”‚   â”œâ”€â”€ prettier.config.js
    â”‚   â”‚   â”œâ”€â”€ tsconfig.json
    â”‚   â”‚   â”œâ”€â”€ tsconfig.lint.json
    â”‚   â”‚   â”œâ”€â”€ tsconfig.prod.cjs.json
    â”‚   â”‚   â”œâ”€â”€ tsconfig.prod.esm.json
    â”‚   â”‚   â”œâ”€â”€ typedoc.cjs
    â”‚   â”‚   â”œâ”€â”€ vitest.config.browser.mts
    â”‚   â”‚   â”œâ”€â”€ .c8rc.json
    â”‚   â”‚   â””â”€â”€ cli/
    â”‚   â”‚       â”œâ”€â”€ clean-package.sh
    â”‚   â”‚       â”œâ”€â”€ clean-root.sh
    â”‚   â”‚       â”œâ”€â”€ coverage.sh
    â”‚   â”‚       â”œâ”€â”€ lint-diff.sh
    â”‚   â”‚       â”œâ”€â”€ lint-fix.sh
    â”‚   â”‚       â”œâ”€â”€ lint.sh
    â”‚   â”‚       â”œâ”€â”€ prepublish.sh
    â”‚   â”‚       â”œâ”€â”€ ts-build.sh
    â”‚   â”‚       â””â”€â”€ ts-compile.sh
    â”‚   â””â”€â”€ typescript/
    â”‚       â”œâ”€â”€ eslint-config-base.js
    â”‚       â””â”€â”€ tsconfig.base.json
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ Airdrop2025.md
    â”‚   â””â”€â”€ TEAM_FORMATTING_RULES.md
    â”œâ”€â”€ packages/
    â”‚   â”œâ”€â”€ .dockerignore
    â”‚   â”œâ”€â”€ backend/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”‚   â”œâ”€â”€ download-ICICLE-lib.sh
    â”‚   â”‚   â”œâ”€â”€ libs/
    â”‚   â”‚   â”‚   â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”‚   â”œâ”€â”€ benches/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ benchmarks.rs
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ matrix_matrix_mul_bench.rs
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ outer_product_bench.rs
    â”‚   â”‚   â”‚   â””â”€â”€ src/
    â”‚   â”‚   â”‚       â”œâ”€â”€ lib.rs
    â”‚   â”‚   â”‚       â”œâ”€â”€ tests.rs
    â”‚   â”‚   â”‚       â”œâ”€â”€ bivariate_polynomial/
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ mod.rs
    â”‚   â”‚   â”‚       â”œâ”€â”€ field_structures/
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ mod.rs
    â”‚   â”‚   â”‚       â”œâ”€â”€ group_structures/
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ mod.rs
    â”‚   â”‚   â”‚       â”œâ”€â”€ iotools/
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ mod.rs
    â”‚   â”‚   â”‚       â”œâ”€â”€ polynomial_structures/
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ mod.rs
    â”‚   â”‚   â”‚       â”œâ”€â”€ utils/
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ mod.rs
    â”‚   â”‚   â”‚       â””â”€â”€ vector_operations/
    â”‚   â”‚   â”‚           â””â”€â”€ mod.rs
    â”‚   â”‚   â”œâ”€â”€ prove/
    â”‚   â”‚   â”‚   â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”‚   â””â”€â”€ src/
    â”‚   â”‚   â”‚       â”œâ”€â”€ lib.rs
    â”‚   â”‚   â”‚       â””â”€â”€ main.rs
    â”‚   â”‚   â”œâ”€â”€ setup/
    â”‚   â”‚   â”‚   â”œâ”€â”€ mpc-setup/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Data_formats.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile.amd64
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile.arm64
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ README_mpc.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ src/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ accumulator.rs
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ contributor.rs
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ conversions.rs
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ lib.rs
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ mpc_utils.rs
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ sigma.rs
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ utils.rs
    â”‚   â”‚   â”‚   â””â”€â”€ trusted-setup/
    â”‚   â”‚   â”‚       â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”‚       â””â”€â”€ src/
    â”‚   â”‚   â”‚           â”œâ”€â”€ lib.rs
    â”‚   â”‚   â”‚           â””â”€â”€ main.rs
    â”‚   â”‚   â”œâ”€â”€ verify/
    â”‚   â”‚   â”‚   â”œâ”€â”€ preprocess/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ src/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ lib.rs
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ main.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ solidity/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ foundry.toml
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ broadcast/
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Deploy.s.sol/
    â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 11155111/
    â”‚   â”‚   â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ run-1748339671.json
    â”‚   â”‚   â”‚   â”‚   â”‚   â”‚       â””â”€â”€ run-latest.json
    â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DeployScript.sol/
    â”‚   â”‚   â”‚   â”‚   â”‚       â””â”€â”€ 11155111/
    â”‚   â”‚   â”‚   â”‚   â”‚           â”œâ”€â”€ run-1748859233.json
    â”‚   â”‚   â”‚   â”‚   â”‚           â””â”€â”€ run-latest.json
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cache/
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ solidity-files-cache.json
    â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ test-failures
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ src/
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ VerifierLatest.sol
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ VerifierV1.sol
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ VerifierV2.sol
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ VerifierV3.sol
    â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ interface/
    â”‚   â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ IVerifierV1.sol
    â”‚   â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ IVerifierV2.sol
    â”‚   â”‚   â”‚   â”‚   â”‚       â””â”€â”€ IVerifierV3.sol
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ test/
    â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ verifierV1.t.sol
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ .github/
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ workflows/
    â”‚   â”‚   â”‚   â”‚           â””â”€â”€ test.yml
    â”‚   â”‚   â”‚   â”œâ”€â”€ verify-rust/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ src/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ lib.rs
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ main.rs
    â”‚   â”‚   â”‚   â””â”€â”€ verify-wasm/
    â”‚   â”‚   â”‚       â”œâ”€â”€ README.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ build-and-test.sh
    â”‚   â”‚   â”‚       â”œâ”€â”€ build.sh
    â”‚   â”‚   â”‚       â”œâ”€â”€ Cargo.toml
    â”‚   â”‚   â”‚       â”œâ”€â”€ example-browser.html
    â”‚   â”‚   â”‚       â”œâ”€â”€ example-complete-verify.html
    â”‚   â”‚   â”‚       â”œâ”€â”€ example-node.js
    â”‚   â”‚   â”‚       â”œâ”€â”€ example-real-files.html
    â”‚   â”‚   â”‚       â”œâ”€â”€ example-simple.html
    â”‚   â”‚   â”‚       â”œâ”€â”€ NPM_USAGE.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ package.json
    â”‚   â”‚   â”‚       â”œâ”€â”€ publish.sh
    â”‚   â”‚   â”‚       â”œâ”€â”€ QUICK_START.md
    â”‚   â”‚   â”‚       â”œâ”€â”€ test-node.cjs
    â”‚   â”‚   â”‚       â”œâ”€â”€ data/
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ instance.json
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ preprocess.json
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ proof.json
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ setupParams.json
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ sigma_verify.json
    â”‚   â”‚   â”‚       â”œâ”€â”€ src/
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ bindings.ts
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ lib.rs
    â”‚   â”‚   â”‚       â””â”€â”€ .cargo/
    â”‚   â”‚   â”‚           â””â”€â”€ config.toml
    â”‚   â”‚   â””â”€â”€ .cargo/
    â”‚   â”‚       â””â”€â”€ config.toml
    â”‚   â””â”€â”€ frontend/
    â”‚       â”œâ”€â”€ qap-compiler/
    â”‚       â”‚   â”œâ”€â”€ README.md
    â”‚       â”‚   â”œâ”€â”€ LICENSE-APACHE
    â”‚       â”‚   â”œâ”€â”€ LICENSE-MIT
    â”‚       â”‚   â”œâ”€â”€ package.json
    â”‚       â”‚   â”œâ”€â”€ tsconfig.json
    â”‚       â”‚   â”œâ”€â”€ benchmark/
    â”‚       â”‚   â”‚   â”œâ”€â”€ fixed_compile.txt
    â”‚       â”‚   â”‚   â””â”€â”€ old_compile.txt
    â”‚       â”‚   â”œâ”€â”€ functions/
    â”‚       â”‚   â”‚   â”œâ”€â”€ arithmetic.circom
    â”‚       â”‚   â”‚   â”œâ”€â”€ poseidon.circom
    â”‚       â”‚   â”‚   â””â”€â”€ two_complement.circom
    â”‚       â”‚   â”œâ”€â”€ scripts/
    â”‚       â”‚   â”‚   â”œâ”€â”€ compile.sh
    â”‚       â”‚   â”‚   â”œâ”€â”€ compile_test.sh
    â”‚       â”‚   â”‚   â”œâ”€â”€ configure.js
    â”‚       â”‚   â”‚   â”œâ”€â”€ parse.js
    â”‚       â”‚   â”‚   â”œâ”€â”€ test_jubjub.ts
    â”‚       â”‚   â”‚   â””â”€â”€ exporter/
    â”‚       â”‚   â”‚       â”œâ”€â”€ exporter.ts
    â”‚       â”‚   â”‚       â””â”€â”€ types.ts
    â”‚       â”‚   â”œâ”€â”€ subcircuits/
    â”‚       â”‚   â”‚   â”œâ”€â”€ circom/
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Accumulator_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ALU1_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ALU2_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ALU3_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ALU4_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ALU5_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ AND_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ bufferBlockIn_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ bufferEVMIn_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ bufferPrvIn_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ bufferPubIn_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ bufferPubOut_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ constants.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ DecToBit_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ EdDsaVerify_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ JubjubExpBatch_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ OR_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Poseidon2xCompress_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Poseidon_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ PrepareEdDsaScalars_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ SubExpBatch_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ VerifyMerkleProof2x_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ VerifyMerkleProof3x_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ VerifyMerkleProof_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ XOR_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ 2048/
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ALU_based_on_div_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ALU_basic_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ALU_bitwise_circuit.circom
    â”‚       â”‚   â”‚   â”‚   â””â”€â”€ unused/
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ add_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ addmod1_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ addmod2_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ byte_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ div_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ eq_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ gt_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ iszero_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ lt_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ mod_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ mul_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ mulmod1_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ mulmod2_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ not_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ sar_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ sdiv_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ sgt_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ ShiftMask_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ shl_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ shr_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ signextend_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ slt_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ smod_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ sub_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ SubEXP_circuit.circom
    â”‚       â”‚   â”‚   â”‚       â””â”€â”€ test_range_check_circuit.circom
    â”‚       â”‚   â”‚   â”œâ”€â”€ library/
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ frontendCfg.json
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ generate_witness.js
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ globalWireList.json
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ setupParams.json
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuitInfo.json
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ witness_calculator.js
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ info/
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit0_bufferPubOut_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit10_OR_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit11_XOR_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit12_AND_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit13_DecToBit_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit14_SubExpBatch_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit15_Accumulator_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit16_Poseidon_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit17_Poseidon2xCompress_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit18_JubjubExpBatch_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit19_EdDsaVerify_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit1_bufferPubIn_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit20_VerifyMerkleProof_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit21_VerifyMerkleProof2x_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit22_VerifyMerkleProof3x_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit2_bufferBlockIn_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit3_bufferEVMIn_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit4_bufferPrvIn_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit5_ALU1_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit6_ALU2_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit7_ALU3_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit8_ALU4_info.txt
    â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â”€ subcircuit9_ALU5_info.txt
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ json/
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit0.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit1.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit10.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit11.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit12.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit13.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit14.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit15.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit16.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit17.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit18.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit19.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit2.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit20.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit21.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit22.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit3.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit4.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit5.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit6.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit7.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit8.json
    â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â”€ subcircuit9.json
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ r1cs/
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit0.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit1.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit10.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit11.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit12.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit13.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit14.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit15.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit16.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit17.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit18.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit19.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit2.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit20.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit21.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit22.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit3.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit4.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit5.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit6.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit7.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ subcircuit8.r1cs
    â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â”€ subcircuit9.r1cs
    â”‚       â”‚   â”‚   â”‚   â””â”€â”€ wasm/
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit0.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit1.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit10.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit11.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit12.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit13.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit14.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit15.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit16.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit17.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit18.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit19.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit2.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit20.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit21.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit22.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit3.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit4.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit5.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit6.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit7.wasm
    â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ subcircuit8.wasm
    â”‚       â”‚   â”‚   â”‚       â””â”€â”€ subcircuit9.wasm
    â”‚       â”‚   â”‚   â””â”€â”€ test/
    â”‚       â”‚   â”‚       â”œâ”€â”€ helper_functions.js
    â”‚       â”‚   â”‚       â”œâ”€â”€ test_cases.js
    â”‚       â”‚   â”‚       â”œâ”€â”€ test_script_1024.js
    â”‚       â”‚   â”‚       â”œâ”€â”€ test_script_2048.js
    â”‚       â”‚   â”‚       â””â”€â”€ wasm/
    â”‚       â”‚   â”‚           â”œâ”€â”€ Accumulator_circuit.wasm
    â”‚       â”‚   â”‚           â”œâ”€â”€ ALU_based_on_div_circuit.wasm
    â”‚       â”‚   â”‚           â”œâ”€â”€ ALU_basic_circuit.wasm
    â”‚       â”‚   â”‚           â”œâ”€â”€ ALU_bitwise_circuit.wasm
    â”‚       â”‚   â”‚           â”œâ”€â”€ DecToBit_circuit.wasm
    â”‚       â”‚   â”‚           â”œâ”€â”€ generate_witness.js
    â”‚       â”‚   â”‚           â””â”€â”€ witness_calculator.js
    â”‚       â”‚   â””â”€â”€ templates/
    â”‚       â”‚       â”œâ”€â”€ buffer.circom
    â”‚       â”‚       â”œâ”€â”€ 128bit/
    â”‚       â”‚       â”‚   â”œâ”€â”€ arithmetic.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ bitwise.circom
    â”‚       â”‚       â”‚   â””â”€â”€ mux.circom
    â”‚       â”‚       â”œâ”€â”€ 255bit/
    â”‚       â”‚       â”‚   â”œâ”€â”€ jubjub.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ merkleTree.circom
    â”‚       â”‚       â”‚   â””â”€â”€ poseidon.circom
    â”‚       â”‚       â”œâ”€â”€ 256bit/
    â”‚       â”‚       â”‚   â”œâ”€â”€ alu_safe.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ arithmetic_safe.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ arithmetic_unsafe_type1.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ arithmetic_unsafe_type2.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ bitify_safe.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ bitwise_safe.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ compare_safe.circom
    â”‚       â”‚       â”‚   â”œâ”€â”€ mux.circom
    â”‚       â”‚       â”‚   â””â”€â”€ two_complement_unsafe.circom
    â”‚       â”‚       â”œâ”€â”€ 512bit/
    â”‚       â”‚       â”‚   â”œâ”€â”€ arithmetic.circom
    â”‚       â”‚       â”‚   â””â”€â”€ mux.circom
    â”‚       â”‚       â””â”€â”€ unused/
    â”‚       â”‚           â””â”€â”€ safe_comparator.circom
    â”‚       â””â”€â”€ synthesizer/
    â”‚           â”œâ”€â”€ README.md
    â”‚           â”œâ”€â”€ BINARY_USAGE.md
    â”‚           â”œâ”€â”€ build-binary.sh
    â”‚           â”œâ”€â”€ LICENSE-APACHE
    â”‚           â”œâ”€â”€ LICENSE-MIT
    â”‚           â”œâ”€â”€ NOTICE
    â”‚           â”œâ”€â”€ package.json
    â”‚           â”œâ”€â”€ tsconfig.json
    â”‚           â”œâ”€â”€ tsconfig.lint.json
    â”‚           â”œâ”€â”€ tsconfig.prod.cjs.json
    â”‚           â”œâ”€â”€ tsconfig.prod.esm.json
    â”‚           â”œâ”€â”€ tsconfig.tsbuildinfo
    â”‚           â”œâ”€â”€ .eslintignore
    â”‚           â”œâ”€â”€ .eslintrc.cjs
    â”‚           â”œâ”€â”€ .prettierignore
    â”‚           â”œâ”€â”€ docs/
    â”‚           â”‚   â”œâ”€â”€ synthesizer-architecture.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-class-structure.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-code-examples.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-data-structure.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-execution-flow.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-introduction.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-opcodes.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-output-files.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-repository-structure.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-terminology.md
    â”‚           â”‚   â”œâ”€â”€ synthesizer-transaction-flow.md
    â”‚           â”‚   â””â”€â”€ synthesizer.md
    â”‚           â”œâ”€â”€ examples/
    â”‚           â”‚   â”œâ”€â”€ L2StateChannel/
    â”‚           â”‚   â”‚   â”œâ”€â”€ block_info.json
    â”‚           â”‚   â”‚   â”œâ”€â”€ contract_code.json
    â”‚           â”‚   â”‚   â””â”€â”€ previous_state_snapshot.json
    â”‚           â”‚   â””â”€â”€ L2TONTransfer/
    â”‚           â”‚       â”œâ”€â”€ input.json
    â”‚           â”‚       â”œâ”€â”€ input2.json
    â”‚           â”‚       â”œâ”€â”€ input3.json
    â”‚           â”‚       â”œâ”€â”€ input4.json
    â”‚           â”‚       â””â”€â”€ main.ts
    â”‚           â””â”€â”€ src/
    â”‚               â”œâ”€â”€ circuitGenerator/
    â”‚               â”‚   â”œâ”€â”€ circuitGenerator.ts
    â”‚               â”‚   â”œâ”€â”€ handlers/
    â”‚               â”‚   â”‚   â”œâ”€â”€ permutationGenerator.ts
    â”‚               â”‚   â”‚   â””â”€â”€ variableGenerator.ts
    â”‚               â”‚   â”œâ”€â”€ types/
    â”‚               â”‚   â”‚   â””â”€â”€ types.ts
    â”‚               â”‚   â””â”€â”€ utils/
    â”‚               â”‚       â””â”€â”€ witness_calculator.ts
    â”‚               â”œâ”€â”€ interface/
    â”‚               â”‚   â”œâ”€â”€ index.ts
    â”‚               â”‚   â”œâ”€â”€ cli/
    â”‚               â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚               â”‚   â”‚   â””â”€â”€ utils/
    â”‚               â”‚   â”‚       â”œâ”€â”€ node.ts
    â”‚               â”‚   â”‚       â””â”€â”€ resultAnalyzer.ts
    â”‚               â”‚   â”œâ”€â”€ debugging/
    â”‚               â”‚   â”‚   â””â”€â”€ utils.ts
    â”‚               â”‚   â”œâ”€â”€ node/
    â”‚               â”‚   â”‚   â”œâ”€â”€ jsonWriter.ts
    â”‚               â”‚   â”‚   â””â”€â”€ wasmLoader.ts
    â”‚               â”‚   â”œâ”€â”€ qapCompiler/
    â”‚               â”‚   â”‚   â”œâ”€â”€ configuredTypes.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ importedConstants.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚               â”‚   â”‚   â””â”€â”€ utils.ts
    â”‚               â”‚   â””â”€â”€ rpc/
    â”‚               â”‚       â””â”€â”€ rpc.ts
    â”‚               â”œâ”€â”€ synthesizer/
    â”‚               â”‚   â”œâ”€â”€ constructors.ts
    â”‚               â”‚   â”œâ”€â”€ index.ts
    â”‚               â”‚   â”œâ”€â”€ synthesizer.ts
    â”‚               â”‚   â”œâ”€â”€ dataStructure/
    â”‚               â”‚   â”‚   â”œâ”€â”€ arithmeticOperations.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ dataPt.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ memoryPt.ts
    â”‚               â”‚   â”‚   â””â”€â”€ stackPt.ts
    â”‚               â”‚   â”œâ”€â”€ handlers/
    â”‚               â”‚   â”‚   â”œâ”€â”€ arithmeticManager.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ bufferManager.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ instructionHandler.ts
    â”‚               â”‚   â”‚   â”œâ”€â”€ memoryManager.ts
    â”‚               â”‚   â”‚   â””â”€â”€ stateManager.ts
    â”‚               â”‚   â”œâ”€â”€ params/
    â”‚               â”‚   â”‚   â””â”€â”€ index.ts
    â”‚               â”‚   â””â”€â”€ types/
    â”‚               â”‚       â”œâ”€â”€ buffers.ts
    â”‚               â”‚       â”œâ”€â”€ dataStructure.ts
    â”‚               â”‚       â”œâ”€â”€ index.ts
    â”‚               â”‚       â”œâ”€â”€ instructions.ts
    â”‚               â”‚       â”œâ”€â”€ placements.ts
    â”‚               â”‚       â””â”€â”€ synthesizer.ts
    â”‚               â””â”€â”€ unused/
    â”‚                   â”œâ”€â”€ index.ts
    â”‚                   â”œâ”€â”€ REFACTORING.md
    â”‚                   â”œâ”€â”€ unused/
    â”‚                   â”‚   â”œâ”€â”€ functions.ts
    â”‚                   â”‚   â”œâ”€â”€ index.ts
    â”‚                   â”‚   â”œâ”€â”€ provider.ts
    â”‚                   â”‚   â”œâ”€â”€ unused.ts
    â”‚                   â”‚   â””â”€â”€ utils.ts
    â”‚                   â”œâ”€â”€ utils/
    â”‚                   â”‚   â””â”€â”€ functions.ts
    â”‚                   â””â”€â”€ validation/
    â”‚                       â”œâ”€â”€ errors.ts
    â”‚                       â”œâ”€â”€ index.ts
    â”‚                       â””â”€â”€ validator.ts
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ channel-functions.sh
    â”‚   â”œâ”€â”€ interface.sh
    â”‚   â”œâ”€â”€ packaging.sh
    â”‚   â”œâ”€â”€ setup-macos.sh
    â”‚   â””â”€â”€ tokamak-cli-core
    â”œâ”€â”€ synthesizer-input-template/
    â”‚   â””â”€â”€ input.json
    â”œâ”€â”€ .github/
    â”‚   â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md
    â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/
    â”‚   â”‚   â”œâ”€â”€ bug_report.md
    â”‚   â”‚   â””â”€â”€ feature_request.md
    â”‚   â””â”€â”€ workflows/
    â”‚       â”œâ”€â”€ build-release.yml
    â”‚       â””â”€â”€ notification.yml
    â””â”€â”€ .run_scripts/
        â”œâ”€â”€ linux/
        â”‚   â”œâ”€â”€ 1_run-trusted-setup.sh
        â”‚   â”œâ”€â”€ 2_run-synthesizer.sh
        â”‚   â”œâ”€â”€ 3_run-preprocess.sh
        â”‚   â”œâ”€â”€ 4_run-prove.sh
        â”‚   â””â”€â”€ 5_run-verify.sh
        â””â”€â”€ macOS/
            â”œâ”€â”€ 1_run-trusted-setup.sh
            â”œâ”€â”€ 2_run-synthesizer.sh
            â”œâ”€â”€ 3_run-preprocess.sh
            â”œâ”€â”€ 4_run-prove.sh
            â””â”€â”€ 5_run-verify.sh


Files Content:

(Files content cropped to 300k characters, download full ingest to see more)
================================================
FILE: README.md
================================================
# Tokamak-zk-EVM

Tokamak zk-EVM is a tool that converts [TokamakL2JS](https://github.com/tokamak-network/TokamakL2JS) transactions into ZKPs.

[TokamakL2JS](https://github.com/tokamak-network/TokamakL2JS), which is a variant of [EthereumJS](https://github.com/ethereumjs/ethereumjs-monorepo), specifies a layer 2 protocol of Tokamak Network.

If you are interested in converting Ethereum transactions to ZKP, check out branch "[archive-airdrop-Sep25](https://github.com/tokamak-network/Tokamak-zk-EVM/tree/archive-airdrop-Sep25)" (incomplete development).

# Getting started

This section describes how to use the **main CLI** named **`tokamak-cli`**.

## Prerequisites
### Alchemy API key
1. Create an Alchemy account and log in to the dashboard (https://dashboard.alchemy.com/).
2. Create a new app/project for **Ethereum Mainnet**.
3. Copy the **API Key** (the short token).  
   You will pass this key to the CLI as `--install <API_KEY>`.
> Note: You can paste the full RPC URL obtained from any provider other than Alchemy.

### For Windows users
1. Install Docker Desktop for Windows â€“ https://docs.docker.com/desktop/install/windows-install/
2. (If you want to use CUDA/GPU) Install **NVIDIA GPU driver** on Windows and verify Docker GPU pass-through.
   - Install [the latest NVIDIA driver](https://developer.nvidia.com/cuda/wsl).
   - Ensure Docker Desktop is using **Linux containers** with the **WSL 2** backend.
   - (Optional) Test that CUDA is visible inside containers (at the host):
     ```
     nvidia-smi

     docker run --rm --gpus all nvidia/cuda:12.2.0-runtime-ubuntu22.04 nvidia-smi
     ```
3. Run Docker
    - Make sure that you are in the root directory, `Tokamak-zk-EVM`.
        ```bash
        docker build -f dockerfile -t tokamak-zkevm:win .

        # If you will use CUDA/GPU
        docker run --gpus all --rm -it -v "$(cygpath -m "$PWD"):/workspace" tokamak-zkevm:win bash 
        # Else
        docker run --rm -it -v "$(cygpath -m "$PWD"):/workspace" tokamak-zkevm:win bash 
        ```

### For macOS users

**Option 1: Automatic Setup (Recommended)**

Run the setup script to automatically check and install all prerequisites:
```bash
./scripts/setup-macos.sh
```
This script will detect the following dependencies and install any missing automatically.

**Option 2: Manual Installation**
- Install Node.js â€“ https://nodejs.org/
- Install Circom â€“ https://docs.circom.io/getting-started/installation/
- Install Rust â€“ https://www.rust-lang.org/tools/install
- Install CMake â€“ https://cmake.org/download/
- Install Bun â€“ https://bun.sh/ (required for Synthesizer binary build)
- Install dos2unix
    ```zsh 
    brew install dos2unix
    ```

### For Linux users
- Install Node.js â€“ https://nodejs.org/
- Install Circom â€“ https://docs.circom.io/getting-started/installation/
- Install Rust â€“ https://www.rust-lang.org/tools/install
- Install CMake â€“ https://cmake.org/download/
- Install dos2unix
  - For example, Ubuntu/Debian:
    ```bash
    sudo apt-get update && sudo apt-get install -y dos2unix
    ```
- If you want to use CUDA for GPU acceleration:
  1. Install the **NVIDIA GPU driver** appropriate for your distro (verify with `nvidia-smi`).  
    Docs: https://docs.nvidia.com/cuda/
  2. Install **CUDA runtime libraries** (matching your driverâ€™s supported CUDA version).  
    Follow the **CUDA Installation Guide for Linux** in the docs above.
  3. (Optional) Quick checks:
        ```bash
        nvidia-smi
        ldconfig -p | grep -E 'libcudart|libcublas|libcudnn' || true
        ```

### Before first run (line endings & permissions)

To avoid compatibility/permission issues on the main script itself:

- Convert CRLF â†’ LF on the CLI script:
  ```bash
  # Run from the repo root
  dos2unix tokamak-cli
  ```

- Make the CLI executable:
  ```bash
  chmod +x tokamak-cli
  ```

## How to run (for all platforms)

From the repository root:

1) **Install** (Install deps, compile circuits, write RPC URL using your **Alchemy API key**, run trusted setup, then run OS-specific backend packaging)
```bash
./tokamak-cli --install <YOUR_ALCHEMY_API_KEY | FULL_RPC_URL>
```

2) **Synthesize** (prepare inputs using a transaction config JSON)
```bash
./tokamak-cli --synthesize <PATH_TO_CONFIG_JSON>
```
> A template for the config JSON lives in `synthesizer-input-template/`.

3) **Preprocess** (backend preprocess stage)
```bash
./tokamak-cli --preprocess
```

4) **Prove** (backend prove stage; outputs stay under `dist/<platform>/resource/prove/output`)
```bash
./tokamak-cli --prove
```

5) **Verify** (verify proof artifacts in dist; optional resource overlay path)
```bash
# Uses dist/<platform>/resource by default
./tokamak-cli --verify

# Or provide a directory containing a resource/ folder to overlay into dist before verifying
./tokamak-cli --verify <PATH_WITH_RESOURCE>
```

6) **Extract proof bundle** (optional; zip key artifacts)
```bash
./tokamak-cli --extract-proof <OUTPUT_ZIP_PATH>
```

## Disclaimer
- The Tokamakâ€‘zkâ€‘EVM project and its maintainers are **not responsible for any leakage or misuse of your API keys or credentials**.
- For local testing, use a **free, nonâ€‘sensitive Alchemy API key**. Do **not** use production or paid keys, or keys tied to sensitive data.
- During `--install`, the CLI writes your RPC endpoint to `packages/frontend/synthesizer/.env`. We **recommend deleting `.env` after use** (or rotating the key) and ensuring it is **not committed** to version control.

## Package Composition
![Tokamak-zk-EVM Flow Chart](.github/assets/flowchart.png)

### Frontend Packages (compilers)

| Package                                            | Description                                                                        | Language   | Status   |
| -------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------- | -------- |
| [`qap-compiler`](./packages/frontend/qap-compiler) | Library of subcircuits for basic EVM operations                                    | Circom     | ğŸ§ª Beta |
| [`synthesizer`](./packages/frontend/synthesizer)   | Compiler that converts an Ethereum transaction into a circuit for Tokamak zk-SNARK | TypeScript | ğŸ§ª Beta |

### Backend Packages


| Package                                                   | Description                                                                       | Language       | Status  |
| --------------------------------------------------------- | --------------------------------------------------------------------------------- | -------------- | ------- |
| [`mpc-setup`](./packages/backend/setup/mpc-setup)         | Tokamak zk-SNARK's setup algorithm (multi-party computation version)              | Rust           | ğŸ§ª Beta |
| [`trusted-setup`](./packages/backend/setup/trusted-setup) | Tokamak zk-SNARK's setup algorithm (trusted single entity version)                | Rust           | ğŸ§ª Beta |
| [`prover`](./packages/backend/prove)                      | Tokamak zk-SNARK's proving algorithm                                              | Rust           | ğŸ§ª Beta |
| [`verify`](./packages/backend/verify)                     | Tokamak zk-SNARK's verifying algorithm                                            | Rust, Solidity | ğŸ§ª Beta |

> Notes:
> - ğŸ”¥ Alpha: Initial proof-of-concept for testing
> - ğŸ§ª Beta: Fully featured, but unstable and unoptimized
> - â­ï¸ Stable (v1.0.0): Fully featured, stable, and optimized

## Development status
### Sep. 2025
- Archived in branch "[archive-airdrop-Sep25](https://github.com/tokamak-network/Tokamak-zk-EVM/tree/archive-airdrop-Sep25)".
- Incomplete conversion of Ethereum transactions into ZKPs.
- What does "incomplete" mean? ZKPs only include the execution of a transaction's opcodes. Verification of input state and the transaction signature, as well as reconstruction of output state, are excluded.
- The Tokamak zk-SNARK backend is ready to use:
  - MSM and NTT are accelerated by [ICICLE APIs](https://github.com/ingonyama-zk/icicle).
  - It requires < 10GB memory.
  - A ZKP can be generated in 1-2 mins on CUDA or Apple silicon.

## Jan. 2026
- The current main branch.
- Complete conversion of Tokamak Layer 2 transactions into ZKPs, which covers:
  - Verification of transaction signatures,
  - Verification of input state,
  - Execution of transaction opcodes,
  - Reconstruction of output state.
- Compatible with [Tokamak Private App Channels](https://github.com/tokamak-network/Tokamak-zkp-channel-manager).


## Documentation

- [Project Tokamak Network ZKP (Medium)](https://medium.com/tokamak-network/project-tokamak-zk-evm-67483656fd21) (Last updated in Nov. 2025)
- [Project Tokamak zk-EVM(Slide)](https://docs.google.com/presentation/d/1D49fRElwkZYbEvQXB_rp5DEy22HFsabnXyeMQdNgjRw/edit?usp=sharing) (Last updated in Jul. 2025)
- [Tokamak zk-SNARK Paper](https://eprint.iacr.org/2024/507) (Last updated in Apr. 2025)
- Frontend - [Synthesizer](https://tokamak-network-zk-evm.gitbook.io/tokamak-network-zk-evm) (work in progress)
<!-- - [API Reference](./docs/api) -->

## Contributing

We welcome contributions! Please see our [Contributing Guidelines](./CONTRIBUTING.md) for details.

## License

This project is dual-licensed under:

- [MIT License](./LICENSE-MIT)
- [Apache License 2.0](./LICENSE-APACHE)

You may choose either license when using this software. This dual-licensing approach is standard in the Rust ecosystem and provides maximum compatibility with other open-source projects.



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Tokamak-zk-EVM

Thank you for your interest in contributing to Tokamak-zk-EVM! This document provides guidelines and instructions for contributing.

## Getting Started

1. Fork the repository
2. Clone your fork:
   ```bash
   git clone https://github.com/your-username/tokamak-zk-evm.git
   ```
3. Add the upstream remote:
   ```bash
   git remote add upstream https://github.com/tokamak-network/tokamak-zk-evm.git
   ```

## Development Workflow

1. Create a new branch from `dev`:
   ```bash
   git checkout dev
   git pull upstream dev
   git checkout -b feature/your-feature
   ```

2. Make your changes following our coding conventions

3. Commit your changes:
   ```bash
   git commit -m "feat: add new feature"
   ```
   
   We use conventional commits with the following types:
   - `feat`: New feature
   - `fix`: Bug fix
   - `docs`: Documentation changes
   - `chore`: Maintenance tasks
   - `test`: Adding or updating tests
   - `refactor`: Code refactoring

4. Push to your fork:
   ```bash
   git push origin feature/your-feature
   ```

5. Open a Pull Request

## Pull Request Guidelines

- PRs should be made against the `dev` branch
- Include a clear description of the changes
- Update relevant documentation
- Add or update tests as needed
- Ensure all tests pass
- Follow existing code style

## Code Style

- Use consistent naming conventions
- Write clear comments and documentation
- Follow language-specific conventions:
  - Rust: Follow `rustfmt` guidelines
  - Solidity: Follow Solidity style guide
  - TypeScript: Use prettier and eslint configurations

## Testing

- Write unit tests for new features
- Ensure all tests pass before submitting PR
- Include integration tests where appropriate

## Questions or Problems?

- Open an issue for bugs
- Join our [Discord](https://discord.gg/tokamak) for questions
- Check existing issues and PRs before creating new ones

## License

By contributing, you agree that your contributions will be licensed under the project's [MPL-2.0 License](./LICENSE). 


================================================
FILE: docker-compose.yaml
================================================
services:
  cli:
    build: ./
    volumes:
      - ./:/app
    working_dir: /app
    tty: true       
    stdin_open: true
    environment:
      - TOKAMAK_ZK_EVM_ROOT=/app


================================================
FILE: dockerfile
================================================
FROM ubuntu:latest
# ---------- OS packages ----------
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive \
    apt-get install -y \
        git curl make build-essential unzip pkg-config libssl-dev jq \
        git-lfs \
        cmake \
        dos2unix

# ---------- Rust ----------
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"

# ---------- Circom ----------
RUN git clone https://github.com/iden3/circom.git /tmp/circom \
 && cd /tmp/circom && cargo build --release \
 && cp /tmp/circom/target/release/circom /usr/local/bin/ \
 && rm -rf /tmp/circom

# ---------- Node ----------
RUN curl -fsSL https://deb.nodesource.com/setup_22.x | bash -
RUN apt-get install -y nodejs
ENV PNPM_HOME="/root/.local/share/pnpm"
ENV PATH="${PNPM_HOME}:${PATH}"
RUN npm install -g pnpm
RUN pnpm add -g snarkjs

# ---------- Foundry (forge / anvil) ----------
RUN curl -L https://foundry.paradigm.xyz | bash \
 && ~/.foundry/bin/foundryup


================================================
FILE: entitlements.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
  <!-- Allwoing JIT -->
  <key>com.apple.security.cs.allow-jit</key><true/>
  <!-- For improved compatibility with old MacOS -->
  <!-- <key>com.apple.security.cs.allow-unsigned-executable-memory</key><true/> -->
</dict>
</plist>



================================================
FILE: learna.json
================================================
{
  "version": "independent",
  "npmClient": "yarn",
  "useWorkspaces": true,
  "packages": ["packages/*"],
  "command": {
    "publish": {
      "conventionalCommits": true,
      "message": "chore(release): publish"
    }
  }
}


================================================
FILE: LICENSE
================================================
Mozilla Public License Version 2.0
==================================

1. Definitions
--------------

1.1. "Contributor"
    means each individual or legal entity that creates, contributes to
    the creation of, or owns Covered Software.

1.2. "Contributor Version"
    means the combination of the Contributions of others (if any) used
    by a Contributor and that particular Contributor's Contribution.

1.3. "Contribution"
    means Covered Software of a particular Contributor.

1.4. "Covered Software"
    means Source Code Form to which the initial Contributor has attached
    the notice in Exhibit A, the Executable Form of such Source Code
    Form, and Modifications of such Source Code Form, in each case
    including portions thereof.

1.5. "Incompatible With Secondary Licenses"
    means

    (a) that the initial Contributor has attached the notice described
        in Exhibit B to the Covered Software; or

    (b) that the Covered Software was made available under the terms of
        version 1.1 or earlier of the License, but not also under the
        terms of a Secondary License.

1.6. "Executable Form"
    means any form of the work other than Source Code Form.

1.7. "Larger Work"
    means a work that combines Covered Software with other material, in
    a separate file or files, that is not Covered Software.

1.8. "License"
    means this document.

1.9. "Licensable"
    means having the right to grant, to the maximum extent possible,
    whether at the time of the initial grant or subsequently, any and
    all of the rights conveyed by this License.

1.10. "Modifications"
    means any of the following:

    (a) any file in Source Code Form that results from an addition to,
        deletion from, or modification of the contents of Covered
        Software; or

    (b) any new file in Source Code Form that contains any Covered
        Software.

1.11. "Patent Claims" of a Contributor
    means any patent claim(s), including without limitation, method,
    process, and apparatus claims, in any patent Licensable by such
    Contributor that would be infringed, but for the grant of the
    License, by the making, using, selling, offering for sale, having
    made, import, or transfer of either its Contributions or its
    Contributor Version.

1.12. "Secondary License"
    means either the GNU General Public License, Version 2.0, the GNU
    Lesser General Public License, Version 2.1, the GNU Affero General
    Public License, Version 3.0, or any later versions of those
    licenses.

1.13. "Source Code Form"
    means the form of the work preferred for making modifications.

1.14. "You" (or "Your")
    means an individual or a legal entity exercising rights under this
    License. For legal entities, "You" includes any entity that
    controls, is controlled by, or is under common control with You. For
    purposes of this definition, "control" means (a) the power, direct
    or indirect, to cause the direction or management of such entity,
    whether by contract or otherwise, or (b) ownership of more than
    fifty percent (50%) of the outstanding shares or beneficial
    ownership of such entity.

2. License Grants and Conditions
--------------------------------

2.1. Grants

Each Contributor hereby grants You a world-wide, royalty-free,
non-exclusive license:

(a) under intellectual property rights (other than patent or trademark)
    Licensable by such Contributor to use, reproduce, make available,
    modify, display, perform, distribute, and otherwise exploit its
    Contributions, either on an unmodified basis, with Modifications, or
    as part of a Larger Work; and

(b) under Patent Claims of such Contributor to make, use, sell, offer
    for sale, have made, import, and otherwise transfer either its
    Contributions or its Contributor Version.

2.2. Effective Date

The licenses granted in Section 2.1 with respect to any Contribution
become effective for each Contribution on the date the Contributor first
distributes such Contribution.

2.3. Limitations on Grant Scope

The licenses granted in this Section 2 are the only rights granted under
this License. No additional rights or licenses will be implied from the
distribution or licensing of Covered Software under this License.
Notwithstanding Section 2.1(b) above, no patent license is granted by a
Contributor:

(a) for any code that a Contributor has removed from Covered Software;
    or

(b) for infringements caused by: (i) Your and any other third party's
    modifications of Covered Software, or (ii) the combination of its
    Contributions with other software (except as part of its Contributor
    Version); or

(c) under Patent Claims infringed by Covered Software in the absence of
    its Contributions.

This License does not grant any rights in the trademarks, service marks,
or logos of any Contributor (except as may be necessary to comply with
the notice requirements in Section 3.4).

2.4. Subsequent Licenses

No Contributor makes additional grants as a result of Your choice to
distribute the Covered Software under a subsequent version of this
License (see Section 10.2) or under the terms of a Secondary License (if
permitted under the terms of Section 3.3).

2.5. Representation

Each Contributor represents that the Contributor believes its
Contributions are its original creation(s) or it has sufficient rights
to grant the rights to its Contributions conveyed by this License.

2.6. Fair Use

This License is not intended to limit any rights You have under
applicable copyright doctrines of fair use, fair dealing, or other
equivalents.

2.7. Conditions

Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
in Section 2.1.

3. Responsibilities
-------------------

3.1. Distribution of Source Form

All distribution of Covered Software in Source Code Form, including any
Modifications that You create or to which You contribute, must be under
the terms of this License. You must inform recipients that the Source
Code Form of the Covered Software is governed by the terms of this
License, and how they can obtain a copy of this License. You may not
attempt to alter or restrict the recipients' rights in the Source Code
Form.

3.2. Distribution of Executable Form

If You distribute Covered Software in Executable Form then:

(a) such Covered Software must also be made available in Source Code
    Form, as described in Section 3.1, and You must inform recipients of
    the Executable Form how they can obtain a copy of such Source Code
    Form by reasonable means in a timely manner, at a charge no more
    than the cost of distribution to the recipient; and

(b) You may distribute such Executable Form under the terms of this
    License, or sublicense it under different terms, provided that the
    license for the Executable Form does not attempt to limit or alter
    the recipients' rights in the Source Code Form under this License.

3.3. Distribution of a Larger Work

You may create and distribute a Larger Work under terms of Your choice,
provided that You also comply with the requirements of this License for
the Covered Software. If the Larger Work is a combination of Covered
Software with a work governed by one or more Secondary Licenses, and the
Covered Software is not Incompatible With Secondary Licenses, this
License permits You to additionally distribute such Covered Software
under the terms of such Secondary License(s), so that the recipient of
the Larger Work may, at their option, further distribute the Covered
Software under the terms of either this License or such Secondary
License(s).

3.4. Notices

You may not remove or alter the substance of any license notices
(including copyright notices, patent notices, disclaimers of warranty,
or limitations of liability) contained within the Source Code Form of
the Covered Software, except that You may alter any license notices to
the extent required to remedy known factual inaccuracies.

3.5. Application of Additional Terms

You may choose to offer, and to charge a fee for, warranty, support,
indemnity or liability obligations to one or more recipients of Covered
Software. However, You may do so only on Your own behalf, and not on
behalf of any Contributor. You must make it absolutely clear that any
such warranty, support, indemnity, or liability obligation is offered by
You alone, and You hereby agree to indemnify every Contributor for any
liability incurred by such Contributor as a result of warranty, support,
indemnity or liability terms You offer. You may include additional
disclaimers of warranty and limitations of liability specific to any
jurisdiction.

4. Inability to Comply Due to Statute or Regulation
---------------------------------------------------

If it is impossible for You to comply with any of the terms of this
License with respect to some or all of the Covered Software due to
statute, judicial order, or regulation then You must: (a) comply with
the terms of this License to the maximum extent possible; and (b)
describe the limitations and the code they affect. Such description must
be placed in a text file included with all distributions of the Covered
Software under this License. Except to the extent prohibited by statute
or regulation, such description must be sufficiently detailed for a
recipient of ordinary skill to be able to understand it.

5. Termination
--------------

5.1. The rights granted under this License will terminate automatically
if You fail to comply with any of its terms. However, if You become
compliant, then the rights granted under this License from a particular
Contributor are reinstated (a) provisionally, unless and until such
Contributor explicitly and finally terminates Your grants, and (b) on an
ongoing basis, if such Contributor fails to notify You of the
non-compliance by some reasonable means prior to 60 days after You have
come back into compliance. Moreover, Your grants from a particular
Contributor are reinstated on an ongoing basis if such Contributor
notifies You of the non-compliance by some reasonable means, this is the
first time You have received notice of non-compliance with this License
from such Contributor, and You become compliant prior to 30 days after
Your receipt of the notice.

5.2. If You initiate litigation against any entity by asserting a patent
infringement claim (excluding declaratory judgment actions,
counter-claims, and cross-claims) alleging that a Contributor Version
directly or indirectly infringes any patent, then the rights granted to
You by any and all Contributors for the Covered Software under Section
2.1 of this License shall terminate.

5.3. In the event of termination under Sections 5.1 or 5.2 above, all
end user license agreements (excluding distributors and resellers) which
have been validly granted by You or Your distributors under this License
prior to termination shall survive termination.

************************************************************************
*                                                                      *
*  6. Disclaimer of Warranty                                           *
*  -------------------------                                           *
*                                                                      *
*  Covered Software is provided under this License on an "as is"       *
*  basis, without warranty of any kind, either expressed, implied, or  *
*  statutory, including, without limitation, warranties that the       *
*  Covered Software is free of defects, merchantable, fit for a        *
*  particular purpose or non-infringing. The entire risk as to the     *
*  quality and performance of the Covered Software is with You.        *
*  Should any Covered Software prove defective in any respect, You     *
*  (not any Contributor) assume the cost of any necessary servicing,   *
*  repair, or correction. This disclaimer of warranty constitutes an   *
*  essential part of this License. No use of any Covered Software is   *
*  authorized under this License except under this disclaimer.         *
*                                                                      *
************************************************************************

************************************************************************
*                                                                      *
*  7. Limitation of Liability                                          *
*  --------------------------                                          *
*                                                                      *
*  Under no circumstances and under no legal theory, whether tort      *
*  (including negligence), contract, or otherwise, shall any           *
*  Contributor, or anyone who distributes Covered Software as          *
*  permitted above, be liable to You for any direct, indirect,         *
*  special, incidental, or consequential damages of any character      *
*  including, without limitation, damages for lost profits, loss of    *
*  goodwill, work stoppage, computer failure or malfunction, or any    *
*  and all other commercial damages or losses, even if such party      *
*  shall have been informed of the possibility of such damages. This   *
*  limitation of liability shall not apply to liability for death or   *
*  personal injury resulting from such party's negligence to the       *
*  extent applicable law prohibits such limitation. Some               *
*  jurisdictions do not allow the exclusion or limitation of           *
*  incidental or consequential damages, so this exclusion and          *
*  limitation may not apply to You.                                    *
*                                                                      *
************************************************************************

8. Litigation
-------------

Any litigation relating to this License may be brought only in the
courts of a jurisdiction where the defendant maintains its principal
place of business and such litigation shall be governed by laws of that
jurisdiction, without reference to its conflict-of-law provisions.
Nothing in this Section shall prevent a party's ability to bring
cross-claims or counter-claims.

9. Miscellaneous
----------------

This License represents the complete agreement concerning the subject
matter hereof. If any provision of this License is held to be
unenforceable, such provision shall be reformed only to the extent
necessary to make it enforceable. Any law or regulation which provides
that the language of a contract shall be construed against the drafter
shall not be used to construe this License against a Contributor.

10. Versions of the License
---------------------------

10.1. New Versions

Mozilla Foundation is the license steward. Except as provided in Section
10.3, no one other than the license steward has the right to modify or
publish new versions of this License. Each version will be given a
distinguishing version number.

10.2. Effect of New Versions

You may distribute the Covered Software under the terms of the version
of the License under which You originally received the Covered Software,
or under the terms of any subsequent version published by the license
steward.

10.3. Modified Versions

If you create software not governed by this License, and you want to
create a new license for such software, you may create and use a
modified version of this License if you rename the license and remove
any references to the name of the license steward (except to note that
such modified license differs from this License).

10.4. Distributing Source Code Form that is Incompatible With Secondary
Licenses

If You choose to distribute Source Code Form that is Incompatible With
Secondary Licenses under the terms of this version of the License, the
notice described in Exhibit B of this License must be attached.

Exhibit A - Source Code Form License Notice
-------------------------------------------

  This Source Code Form is subject to the terms of the Mozilla Public
  License, v. 2.0. If a copy of the MPL was not distributed with this
  file, You can obtain one at https://mozilla.org/MPL/2.0/.

If it is not possible or desirable to put the notice in a particular
file, then You may include the notice in a location (such as a LICENSE
file in a relevant directory) where a recipient would be likely to look
for such a notice.

You may add additional accurate notices of copyright ownership.

Exhibit B - "Incompatible With Secondary Licenses" Notice
---------------------------------------------------------

  This Source Code Form is "Incompatible With Secondary Licenses", as
  defined by the Mozilla Public License, v. 2.0.



================================================
FILE: LICENSE-APACHE
================================================
Apache License

Version 2.0, January 2004

http://www.apache.org/licenses/

Copyright (c) 2024-2025 Tokamak Network

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

---

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.




================================================
FILE: LICENSE-MIT
================================================
MIT License

Copyright (c) 2024-2025 Tokamak Network

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.




================================================
FILE: Makefile
================================================
.PHONY: all build test clean

# Default command
all: build

# Build commands
build: build-buildqap build-mpc build-tokamak

build-buildqap:
	@echo "Building buildQAP..."
	cd packages/buildQAP && npm install && npm run build

build-mpc:
	@echo "Building MPC..."
	cd packages/mpc/rust-src && cargo build
	cd packages/mpc/contracts && forge build

build-tokamak:
	@echo "Building Tokamak zk-SNARK..."
	cd packages/tokamak-zk-snark/rust-src && cargo build
	cd packages/tokamak-zk-snark/contracts && forge build

# Test commands
test: test-buildqap test-mpc test-tokamak

test-buildqap:
	@echo "Testing buildQAP..."
	cd packages/buildQAP && npm test

test-mpc:
	@echo "Testing MPC..."
	cd packages/mpc/rust-src && cargo test
	cd packages/mpc/contracts && forge test

test-tokamak:
	@echo "Testing Tokamak zk-SNARK..."
	cd packages/tokamak-zk-snark/rust-src && cargo test
	cd packages/tokamak-zk-snark/contracts && forge test

# Clean commands
clean:
	@echo "Cleaning all packages..."
	cd packages/buildQAP && npm run clean
	cd packages/mpc/rust-src && cargo clean
	cd packages/tokamak-zk-snark/rust-src && cargo clean
	find . -name "node_modules" -type d -prune -exec rm -rf '{}' +


================================================
FILE: package.json
================================================
{
  "name": "tokamak-zk-evm-monorepo",
  "version": "1.0.0",
  "description": "Tokamak zk-EVM: A tool that converts Ethereum transactions into Zero-Knowledge Proofs",
  "license": "MIT OR Apache-2.0",
  "author": "Tokamak Network",
  "private": true,
  "workspaces": [
    "packages/frontend/synthesizer/libs/*"
  ],
  "scripts": {
    "test": "lerna run test",
    "lint": "lerna run lint",
    "lint:fix": "lerna run lint:fix",
    "format": "prettier --write \"**/*.{ts,js,json,md}\"",
    "format:check": "prettier --check \"**/*.{ts,js,json,md}\"",
    "clean": "lerna run clean"
  },
  "devDependencies": {
    "eslint": "^8.0.0",
    "lerna": "^8.0.0",
    "prettier": "^3.5.3",
    "typescript": "^5.0.0"
  },
  "dependencies": {
    "pip": "^0.0.1",
    "tsx": "^4.20.5"
  }
}



================================================
FILE: tokamak-cli
================================================
#!/usr/bin/env bash
# =========================
# Tokamak zkEVM CLI Wrapper
# This script forwards all commands to the actual CLI in scripts/
# =========================

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Forward all arguments to the actual tokamak-cli-core script
exec "${SCRIPT_DIR}/scripts/tokamak-cli-core" "$@"


================================================
FILE: vercel.json
================================================
[Empty file]


================================================
FILE: .dockerignore
================================================
# Git
.git
.gitignore

# Node
node_modules
npm-debug.log

# Build artifacts
/packages/backend/target
/packages/frontend/synthesizer/dist

# IDE / OS specific
.vscode
.idea
.DS_Store


================================================
FILE: .editorconfig
================================================
# EditorConfig is awesome: https://EditorConfig.org

# Top-most EditorConfig file
root = true

# Unix-style newlines with a newline ending every file
[*]
charset = utf-8
end_of_line = lf
insert_final_newline = true
trim_trailing_whitespace = true

# TypeScript, JavaScript, JSON
[*.{ts,js,mts,cts,json,jsonc}]
indent_style = space
indent_size = 2
max_line_length = 120

# Markdown
[*.md]
trim_trailing_whitespace = false
max_line_length = off

# YAML
[*.{yml,yaml}]
indent_style = space
indent_size = 2

# Rust
[*.rs]
indent_style = space
indent_size = 4
max_line_length = 100

# Shell scripts
[*.sh]
indent_style = space
indent_size = 2

# Makefiles
[Makefile]
indent_style = tab




================================================
FILE: .prettierignore
================================================
# Dependencies
node_modules
**/node_modules

# Build outputs
dist
build
out
target
*.tsbuildinfo

# Generated files
coverage
.nyc_output

# Lock files
package-lock.json
yarn.lock
pnpm-lock.yaml
bun.lockb

# Cache
.cache
.parcel-cache
.next
.nuxt
.vuepress/dist

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# OS files
.DS_Store
Thumbs.db

# IDE
.vscode
.idea

# Temporary files
*.tmp
*.temp

# Rust
Cargo.lock

# Git
.git
.gitignore

# CI/CD
.github

# Documentation that shouldn't be formatted
CHANGELOG.md




================================================
FILE: .prettierrc
================================================
{
  "semi": true,
  "trailingComma": "all",
  "singleQuote": true,
  "printWidth": 120,
  "tabWidth": 2,
  "arrowParens": "avoid",
  "bracketSpacing": true,
  "endOfLine": "lf",
  "useTabs": false,
  "proseWrap": "preserve"
}



================================================
FILE: config/monorepo-js/README.md
================================================
# EthereumJS - Monorepo and Shared Config

This folder contains documentation on monorepo setup and configuration as well as shared
configuration settings and scripts for common modules (like a shared `tsconfig.json` TypeScript
configuration file or `cli/coverage.sh` script).

- [Monorepo](./MONOREPO.md) (separate docs)
- [E2E Testing](./E2E_TESTING.md) (separate docs)
- [Linting](#linting)
- [Coverage](#coverage)
- [TypeScript](#typescript)
- [Documentation](#documentation)

The cli scripts (`./config/cli`) are used in the child packages' `package.json` to ease repetitive script use.

## Dependencies

Shared config `devDependencies` across all packages can go in the root `package.json`. However, if a package needs access to the dependency's `bin` (e.g. `eslint` or `nyc`) then it must be defined in the child packages' `package.json`.

## Linting

Common linting configuration utilizing:

- [ESLint](https://eslint.org/)
- [TypeScript ESLint](https://github.com/typescript-eslint/typescript-eslint)
- [TypeStrict](https://github.com/krzkaczor/TypeStrict)
- [Prettier](https://prettier.io/docs/en/integrating-with-linters.html)

Exposed CLI commands:

- `./config/cli/lint.sh`
- `./config/cli/lint-fix.sh`

### Usage

Add `.eslintrc.js`:

```js
module.exports = {
  extends: '../../config/eslint.js',
}
```

In this file you can add rule adjustments or overrides for the specific package.

Add `prettier.config.js`:

```js
module.exports = require('../../config/prettier.config')
```

Use CLI commands above in your `package.json`:

```json
  "scripts": {
    "lint": "../../config/cli/lint.sh",
    "lint:fix": "../../config/cli/lint-fix.sh",
  }
```

### Getting the most out of linting

This lint package is used as git pre-push hook on with the help of [Husky](https://www.npmjs.com/package/husky).

## Coverage

Tool: [nyc](https://istanbul.js.org/)

Exposed CLI command:

- `./config/cli/coverage.sh`

### Usage

Add `.nycrc`:

```json
{
  "extends": "../../config/.c8rc.json"
}
```

Use script above in `package.json`:

```json
  "scripts": {
    "coverage": "../../config/cli/coverage.sh"
  }
```

## TypeScript

Tool: [TypeScript](https://www.typescriptlang.org/)

Exposed CLI commands:

- `./config/cli/ts-build.sh`
- `./config/cli/ts-compile.sh`

### Usage

The three files below compose the functionality built into `ts-build.sh` and `ts-compile.sh`. Note that the build is browser compatible with `ES2020` target.
Add `tsconfig.json`:

```json
{
  "extends": "../../../config/monorepo-js/tsconfig.json",
  "include": ["src/**/*.ts", "test/**/*.ts"]
}
```

Add `tsconfig.prod.json`:

```json
{
  "extends": "../../../config/monorepo-js/tsconfig.prod.json",
  "include": ["src/**/*.ts"],
  "compilerOptions": {
    "outDir": "./dist"
  }
}
```

Note: the `outDir` property is mandatory to generate assets to a directory.

Use CLI commands above in your `package.json`:

```json
  "scripts": {
    "tsc":   "../../config/cli/ts-compile.sh",
    "build": "../../config/cli/ts-build.sh"
  }
```

## Documentation

Add `typedoc.js` to a package that extends the generic TypeDoc configuration:

```js
module.exports = {
  extends: '../../config/typedoc.js',
  // Additional directives
}
```



================================================
FILE: config/monorepo-js/cspell-md.json
================================================
{
  "language": "en-US",
  "ignoreRegExpList": ["/0x[0-9A-Fa-f]+/"],
  "words": [
    "trienode",
    "t8ntool",
    "calldatasize",
    "Dencun",
    "Hardfork",
    "acolytec",
    "hardfork",
    "Holesky",
    "MCOPY",
    "SELFDESTRUCT",
    "BLOBBASEFEE",
    "keccak",
    "Verkle",
    "devnet",
    "devp2p",
    "hardforks",
    "renamings",
    "chainsafe",
    "secp256k1",
    "eips",
    "extradata",
    "devnets",
    "tada",
    "statemanager",
    "ethash",
    "NOTURN",
    "subclassing",
    "Randao",
    "PREVRANDAO",
    "Ghorbanian",
    "merkle",
    "backports",
    "behaviour",
    "Ethash",
    "Chainstart",
    "Promisification",
    "promisified",
    "chainstart",
    "callbackify",
    "vpulim",
    "chaindata",
    "leveldb",
    "Gitter",
    "ethjs",
    "ruleset",
    "polkadot",
    "stabilizations",
    "holesky",
    "newpayload",
    "bootnodes",
    "bootnode",
    "vmexecution",
    "sszify",
    "libp2p",
    "forkchoice",
    "blockhash",
    "Dockerfiles",
    "blockfetcher",
    "txpool",
    "subchain",
    "enode",
    "unpadded",
    "Ints",
    "reverseblockfetcher",
    "forkhash",
    "Logvinov",
    "Taunas",
    "helprpc",
    "loglevel",
    "Logfile",
    "prefund",
    "Yolov",
    "Calaveras",
    "Vinay",
    "Pulim",
    "Rlpx",
    "rlpx",
    "hiveview",
    "Prysm",
    "prysm",
    "prysmctl",
    "datadir",
    "jwtsecret",
    "rustup",
    "DATADIR",
    "maxpeers",
    "syncmode",
    "nodiscover",
    "sealhash",
    "Nethermind",
    "Merkle",
    "ethpandaops",
    "Teku",
    "prefunded",
    "etherbase",
    "findneighbour",
    "testnetworks",
    "testnests",
    "DATAFEE",
    "MULTIPEER",
    "syncpeer",
    "Beaconsync",
    "datadirs",
    "NETWORKID",
    "ELCLIENT",
    "beaconsync",
    "testvectors",
    "Snapsync",
    "snapsync",
    "beaconchain",
    "VZWK",
    "Rwlk",
    "USVGK",
    "Kmhd",
    "EAAAEAIAAAAAAAAAI",
    "Ahtu",
    "NBHJMH",
    "Nwgi",
    "multiaddrs",
    "teku",
    "nethermind",
    "besu",
    "Forkid",
    "initcode",
    "erigon",
    "Lvmc",
    "DATAHASH",
    "BLOBHASH",
    "timebased",
    "Renamings",
    "Authcall",
    "Tyneway",
    "SLOAD",
    "SSTORE",
    "calaveras",
    "yolov",
    "BASEFEE",
    "EIPs",
    "Forkhash",
    "Forkhashes",
    "AUTHCALL",
    "Kademlia",
    "forkid",
    "scure",
    "paulmillr",
    "Neighbours",
    "neighbour",
    "ecies",
    "RLPX",
    "BOOTNODES",
    "Kbucket",
    "datagram",
    "UDPPORT",
    "TCPPORT",
    "ECIES",
    "Neighbour",
    "findneighbours",
    "pydevp",
    "mkcache",
    "ethashjs",
    "validblock",
    "rustbn",
    "Preimage",
    "verkle",
    "Preimages",
    "modexp",
    "roninjin",
    "kchojn",
    "ripemd",
    "Jochem",
    "JUMPDEST",
    "MLOAD",
    "MSTORE",
    "KECCAK",
    "TLOAD",
    "TSTORE",
    "Initcode",
    "bnadd",
    "bnmul",
    "fulltext",
    "eventemitter",
    "Codesize",
    "BLOCKHASH",
    "selfdestruct",
    "jochem",
    "brouwer",
    "alcuadrado",
    "Checkpointing",
    "checkpointing",
    "MODEXP",
    "RIPEMD",
    "Rebalance",
    "Sina",
    "Promisified",
    "ecmul",
    "Stateroot",
    "hotfixing",
    "EXTCODEHASH",
    "mattdean",
    "digicatapult",
    "rmeissner",
    "jwasinger",
    "Agusx",
    "Holger",
    "danjm",
    "whymarrh",
    "seesemichaelj",
    "axic",
    "totalgas",
    "RETURNDATA",
    "STATICCALL",
    "sstore",
    "CALLCODE",
    "Mgas",
    "wemeetagain",
    "Schtroumpf",
    "holgerd",
    "rbtree",
    "sdsl",
    "vitalik",
    "Vitalik",
    "Kaustinen",
    "faustbrian",
    "bmark",
    "samlior",
    "checkpointed",
    "retwrite",
    "maindb",
    "memdown",
    "Rocheleau",
    "Vitalik's",
    "LMDB",
    "lmdb",
    "flamegraph",
    "thislog",
    "thatlog",
    "otherlog",
    "sublog",
    "myscript",
    "behaviours",
    "unhashed",
    "xghi",
    "gregthegreek",
    "danksharding",
    "ecsign",
    "Unpadded",
    "talentlessguy",
    "Nomic",
    "unpad",
    "Keccak",
    "prebuilds",
    "keccakjs",
    "retesteth",
    "testdata",
    "Statetest",
    "jsontrace",
    "nomemory",
    "statetest",
    "pyethereum",
    "evmlab",
    "holiman",
    "flamegraphs",
    "testpath",
    "TESTPATH",
    "RETESTETH",
    "thirdparty",
    "hdkey",
    "jackalope",
    "Libray",
    "explcit",
    "ricmoo",
    "scryptsy",
    "randombytes",
    "ICAP",
    "cryptocoinjs",
    "xpub",
    "xpriv",
    "Thirdparty",
    "icap",
    "ethereumhdkey",
    "dklen",
    "hmac",
    "Beregszaszi",
    "mfornet",
    "blockfill",
    "Heydiho",
    "bazel",
    "tlsv",
    "lcli",
    "extip",
    "unauditability",
    "ized",
    "`scanf`",
    "tomonari",
    "dryajov",
    "dgram",
    "krzkaczor",
    "libotony",
    "xvfb",
    "alextsg",
    "Merkling",
    "ledgerhq",
    "accountprivate",
    "accountimport",
    "echash",
    "pubkey",
    "unpublish",
    "poap",
    "nycrc",
    "gitpkg",
    "multiaddress",
    "triggerable",
    "MCLBLS",
    "heartedly",
    "beaconroot"
  ]
}



================================================
FILE: config/monorepo-js/cspell-ts.json
================================================
{
  "language": "en-US",
  "ignoreRegExpList": ["/0x[0-9A-Fa-f]+/", "@scure"],
  "overrides": [
    {
      "filename": "**/packages/devp2p/**",
      "ignoreWords": ["pirl", "ubiq", "gwhale", "prichain", "zfill"]
    },
    {
      "filename": "**/packages/ethash/**",
      "ignoreWords": ["epoc", "cmix"]
    }
  ],
  "words": [
    "paulmillr",
    "t8ntool",
    "!Json",
    "!Rpc",
    "Hardfork",
    "hardfork",
    "Chainstart",
    "ethash",
    "KECCAK",
    "keccak",
    "Verkle",
    "verkle",
    "maxblob",
    "Randao",
    "hardforks",
    "ecsign",
    "extradata",
    "randao",
    "Unpadded",
    "Ethash",
    "blocktime",
    "timebomb",
    "prestate",
    "testdata",
    "eips",
    "chainstart",
    "Besu",
    "devnet",
    "Kaustinen",
    "kaustinen",
    "premerge",
    "txns",
    "blockdata",
    "Mockchain",
    "HARDFORK",
    "BLOCKHASH",
    "checkpointed",
    "headerchain",
    "Merkle",
    "cornercase",
    "Deauthorizations",
    "Deauthorizing",
    "deauthorized",
    "Deauthorize",
    "checkpointing",
    "Holesky",
    "holesky",
    "newblock",
    "newheader",
    "libp2p",
    "multiaddrs",
    "bootnode",
    "bootnodes",
    "Multiaddrs",
    "peerpool",
    "ethprotocol",
    "flowcontrol",
    "lesprotocol",
    "fullethereumservice",
    "lightethereumservice",
    "fullsync",
    "lightsync",
    "loglevel",
    "datadir",
    "DATADIR",
    "syncmode",
    "Libp",
    "chainsafe",
    "Multiaddr",
    "multiformats",
    "multiaddr",
    "Websockets",
    "websockets",
    "Bootnodes",
    "secp256k1",
    "Polkadot",
    "polkadot",
    "helprpc",
    "SYNCMODE",
    "LIGHTSERV",
    "enode",
    "discport",
    "MAXPERREQUEST",
    "MAXFETCHERJOBS",
    "MINPEERS",
    "MAXPEERS",
    "DNSADDR",
    "reexecuting",
    "DEBUGCODE",
    "findneighbour",
    "prefunded",
    "etherbase",
    "merkle",
    "Newpayload",
    "lightserv",
    "txpool",
    "Devnet",
    "prefund",
    "recid",
    "prefunding",
    "mockserver",
    "jwtsecret",
    "pkey",
    "qheap",
    "leveldown",
    "devp2p",
    "Rlpx",
    "backstep",
    "subchains",
    "subchain",
    "Subchain",
    "unsynced",
    "forkchoice",
    "preimages",
    "Preimages",
    "MAXFETCHERREQUESTS",
    "BACKSTEP",
    "SUBCHAIN",
    "NEWPAYLOAD",
    "snapstate",
    "vmexecution",
    "lightchain",
    "leveldb",
    "preimage",
    "Preimage",
    "bitvector",
    "MCLBLS",
    "statemanager",
    "rustbn",
    "resetted",
    "stateroot",
    "backstepping",
    "urlnopad",
    "bubbleup",
    "bubbledown",
    "payloadid",
    "blobsbundles",
    "blockhash",
    "rlpxpeer",
    "handshaked",
    "Devp",
    "rlpx",
    "Reinitiating",
    "tablesize",
    "peertablesize",
    "snapprotocol",
    "typeguard",
    "boundprotocol",
    "rlpxsender",
    "rlpxserver",
    "ECIES",
    "Neighbours",
    "Forkchoice",
    "FORKCHOICE",
    "errormsg",
    "snapsync",
    "newpayloadv",
    "vmhead",
    "sethead",
    "statebuild",
    "safeblock",
    "prio",
    "Keccak",
    "accountfetcher",
    "Snapsync",
    "headstate",
    "syncer",
    "syncedchain",
    "lastfilled",
    "lastexecuted",
    "lastfetched",
    "lastvalid",
    "lastsynced",
    "lastchain",
    "inited",
    "Initing",
    "unfinalized",
    "canonicality",
    "tailparent",
    "Backstepped",
    "Unfinalized",
    "stepback",
    "vmlog",
    "snapprogress",
    "trienodes",
    "minblob",
    "basefee",
    "sendobject",
    "broadcasterrors",
    "knownpeers",
    "sendobjects",
    "handledadds",
    "handlederrors",
    "handledobject",
    "successfuladds",
    "failedadds",
    "syncable",
    "bytecodefetcher",
    "storagefetcher",
    "trienodefetcher",
    "trienode",
    "reqs",
    "SNAPSYNC",
    "rerequest",
    "Receivedhash",
    "blockfetcherbase",
    "blockfetcher",
    "CODEHASH",
    "bytecodes",
    "headerfetcher",
    "reverseblockfetcher",
    "multiaccount",
    "slotset",
    "hashset",
    "proofset",
    "sdsl",
    "subtrie",
    "beaconsync",
    "reinited",
    "syncability",
    "ethjs",
    "matchip",
    "withengine",
    "withoutengine",
    "engineonly",
    "enrtree",
    "Hardforks",
    "restopped",
    "keyid",
    "mockchain",
    "pushable",
    "mocksender",
    "Pushable",
    "mockpeer",
    "simutils",
    "codehash",
    "nodeinfo",
    "postmerge",
    "newpayload",
    "testvectors",
    "statediffs",
    "feerecipient",
    "Testvectors",
    "rpctestnet",
    "blockopt",
    "TXINDEX",
    "badhex",
    "checksummed",
    "invalidlength",
    "blockhashes",
    "chainid",
    "txes",
    "DATAFEE",
    "kzgs",
    "iszero",
    "staticcall",
    "mload",
    "multipeer",
    "startnetwork",
    "NETWORKID",
    "MULTIPEER",
    "snapfetcher",
    "sidechain",
    "wasmecrecover",
    "forkhash",
    "besu",
    "EIPs",
    "SWAPN",
    "DUPN",
    "BASEFEE",
    "initcode",
    "PREVRANDAO",
    "MCOPY",
    "JUMPF",
    "SELFDESTRUCT",
    "triggerable",
    "RETURNDATASIZE",
    "RETURNDATACOPY",
    "STATICCALL",
    "BLOBBASEFEE",
    "regenesis",
    "SSTORE",
    "danksharding",
    "selfdestruct",
    "pectra",
    "Checkpointing",
    "typesafe",
    "epochlength",
    "Sstore",
    "sstore",
    "forkhashes",
    "BOOTNODES",
    "CLIENTID",
    "snappyjs",
    "uncompress",
    "sscanf",
    "scanf",
    "unpadded",
    "kbucket",
    "banlist",
    "neighbour",
    "findneighbours",
    "KBUCKET",
    "unstrict",
    "neighbours",
    "typedata",
    "sighash",
    "hashdata",
    "hashfn",
    "ecies",
    "ekey",
    "Neighbour",
    "ethdisco",
    "findnode",
    "misformatted",
    "dpts",
    "rlpxs",
    "RLPXs",
    "RLPX",
    "peername",
    "Comfortability",
    "bitstream",
    "Ecies",
    "Testdata",
    "mkcache",
    "validblock",
    "mixhashes",
    "newdata",
    "SDIV",
    "SMOD",
    "ADDMOD",
    "MULMOD",
    "SIGNEXTEND",
    "ISZERO",
    "CALLVALUE",
    "CALLDATALOAD",
    "CALLDATASIZE",
    "CALLDATACOPY",
    "CODESIZE",
    "CODECOPY",
    "EXTCODECOPY",
    "EXTCODEHASH",
    "CHAINID",
    "SELFBALANCE",
    "BLOBAHASH",
    "MLOAD",
    "MSTORE",
    "SLOAD",
    "JUMPI",
    "MSIZE",
    "JUMPDEST",
    "TLOAD",
    "TSTORE",
    "DATALOAD",
    "DATALOADN",
    "DATASIZE",
    "DATACOPY",
    "RJUMP",
    "RJUMPI",
    "RJUMPV",
    "CALLF",
    "RETF",
    "EOFCREATE",
    "RETURNCONTRACT",
    "CALLCODE",
    "DELEGATECALL",
    "RETURNDATALOAD",
    "EXTCALL",
    "EXTDELEGATECALL",
    "EXTSTATICCALL",
    "Initmode",
    "subcontainers",
    "initmode",
    "subcontainer",
    "Subcontainers",
    "rjump",
    "callf",
    "jumpf",
    "Retf",
    "Extcall",
    "extcall",
    "EOFBYTES",
    "EOFHASH",
    "RJUMPing",
    "delegatecall",
    "INITCODE",
    "Codestore",
    "Codesize",
    "COOG",
    "Accessfee",
    "CODESTORE",
    "BEGINSUB",
    "RETURNSUB",
    "JUMPSUB",
    "jumpdest",
    "beginsub",
    "codechunk",
    "jumpsub",
    "Selfdestruct",
    "eofcreate",
    "RIPEMD",
    "journaling",
    "BLOBHASH",
    "coldaccountaccess",
    "warmstorageread",
    "coldsload",
    "sstorenoop",
    "Sload",
    "jumptable",
    "VERKLE",
    "Toaddress",
    "sload",
    "zeroness",
    "ripemd",
    "sdiv",
    "smod",
    "addmod",
    "mulmod",
    "signextend",
    "callvalue",
    "calldataload",
    "calldatasize",
    "calldatacopy",
    "codesize",
    "codecopy",
    "extcodecopy",
    "mstore",
    "jumpi",
    "msize",
    "callcode",
    "prevrandao",
    "modexp",
    "Gquaddivisor",
    "returndatasize",
    "returndatacopy",
    "extcodehash",
    "selfbalance",
    "dupn",
    "swapn",
    "tstore",
    "tload",
    "rjumpi",
    "rjumpv",
    "retf",
    "blobhash",
    "mcopy",
    "extdelegatecall",
    "extstaticcall",
    "returndataload",
    "dataload",
    "dataloadn",
    "datasize",
    "datacopy",
    "blobbasefee",
    "returncontract",
    "MODEXP",
    "ECADD",
    "ecadd",
    "ECMUL",
    "ecmul",
    "ECPAIRING",
    "ecpairing",
    "BADARGS",
    "Jochem",
    "MAPFPTOG",
    "MAPFP",
    "IRTF",
    "Nethermind",
    "unnormalized",
    "Pippenger's",
    "Exponentiate",
    "Zcash",
    "ECPAIR",
    "behaviour",
    "unpad",
    "returndata",
    "RETURNDATA",
    "multiexp",
    "mclbls",
    "sstores",
    "blobgas",
    "rlptest",
    "vitalik",
    "Vitalik",
    "pedersen",
    "ZEROVALUE",
    "unaccessed",
    "poststate",
    "downleveled",
    "codehashes",
    "misbehaviour",
    "blocktag",
    "downleveling",
    "lmdb",
    "LMDB",
    "trietest",
    "dbkey",
    "unhashed",
    "KEYBYTES",
    "Keybytes",
    "unmatching",
    "keyvals",
    "checkroot",
    "noderef",
    "SECP",
    "startgas",
    "Vitalik's",
    "txbytes",
    "chunkify",
    "reserialized",
    "withdrawalsto",
    "leafnode",
    "lastblockhash",
    "bitvectors",
    "nethermind",
    "sgas",
    "dgas",
    "Mgas",
    "beaconroot",
    "predeploy",
    "Predeploy",
    "ommers",
    "BLOCKHASHes",
    "ommer",
    "Ommer",
    "nibling",
    "Nibling",
    "totalblob",
    "rlpd",
    "staticcalls",
    "PREBALANCE",
    "Blockhash",
    "selfdestructs",
    "Beaconroot",
    "blockroot",
    "CALLDATACOPYs",
    "BROOT",
    "broot",
    "Blockroot",
    "selfdestructed",
    "statelessly",
    "Pkey",
    "alcuadrado",
    "retesteth",
    "tomergeatdiff",
    "shanghaitocancunattime",
    "cancuntopragueattime",
    "jsontrace",
    "alltests",
    "postconditions",
    "hdkey",
    "thirdparty",
    "keysize",
    "ciphertext",
    "userid",
    "cipherparams",
    "encseed",
    "ethaddr",
    "btcaddr",
    "icap",
    "ICAP",
    "fixtureseed",
    "fixturehd",
    "hdnode",
    "testpassword",
    "wrongtestpassword",
    "keybyte",
    "unstub",
    "unmock",
    "unjustifiedly",
    "uncompression",
    "dedicatedly",
    "EVMBLS",
    "EVMBN",
    "EVMONEs",
    "INTURN",
    "NOTURN",
    "Andras",
    "Radics",
    "Fedor",
    "Indutny",
    "Kademlia",
    "Slominski",
    "patarapolw",
    "nickdodson",
    "Kintsugi",
    "deauthorization",
    "mixhash",
    "blockperiodseconds",
    "filledwith",
    "lllcversion",
    "Netsplit",
    "Statetest",
    "testeth",
    "PUSHDATA",
    "chunkified"
  ]
}



================================================
FILE: config/monorepo-js/E2E_TESTING.md
================================================
# EthereumJS - Local and E2E Testing

## Testing packages locally on other projects

There are some ways you can link this repository packages to other projects before publishing. You can symlink dependencies with [`npm link <package>`](https://docs.npmjs.com/cli/link), or install packages from the filesystem using [`npm install <folder>`](https://docs.npmjs.com/cli/install). But they are subject to some externalities and most importantly with how your package manager handles the lifecycle of packages during installs.

_Note: Git references do not work with monorepo setups out of the box due to the lack of directory traversal on the syntax. E.g.:_

`npm install git@github.com:ethereumjs/ethereumjs-monorepo.git`

_One way to fetch packages remotely from GitHub before publishing is using [gitpkg.now.sh](https://gitpkg.now.sh/)._

But there's a cleaner way to manage your dependencies using Verdaccio.

### Install Verdaccio

Verdaccio is an npm registry and proxy that can be of great help to test packages locally. Check out their [Getting Started guide](https://github.com/verdaccio/verdaccio#get-started).

### Installs, hoists dependencies and builds packages

`npm install`

### Publish monorepo packages to Verdaccio

`npm publish --registry http://localhost:4873 --workspaces`

### Unpublish all monorepo packages from Verdaccio

`npm unpublish PACKAGE_NAME --registry http://localhost:4873 --force --workspace=PACKAGE_NAME`

### Setup @ethereumjs scope to local Verdaccio server

`npm config set @ethereumjs:registry http://localhost:4873`

### Teardown @ethereumjs scope to local Verdaccio server

`npm config delete @ethereumjs:registry`

## E2E testing in CI

Verdaccio is also set up in the [`e2e-tests`](https://github.com/ethereumjs/ethereumjs-monorepo/pull/1134/files) CI workflow and provides a way to install @ethereumjs
packages at an arbitrary commit in an external real-world project and run their unit
tests with it. This testing strategy is borrowed from ethereum/solidity which checks latest Solidity
against OpenZeppelin and others to keep abreast of how local changes might affect critical projects
downstream from them.

Tests like this are:

- a pre-publication sanity check that discovers how @ethereumjs performs in the wild
- useful for catching problems which are difficult to anticipate
- exposed to failure for reasons outside of @ethereumjs's control, ex: when fixes here surface bugs
  in the target.

E2E tests are constructed by cloning a real world target and using npm or yarn to replace its
existing @ethereumjs dependencies with the versions published to CI's ephemeral private npm registry.

In practice, complex projects might have several versions of @ethereumjs packages nested in
their dependency tree. It's important to coerce all of them to the virtually published versions
for the test to be valid. This can be done using Yarn's selective dependency resolutions feature.
The verdaccio publication step writes a json map of @ethereumjs package names and their
virtually published versions to `resolutions.json` in the root directory. This object can be
injected into the E2E target's package.json under the `resolutions` key and Yarn will install
new versions everywhere as expected.



================================================
FILE: config/monorepo-js/eslint.cjs
================================================
module.exports = {
  parser: '@typescript-eslint/parser',
  plugins: [
    '@typescript-eslint',
    'github',
    'implicit-dependencies',
    'import',
    'prettier',
    'simple-import-sort',
    'ethereumjs',
  ],
  env: {
    es2020: true,
    node: true,
  },
  ignorePatterns: [
    '.eslintrc.cjs',
    '.eslintrc.js',
    'benchmarks',
    'coverage',
    'dist',
    'node_modules',
    'prettier.config.js',
    'recipes',
    'rlp.cjs',
    'scripts',
    'typedoc.js',
    'webpack.config.js',
    'vitest.config.ts',
    'vitest.config.browser.ts',
    'vitest.config.unit.ts'
  ],
  extends: [
    'typestrict',
    'eslint:recommended',
    'plugin:import/recommended',
    'plugin:import/typescript',
    'prettier',
  ],
  rules: {
    'no-restricted-imports': ['error', 'ethereum-cryptography/utils.js'],
    '@typescript-eslint/consistent-type-imports': 'error',
    '@typescript-eslint/no-use-before-define': 'error',
    '@typescript-eslint/naming-convention': [
      'error',
      {
        selector: 'interface',
        format: ['PascalCase', 'camelCase'],
        custom: {
          regex: '^I[A-Z]',
          match: false,
        },
      },
    ],
    '@typescript-eslint/no-unused-vars': [
      'error',
      { argsIgnorePattern: '^_', varsIgnorePattern: '^_' },
    ],
    '@typescript-eslint/no-redeclare': ['error'],
    '@typescript-eslint/no-unnecessary-condition': 'off',
    '@typescript-eslint/prefer-nullish-coalescing': 'error',
    '@typescript-eslint/restrict-plus-operands': 'off',
    '@typescript-eslint/return-await': 'error',
    '@typescript-eslint/strict-boolean-expressions': ['error'],
    eqeqeq: 'error',
    'github/array-foreach': 'error',
    'implicit-dependencies/no-implicit': ['error', { peer: true, dev: true, optional: true }],
    'import/default': 'error',
    'import/export': 'error',
    'import/exports-last': 'off', // TODO: set to `warn` for fixing and then `error`
    'import/extensions': ['error','ignorePackages'],
    'import/first': 'error',
    'import/group-exports': 'off',
    'import/named': 'error',
    'import/namespace': 'error',
    'import/no-absolute-path': 'error',
    'import/no-anonymous-default-export': 'error',
    'import/no-cycle': 'off', // TODO: set to `warn` for fixing and then `error`
    'import/no-default-export': ['error'],
    'import/no-deprecated': 'off', // TODO: set to `warn` for fixing and then `error`
    'import/no-duplicates': 'error',
    'import/no-dynamic-require': 'off',
    'import/no-extraneous-dependencies': 'error',
    'import/no-mutable-exports': 'error',
    'import/no-namespace': 'off',
    'import/no-self-import': 'error',
    'import/no-unresolved': 'off',
    'import/no-unused-modules': 'error',
    'import/no-useless-path-segments': 'error',
    'import/no-webpack-loader-syntax': 'error',
    'import/order': [
      'error',
      {
        alphabetize: {
          order: 'asc',
        },
        groups: ['object', ['builtin', 'external'], 'parent', 'sibling', 'index', 'type'],
        'newlines-between': 'always',
      },
    ],
    'no-console': 'warn',
    'no-debugger': 'error',
    'no-dupe-class-members': 'off',
    'no-extra-semi': 'off',
    'no-redeclare': 'off',
    'no-unused-vars': 'off',
    'no-var': 'error',
    'object-shorthand': 'error',
    'prefer-const': 'error',
    'prettier/prettier': 'error',
    'simple-import-sort/exports': 'error',
    'sort-imports': ['error', { ignoreDeclarationSort: true }],
    'ethereumjs/noBuffer': 'error',
  },
  parserOptions: {
    extraFileExtensions: ['.json'],
    sourceType: 'module',
    project: './tsconfig.lint.json',
  },
  overrides: [
    {
      files: ['test/**/*.ts', 'tests/**/*.ts', 'examples/**/*.ts'],
      rules: {
        'implicit-dependencies/no-implicit': 'off',
        'import/no-extraneous-dependencies': 'off',
      },
    },
  ],
}



================================================
FILE: config/monorepo-js/MONOREPO.md
================================================
# EthereumJS - Monorepo Setup

## Development quick start

First, make sure you have the `ethereum-tests` git submodule, by running:

```sh
git submodule init
git submodule update
```

This monorepo uses [npm workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). It links the local packages together, making development a lot easier.

TLDR: Setup

```sh
npm i
```

TLDR: To update dependencies

```sh
npm run build --workspaces
```

Above is the quickest way to set you up.

### â„¹ï¸ Note for Windows users:

Windows users might run into the following error when trying to install the repo: `'.' is not recognized as an internal or external command`. To remediate for this, you can force Windows to use Git bash to run scripts (you'll need to install [Git for Windows](https://git-scm.com/download/win) for this) with the following command:

```sh
npm config set script-shell "C:\\Program Files (x86)\\git\\bin\\bash.exe"
```

If you ever need to reset this change, you can do so with this command:

```sh
npm config delete script-shell
```

---

Going down the road, there are two sets of commands: _project_ and _package-specific_ commands. You can find them at `./package.json` and `./packages/*/package.json`, respectively. Here's a breakdown:

### Project scripts â€” run from repository root

#### `npm install` (alias: `npm i`)

Adds dependencies listed in the root package.

#### `npm run build --workspaces`

Builds all monorepo packages.

To build a specific package, use `npm run build --workspace=@ethereumjs/vm`

#### `npm run clean`

Removes root and packages `node_modules` directories, and other generated files, like `coverage`, `dist` and others. This is useful to run after changing branches, to have a clean slate to work with.

#### `npm run lint --workspaces` and `npm run lint:fix --workspaces`

These scripts execute `lint` and `lint:fix` respectively, to all monorepo packages. Worth noting that there is a git hook in place that runs `npm run lint` for every `git push`. This check can be skipped using `git push [command] --no-verify`.

#### `npm run docs:build --workspaces`

Rebuilds all generated docs.

### Package scripts â€” run from `./packages/<name>`

**âš ï¸ Important: if you run `npm install` from the package directory, it will [ignore the workspace](https://github.com/npm/cli/issues/2546). Run `npm install` from the root only.**

There's a set of rather standardized commands you will find in each package of this repository.

#### `npm run build`

Uses TypeScript compiler to build source files. The resulting files can be found at `packages/<name>/dist`.

#### `npm run coverage`

Runs whatever is on `npm run test` script, capturing testing coverage information. By the end, it displays a coverage table. Additional reports can be found at `packages/<name>/coverage/`.

#### `npm run docs:build`

Generates package documentation and saves them to `./packages/<name>/docs`.

#### `npm run lint`

Checks code style according to the eslint rules.

#### `npm run lint:fix`

Fixes code style according to the rules. Differently from `npm run lint`, this command actually writes to files.

#### `npm run test`

Runs the package tests.

#### `npx vitest test/<name>/<filename>.spec.ts`

Run & watch a single test file. Vitest will run the specified test file each time code in the associated package is updated so useful for prototyping when you are working on a specific issue/test case.

_Note that the VM has several test scopes - refer to [packages/vm/package.json](https://github.com/ethereumjs/ethereumjs-monorepo/blob/master/packages/vm/package.json) for more info._



================================================
FILE: config/monorepo-js/prettier.config.js
================================================
module.exports = {
  semi: true,
  trailingComma: 'all',
  singleQuote: true,
  printWidth: 120,
  tabWidth: 2,
  arrowParens: 'avoid',
  bracketSpacing: true,
  endOfLine: 'lf',
  useTabs: false,
  proseWrap: 'preserve',
}



================================================
FILE: config/monorepo-js/tsconfig.json
================================================
{
  "compilerOptions": {
    "sourceMap": true,
    "declaration": true,
    "declarationMap": true,
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "emitDecoratorMetadata": true,
    "experimentalDecorators": true,
    "esModuleInterop": false,
    "allowSyntheticDefaultImports": true,
    "resolveJsonModule": true,
    "downlevelIteration": true,
    "strict": true,
    "target": "es2020",
    "lib": ["ES2020", "DOM"],
    "skipLibCheck": true
  }
}



================================================
FILE: config/monorepo-js/tsconfig.lint.json
================================================
{
  "extends": "./tsconfig.json",
  "include": [
    "../packages/**/src/**/*.ts",
    "../packages/**/test/**/*.ts",
    "../packages/**/examples/**/*.ts",
    "../packages/**/examples/**/*.cjs",
    "../packages/**/examples/**/*.js",
    "../packages/**/benchmarks/**/*.ts",
    "../packages/**/bin/**/*.ts"
  ],
  "compilerOptions": {
    "noEmit": true
  }
}



================================================
FILE: config/monorepo-js/tsconfig.prod.cjs.json
================================================
{
  "extends": "./tsconfig.json",
  "compilerOptions": {
    "module": "Node16",
    "moduleResolution": "node"
  }
}



================================================
FILE: config/monorepo-js/tsconfig.prod.esm.json
================================================
{
  "extends": "./tsconfig.json"
}



================================================
FILE: config/monorepo-js/typedoc.cjs
================================================
module.exports = {
  plugin: 'typedoc-plugin-markdown',
  readme: 'none',
  gitRevision: 'master',
  githubPages: false,
  excludePrivate: true,
  excludeProtected: true,
}



================================================
FILE: config/monorepo-js/vitest.config.browser.mts
================================================
import { defineConfig } from 'vitest/config'
import wasm from 'vite-plugin-wasm'

const config = defineConfig({
  test: {
    browser: {
      enabled: true,
      headless: true,
      isolate: true,
      name: 'chrome',
      fileParallelism: false,
      provider: 'webdriverio'
    },
    maxConcurrency: 1
  },
  plugins: [
    wasm(),
  ],
  optimizeDeps: {
    
    exclude: ['kzg-wasm'],
  },
})

export default config


================================================
FILE: config/monorepo-js/.c8rc.json
================================================
{
  "all": true,
  "cache": false,
  "extension": [".ts"],
  "reporter": ["lcov", "text"]
}



================================================
FILE: config/monorepo-js/cli/clean-package.sh
================================================
#!/bin/sh
set -o xtrace
rm -Rf ./dist* ./coverage *.tsbuildinfo


================================================
FILE: config/monorepo-js/cli/clean-root.sh
================================================
#!/bin/sh
set -o xtrace
rm -Rf node_modules packages/*/node_modules packages/*/dist* packages/*/coverage packages/*/package-lock.json packages/*/*.tsbuildinfo


================================================
FILE: config/monorepo-js/cli/coverage.sh
================================================
#!/bin/sh
set -o xtrace
exec c8 --all --reporter=lcov --reporter=text npm run test



================================================
FILE: config/monorepo-js/cli/lint-diff.sh
================================================
#!/bin/sh
REMOTE=$(git rev-parse --symbolic-full-name --abbrev-ref @{u})

if [ -z "$REMOTE" ]; then
    # test/tokamakì„ ì œì™¸í•˜ê³  ëª¨ë“  .js, .jsx, .ts, .tsx íŒŒì¼ì„ ê²€ì‚¬
    FILESCHANGED=$(find . -type f \( -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" \) -not -path "*/test/tokamak/*")
else
    # git diffì—ì„œë„ test/tokamak ë””ë ‰í† ë¦¬ ì œì™¸
    FILESCHANGED=$(git diff --diff-filter=d --name-only --relative $REMOTE | grep -E '\.(js|jsx|ts|tsx)$' | grep -v "test/tokamak")
fi

echo $FILESCHANGED
BLUE="\033[0;34m"
GREEN="\033[0;32m"
YELLOW="\033[0;33m"
RED="\033[0;31m"
NOCOLOR="\033[0m"
DIM="\033[2m"

blue() {
    echo "${BLUE}$1${NOCOLOR}"
}
green() {
    echo "${GREEN}$1${NOCOLOR}"
}
dim() {
    echo "${DIM}$1${NOCOLOR}"
}

dim "> eslint --format codeframe --config ./.eslintrc.cjs . \\ "
dim "\t --ext .js,.jsx,.ts,.tsx \\ "

blue "[Lint]${NOCOLOR} checking..."

if [ -z "$FILESCHANGED" ]; then
    blue "[Lint]${GREEN} DONE."
    exit
fi

eslint --format codeframe --config ./.eslintrc.cjs $FILESCHANGED

RETURN_CODE=$?

if [ $RETURN_CODE -eq 0 ]; then
    blue "[Lint]${GREEN} DONE."
else
    exit $RETURN_CODE
fi


================================================
FILE: config/monorepo-js/cli/lint-fix.sh
================================================
#!/bin/sh

BLUE="\033[0;34m"
GREEN="\033[0;32m"
YELLOW="\033[0;33m"
RED="\033[0;31m"
NOCOLOR="\033[0m"
DIM="\033[2m"

blue() {
    echo "${BLUE}$1${NOCOLOR}"
}
green() {
    echo "${GREEN}$1${NOCOLOR}"
}
dim() {
    echo "${DIM}$1${NOCOLOR}"
}

dim "> eslint --fix --config ./.eslintrc.cjs . \\ "
dim "\t --ext .js,.jsx,.ts,.tsx \\ "

blue "[Lint]${NOCOLOR} fixing..."

eslint --fix --config ./.eslintrc.cjs . --ext .js,.jsx,.ts,.tsx

RETURN_CODE=$?

if [ $RETURN_CODE -eq 0 ]; then
    blue "[Lint]${GREEN} DONE."
else
    exit $RETURN_CODE
fi


================================================
FILE: config/monorepo-js/cli/lint.sh
================================================
#!/bin/sh

BLUE="\033[0;34m"
GREEN="\033[0;32m"
YELLOW="\033[0;33m"
RED="\033[0;31m"
NOCOLOR="\033[0m"
DIM="\033[2m"

blue() {
    echo "${BLUE}$1${NOCOLOR}"
}
green() {
    echo "${GREEN}$1${NOCOLOR}"
}
dim() {
    echo "${DIM}$1${NOCOLOR}"
}

dim "> eslint --format codeframe --config ./.eslintrc.cjs . \\ "
dim "\t --ext .js,.jsx,.ts,.tsx \\ "

blue "[Lint]${NOCOLOR} checking..."

eslint --format codeframe --config ./.eslintrc.cjs . --ext .js,.jsx,.ts,.tsx

RETURN_CODE=$?

if [ $RETURN_CODE -eq 0 ]; then
    blue "[Lint]${GREEN} DONE."
else
    exit $RETURN_CODE
fi



================================================
FILE: config/monorepo-js/cli/prepublish.sh
================================================
npm run clean && npm run build && npm run test


================================================
FILE: config/monorepo-js/cli/ts-build.sh
================================================
#!/bin/sh
set -e

# Presentational variables and functions declaration.
BLUE="\033[0;34m"
GREEN="\033[0;32m"
YELLOW="\033[0;33m"
RED="\033[0;31m"
NOCOLOR="\033[0m"
DIM="\033[2m"

blue() { printf "${BLUE}$1${NOCOLOR}"; }
green() { printf "${GREEN}$1${NOCOLOR}"; }
yellow() { printf "${YELLOW}$1${NOCOLOR}"; }
red() { printf "${RED}$1${NOCOLOR}"; }
dim() { printf "${DIM}$1${NOCOLOR}"; }

# Test function declaration.
run_tests() {
    blue "[Tests] "
    echo "Running tests before build"

    echo "> npx vitest run test/**"
    printf "${BLUE}[Tests] Working... "

    npx vitest run test/**
    TEST_RESULT=$?

    if [ $TEST_RESULT -eq 0 ]; then
        green "PASSED"
        echo "\n"
        return 0
    else
        red "FAILED"
        echo "\n"
        return 1
    fi
}

# Build function declarations.
build_node() {
    blue "[Node build] "
    echo "Using tsconfig.prod.cjs.json"

    echo "> tsc --build ./tsconfig.prod.cjs.json"
    printf "${BLUE}[Node build] Working... "

    tsc --build ./tsconfig.prod.cjs.json
    green "DONE"

    echo "\n"
}

build_esm() {
    if [ -f ./tsconfig.prod.esm.json ]; then
        blue "[ESM build] "
        echo "Using tsconfig.prod.esm.json"

        echo "> tsc --build ./tsconfig.prod.esm.json"
        printf "${BLUE}[ESM build] Working... "

        tsc --build ./tsconfig.prod.esm.json
        green "DONE"
    else
        echo "Skipping ESM build (no config available)."
    fi
    echo "\n"
}

post_build_fixes() {
    blue "[Post Build Fixes]"
    if [ -f ./dist/esm/index.js ]; then
        echo "Adding ./dist/cjs/package.json"
        rm -f ./dist/cjs/package.json
        cat <<EOT >> ./dist/cjs/package.json
{
    "type": "commonjs"
}
EOT

        echo "Adding ./dist/esm/package.json"
        rm -f ./dist/esm/package.json
        cat <<EOT >> ./dist/esm/package.json
{
    "type": "module"
}
EOT
    else
        echo "Skipping post build fixes (no ESM setup yet)."
    fi
    echo "\n"
}

build_browser() {
    if [ -f ./tsconfig.browser.json ]; then
        blue "[Browser build] "
        echo "Using tsconfig.browser.json"
        echo "> tsc -p ./tsconfig.browser.json"

        blue "[Browser build] "
        printf "Working... "

        tsc -p ./tsconfig.browser.json
        RETURN_CODE=$?

        if [ $RETURN_CODE -eq 0 ]; then
            green "DONE"
        else
            exit $RETURN_CODE
        fi
    else
        dim "Skipping browser build, because no tsconfig.browser.json file is present."
    fi

    echo "\n"
}

# Argument parsing
SKIP_TESTS=false
BUILD_TARGET="default"

for arg in "$@"; do
    case $arg in
        --no-test)
            SKIP_TESTS=true
            shift
            ;;
        browser)
            BUILD_TARGET="browser"
            shift
            ;;
    esac
done

# Begin build process
if [ "$SKIP_TESTS" = false ]; then
    run_tests
    TEST_STATUS=$?

    if [ $TEST_STATUS -ne 0 ]; then
        red "[Build] Aborting build process due to test failures\n"
        exit 1
    fi
fi

if [ "$BUILD_TARGET" = "browser" ]; then
    build_browser
else
    build_node
    build_esm
    post_build_fixes
fi



================================================
FILE: config/monorepo-js/cli/ts-compile.sh
================================================
#!/bin/sh
set -o xtrace
tsc -p ./tsconfig.json --noEmit



================================================
FILE: config/typescript/eslint-config-base.js
================================================
module.exports = {
  parser: '@typescript-eslint/parser',
  extends: [
    'eslint:recommended',
    'plugin:@typescript-eslint/recommended'
  ],
  rules: {
    // ê³µí†µ ê·œì¹™
  }
};


================================================
FILE: config/typescript/tsconfig.base.json
================================================
{
  "compilerOptions": {
    "target": "es2020",
    "module": "commonjs",
    "declaration": true,
    "sourceMap": true,
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  }
}


================================================
FILE: docs/Airdrop2025.md
================================================
# How to Participate in the Tokamak ZKP Airdrop

This guide explains how to participate in the [Tokamak ZKP airdrop](https://zkp.tokamak.network) **without** using the playground app. If you prefer not to rely on the app, we encourage you to review the code, build it yourself, and generate the proof with the binary you built.

## Steps

1. **Clone the Tokamakâ€‘zkâ€‘EVM repository**
   ```bash
   git clone https://github.com/tokamak-network/Tokamak-zk-EVM.git
   cd tokamak-zk-evm
   ```

2. **Review the main script**  
   Read the [`tokamak-cli`](../tokamak-cli) script in the project root.

3. **Follow the Getting Started guide**  
   See the â€œGetting Startedâ€ section in the project [README](../README.md).

4. **Submit your proof**  
   Find the `tokamak-zk-evm-proof.zip` file in the proof directory you set in step 3, and submit it via this [Google Form](https://forms.gle/BocX6o4GZRcmr3fF7).


================================================
FILE: docs/TEAM_FORMATTING_RULES.md
================================================
# Team Formatting Rules - Summary

**Date**: November 14, 2025
**Discussion Reference**: Jake's feedback on PR #135

## TL;DR

âœ… **Compact code is preferred** - Avoid unnecessary line breaks
âœ… **120 character line width** - Up from 80
âœ… **One info unit = one line** - Don't break down simple operations
âœ… **Auto-formatting on save** - VSCode configured for team

## What Changed

### 1. Prettier Configuration

**Before**:
```json
{
  "printWidth": 80,
  "semi": false,  // Inconsistent across configs
}
```

**After**:
```json
{
  "printWidth": 120,
  "semi": true,
  "trailingComma": "all",
  "singleQuote": true,
  "arrowParens": "avoid"
}
```

### 2. New Files Created

- **`.vscode/settings.json`** - Team VSCode settings (format on save)
- **`.editorconfig`** - Cross-editor consistency
- **`.prettierignore`** - Files to skip formatting
- **`FORMATTING_GUIDE.md`** - Detailed philosophy and examples
- **`FORMATTING_SETUP.md`** - How to set up your editor

### 3. New NPM Scripts

```bash
npm run format         # Format all code
npm run format:check   # Check formatting without changes
npm run lint:fix       # Auto-fix lint issues
```

## Key Examples

### âŒ Before (Too many line breaks)

```typescript
const senderL2PubKey = jubjub.Point.BASE.multiply(
  bytesToBigInt(senderL2PrvKey),
).toBytes();

publicKeyListL2: [
  jubjub.keygen(
    setLengthLeft(
      utf8ToBytes('0x838F176D94990E06af9B57E470047F9978403195'),
      32,
    ),
  ).publicKey,
]
```

### âœ… After (Compact and readable)

```typescript
const senderL2PubKey = jubjub.Point.BASE.multiply(bytesToBigInt(senderL2PrvKey)).toBytes();

publicKeyListL2: [
  jubjub.keygen(setLengthLeft(utf8ToBytes('0x838F176D94990E06af9B57E470047F9978403195'), 32)).publicKey,
]
```

## Philosophy

### The "Information Density" Principle

> Each line should contain one **meaningful unit of information**, not be arbitrarily broken at character limits.

**Examples**:

```typescript
// GOOD: One transformation chain = one line
const hash = sha256(serialize(transaction)).toHex();

// GOOD: Each array entry = one line (even if long)
const addresses = [
  computeAddress(keygen(privateKey1).publicKey),
  computeAddress(keygen(privateKey2).publicKey),
  computeAddress(keygen(privateKey3).publicKey),
];

// GOOD: Distinct operations get separate lines
program
  .command('run')
  .description('Run synthesizer for a given transaction hash')
  .argument('<txHash>', 'Ethereum transaction hash (0x...)')
  .option('-r, --rpc <url>', 'RPC URL for Ethereum node');
```

### When to Break Lines

âœ… **DO break lines** for:
- Multiple distinct operations (like `.command()`, `.option()`)
- Complex function calls with many meaningful parameters
- Objects with multiple properties

âŒ **DON'T break lines** for:
- Simple constant parameters (like buffer size `32`)
- Method chaining for transformations (`.toBytes()`, `.toString()`)
- Single complex expressions (better to extract to variable if too complex)

## Setup Instructions

### For VSCode Users (Recommended)

1. **Install Extensions**:
   - Prettier - Code formatter
   - ESLint

2. **Reload VSCode** - Settings will apply automatically from `.vscode/settings.json`

3. **Format on save** is enabled by default

### For Other Editors

1. Install Prettier and ESLint plugins
2. Configure them to use workspace settings
3. The `.editorconfig` will handle basic settings

### Manual Commands

```bash
# Format files
npm run format

# Check formatting
npm run format:check

# Fix lint errors
npm run lint:fix
```

## Team Agreement

âœ… We prioritize **readability through compactness**
âœ… We use **information density** as our guide
âœ… We trust **individual judgment** for specific cases
âœ… We let **automated tools** handle the details
âŒ We don't break lines just to satisfy character limits
âŒ We don't apply formatting rules blindly

## Discussion Points for Meeting

1. âœ… **Approved**: 120 character line width
2. âœ… **Approved**: Compact style over aggressive line breaking
3. âœ… **Approved**: Auto-format on save in VSCode
4. ğŸ¤” **To discuss**: Pre-commit hooks (optional)
5. ğŸ¤” **To discuss**: CI/CD formatting checks

## Questions?

- **"What if a line is too long?"** - Use judgment. Extract to a variable if it's truly complex.
- **"What about existing code?"** - We'll format gradually as we touch files, not all at once.
- **"What if I disagree with a format?"** - Discuss with team and update this guide.

## Resources

- **`FORMATTING_GUIDE.md`** - Detailed guide with many examples
- **`FORMATTING_SETUP.md`** - Technical setup instructions
- **`.prettierrc`** - Actual Prettier configuration

---

**Next Steps**:
1. Team reviews this document
2. Everyone sets up their editor (5 minutes)
3. Start using the new rules for new code
4. Gradually update existing code as we touch it

**Remember**: Tools serve us, we don't serve tools. These rules help us write better code, not restrict us.




================================================
FILE: packages/.dockerignore
================================================
# Git
.git
.gitignore

# Node
**/node_modules
npm-debug.log
yarn.lock
pnpm-lock.yaml

# Rust / Build artifacts
**/target
**/dist
**/build

# IDE / OS
.vscode
.idea
.DS_Store
Thumbs.db



================================================
FILE: packages/backend/README.md
================================================
# Tokamak-zk-EVM/backend

## What are the backend algorithms

The three algorithms, "setup", "prove", and "verify", are rust implementation of the backend algorithms defined in [the Tokamak zk-SNARK manuscript](https://eprint.iacr.org/2024/507).
They take a zkp circuit specialized for a transaction as their input. The circuit should be pre-processed by [the frontend compilers](../frontend/).
More specifically,
- setup/trusted-setup: it takes a library of subcircuits generated by [the QAP-compiler](../frontend/qap-compiler/) as input and generates a reference string and commitments to the subcircuit library.
- prove: it takes the subcircuit library and the transaction-specific details generated by [the Synthesizer](../frontend/synthesizer) as input and generates a zk proof.
- verify: it takes public inputs, the setup outputs, and the zk proof as input and accept or reject the proof.

## How to use?
### Prerequisite
- Install Node.js â€“ https://nodejs.org/
- Install Circom â€“ https://docs.circom.io/getting-started/installation/
- Install Rust â€“ https://www.rust-lang.org/tools/install
- Install CMake â€“ https://cmake.org/download/
- Make sure that you have run [Synthesizer](../frontend/synthesizer/README.md).
- Make sure that you have installed [CodeLLDB](https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb) on your VS Code as an extension.

### Run the backend in a debugging mode with CodeLLDB
- Go to the `Run and Debug` menu on the left side of your VS Code.
- Open the dropdown box at the top and select the algorithm you want to debug.
- Click the `Start Debugging (F5)` button.

## Description for the Setup input and output
### Input
The setup (both trusted- and MPC-setups) requires the only input from the QAP-compiler, which is [a library of subcircuits](../frontend/qap-compiler/subcircuits/library). This input would be more likely fixed. It will be changed whenever there is a change in the EVM's spec.
### Output
You can find the only setup output from [the "output" folder](./setup/trusted-setup/output), named "combined_sigma.json", which is denoted and defined as $\mathbb{\sigma}$ in the manuscript. The output would rarely change, as it depends on the EVM sepc.

## Description for the Prove input and output
### Input
The prove takes its input from the following three paths:
- The setup outputs,
- [A library of subcircuits](../frontend/qap-compiler/subcircuits/library) generated by the QAP-compiler, and
- transaction-specific details such as the subcircuits combination, wire map, instances, and witnesses analyzed by the Synthesizer.
### Output
- Zk proof of correct execution of a transaction

## Description for the Verify input and output
### Input
The verify takes its input from the following three paths:
- The setup outputs,
- The zk proof, and
- [Public inputs](../frontend/synthesizer/examples/outputs/publicInstance.json) provided by the Synthesizer.
### Output
- It will display a message "true", if the zk proof is verified.

## Contributing
We welcome contributions! Please see our [Contributing Guidelines](../../CONTRIBUTING.md) for details.

## Original contribution
- [JehyukJang](https://github.com/JehyukJang): Algorithms design, analysis, implementation, and optimization.
- [jason hwang](https://github.com/cd4761): Parallel computing-relevant optimization of the implementations.

## License
[MPL-2.0]



================================================
FILE: packages/backend/Cargo.toml
================================================
[workspace]
members = [
    "libs",
    "setup/trusted-setup",
    "setup/mpc-setup",
    "prove",
    "verify/verify-rust",
    "verify/preprocess"
]

[workspace.package]
edition = "2021"
version = "0.1.0"
license = "MIT OR Apache-2.0"
authors = ["Tokamak Network"]
repository = "https://github.com/tokamak-network/tokamak-zk-evm"
homepage = "https://github.com/tokamak-network/tokamak-zk-evm"

[workspace.dependencies]
icicle-runtime = { git = "https://github.com/ingonyama-zk/icicle.git", tag = "v3.8.0", package = "icicle-runtime" }
icicle-core = { git = "https://github.com/ingonyama-zk/icicle.git", tag = "v3.8.0", package = "icicle-core" }
icicle-hash = { git = "https://github.com/ingonyama-zk/icicle.git", tag = "v3.8.0", package = "icicle-hash" }
icicle-bls12-381 = { git = "https://github.com/ingonyama-zk/icicle.git", tag = "v3.8.0", package = "icicle-bls12-381"}
ark-bls12-381 = "0.5.0"
ark-ec = "0.5.0"
ark-ff = "0.5.0"
libs = { path = "libs" }
prove = { path = "prove" }
verify = { path = "verify/verify-rust" }
preprocess = { path = "verify/preprocess" }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
byteorder = "1.4"
num-bigint = "0.4"
num-traits = "0.2"
rayon = "1.7"
rand = "0.8"
bincode = "1.3"
hex = "0.4"


================================================
FILE: packages/backend/Dockerfile
================================================
# # Use the specified base image
# FROM nvidia/cuda:12.2.0-devel-ubuntu22.04

# Use Ubuntu as base image (No CUDA)
FROM ubuntu:22.04

# Update and install dependencies
RUN apt-get update && apt-get install -y \
    bash \
    cmake \
    protobuf-compiler \
    curl \
    build-essential \
    git \
    clang \
    libclang-dev \
    lldb \
    && rm -rf /var/lib/apt/lists/*

RUN echo 'export PS1="\w\$ "' >> /root/.bashrc

# Install Rust
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"



# Install Golang
ENV GOLANG_VERSION 1.21.1
RUN curl -L https://go.dev/dl/go${GOLANG_VERSION}.linux-amd64.tar.gz | tar -xz -C /usr/local
ENV PATH="/usr/local/go/bin:${PATH}"

# Set the working directory in the container
WORKDIR /app

# Copy the content of the local directory to the working directory
COPY . .

# Specify the default command for the container
CMD ["bash"]


================================================
FILE: packages/backend/download-ICICLE-lib.sh
================================================
#!/usr/bin/env bash
set -Eeuo pipefail
IFS=$'\n\t'

TARGET="external-lib"
BASE_URL="https://github.com/ingonyama-zk/icicle/releases/download/v3.8.0"

OS_NAME="$(uname -s)"
BACKEND_PATH=""
COMMON_TARBALL=""
BACKEND_TARBALL=""
OS_DESC=""

if [[ "$OS_NAME" == "Darwin" ]]; then
  OS_DESC="macOS"
  BACKEND_PATH="mac"
  COMMON_TARBALL="icicle_3_8_0-macOS.tar.gz"
  BACKEND_TARBALL="icicle_3_8_0-macOS-Metal.tar.gz"
elif [[ "$OS_NAME" == "Linux" ]]; then
  LINUX_ID=""
  LINUX_VERSION_ID=""

  if [[ -f /etc/os-release ]]; then
    # shellcheck disable=SC1091
    . /etc/os-release
    LINUX_ID="${ID:-}"
    LINUX_VERSION_ID="${VERSION_ID:-}"
  fi

  if [[ -z "$LINUX_VERSION_ID" ]] && command -v lsb_release >/dev/null 2>&1; then
    LINUX_VERSION_ID="$(lsb_release -rs)"
    LINUX_ID="${LINUX_ID:-$(lsb_release -is 2>/dev/null | tr '[:upper:]' '[:lower:]')}"
  fi

  LINUX_MAJOR="${LINUX_VERSION_ID%%.*}"

  if [[ "$LINUX_ID" != "ubuntu" && ! "${ID_LIKE:-}" =~ ubuntu ]]; then
    echo "Unsupported Linux distribution: ${LINUX_ID:-unknown}. Supported: Ubuntu and its derivatives (20.x or 22.x)." >&2
    exit 1
  fi

  case "$LINUX_MAJOR" in
    22)
      OS_DESC="Ubuntu ${LINUX_VERSION_ID}"
      BACKEND_PATH="linux22"
      COMMON_TARBALL="icicle_3_8_0-ubuntu22.tar.gz"
      BACKEND_TARBALL="icicle_3_8_0-ubuntu22-cuda122.tar.gz"
      ;;
    20)
      OS_DESC="Ubuntu ${LINUX_VERSION_ID}"
      BACKEND_PATH="linux20"
      COMMON_TARBALL="icicle_3_8_0-ubuntu20.tar.gz"
      BACKEND_TARBALL="icicle_3_8_0-ubuntu20-cuda122.tar.gz"
      ;;
    *)
      echo "Unsupported Ubuntu version: ${LINUX_VERSION_ID:-unknown}. Supported: 20.x or 22.x." >&2
      exit 1
      ;;
  esac
else
  echo "Unsupported OS: ${OS_NAME}. Supported: macOS, Ubuntu 20.x, Ubuntu 22.x." >&2
  exit 1
fi

COMMON_URL="${BASE_URL}/${COMMON_TARBALL}"
BACKEND_URL="${BASE_URL}/${BACKEND_TARBALL}"

command -v curl >/dev/null 2>&1 || { echo "curl is required but not found"; exit 1; }
command -v tar  >/dev/null 2>&1 || { echo "tar is required but not found"; exit 1; }

echo "[*] Platform detected: ${OS_DESC}"
echo "[*] Downloading backend package..."
curl -fL --retry 3 -o "$BACKEND_TARBALL" "$BACKEND_URL"
echo "[*] Downloading common runtime package..."
curl -fL --retry 3 -o "$COMMON_TARBALL" "$COMMON_URL"

echo "[*] Extracting packages..."
tar -xzf "$BACKEND_TARBALL"
tar -xzf "$COMMON_TARBALL"

echo "[*] Installing to ${TARGET}/${BACKEND_PATH} ..."
mkdir -p "${TARGET}/${BACKEND_PATH}"
cp -r icicle/* "${TARGET}/${BACKEND_PATH}"

echo "[*] Cleaning up temporary files..."
rm -rf "$BACKEND_TARBALL" "$COMMON_TARBALL" icicle

echo "âœ… ICICLE external library installed to ${TARGET}/${BACKEND_PATH}."



================================================
FILE: packages/backend/libs/Cargo.toml
================================================
[package]
name = "libs"
version = { workspace = true }
edition = { workspace = true }
license = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }

[dependencies]
bincode = { workspace = true }
icicle-runtime = { workspace = true }
icicle-core = { workspace = true }
icicle-bls12-381 = { workspace = true }
icicle-hash = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
num-bigint = { workspace = true }
rayon = { workspace = true }
criterion = "0.3"
hex = "0.4.3"
ark-bls12-381 = { workspace = true }
ark-ec = { workspace = true }
ark-ff = { workspace = true }
rand = { workspace = true }

[[bench]]
name = "outer_product_bench"
harness = false

[[bench]]
name = "matrix_matrix_mul_bench"
harness = false

[features]
default = []
testing-mode = []



================================================
FILE: packages/backend/libs/benches/benchmarks.rs
================================================
// use criterion::{black_box, criterion_group, criterion_main, Criterion};
// use icicle_bls12_381::polynomials::DensePolynomial;
// use icicle_core::polynomials::UnivariatePolynomial;
// use std::time::{Duration, Instant};
// use icicle_bls12_381::curve::{ScalarField, ScalarCfg};
// use icicle_runtime::memory::{HostSlice};
// use icicle_core::traits::{Arithmetic, FieldConfig, FieldImpl, GenerateRandom};
// use icicle_runtime::Device;

// // Original implementation (paste.txt)
// use rust_math::dense_ext::{BivariatePolynomial as originalBipolynomial, DensePolynomialExt as originalDensePolynomial};
// use rust_math::dense_ext_refact::{BivariatePolynomial as optimizedBipolynomial, DensePolynomialExt as optimizedDensePolynomial};

// fn generate_random_polynomial(x_size: usize, y_size: usize) -> Vec<ScalarField> {
//     let size = x_size * y_size;
//     ScalarCfg::generate_random(size)
// }

// fn benchmark_multiplication(c: &mut Criterion) {
   
//     let mut group = c.benchmark_group("resize");
//     for size in [64, 128, 256, 512].iter() {
        
//         group.bench_function(format!("original_mul_{}", size), |b| {
//             // ì…ë ¥ ë‹¤í•­ì‹ì˜ í¬ê¸°ë¥¼ 2ë°°ë¡œ ì„¤ì •
//             let input_size = *size;
//             let output_size = input_size * 2;
            
//             let coeffs1 = generate_random_polynomial(input_size, input_size);
//             let coeffs2 = generate_random_polynomial(input_size, input_size);
//             let coeffs1_slice = HostSlice::from_slice(&coeffs1);
//             let coeffs2_slice = HostSlice::from_slice(&coeffs2);
            
//             // ë” í° í¬ê¸°ë¡œ ì´ˆê¸°í™”
//             let poly1 = originalDensePolynomial::from_coeffs(coeffs1_slice, output_size, output_size);
//             let poly2 = originalDensePolynomial::from_coeffs(coeffs2_slice, output_size, output_size);
            
//             b.iter(|| {
//                 black_box(&poly1 * &poly2);
//             });
//         });
        

//         group.bench_function(format!("optimized_mul_{}", size), |b| {
//             let input_size = *size;
//             let output_size = input_size * 2;
            
//             let coeffs1 = generate_random_polynomial(input_size, input_size);
//             let coeffs2 = generate_random_polynomial(input_size, input_size);
//             let coeffs1_slice = HostSlice::from_slice(&coeffs1);
//             let coeffs2_slice = HostSlice::from_slice(&coeffs2);

//             // let poly1 = originalDensePolynomial::from_coeffs_fixed_size(coeffs1_slice, output_size, output_size);
//             // let poly2 = originalDensePolynomial::from_coeffs_fixed_size(coeffs2_slice, output_size, output_size);
            
//             let poly3 = optimizedDensePolynomial::from_coeffs(coeffs1_slice, output_size, output_size);
//             let poly4 = optimizedDensePolynomial::from_coeffs(coeffs2_slice, output_size, output_size);
            
//             b.iter(|| {
//                 black_box(&poly3 * &poly4);
//             });
//         });
//     }
//     group.finish();
// }

// fn benchmark_resize(c: &mut Criterion) {
//     let mut group = c.benchmark_group("resize");
    
//     for size in [64, 128, 256, 512].iter() {
//         group.bench_function(format!("original_resize_{}", size), |b| {
//             let coeffs = generate_random_polynomial(*size, *size);
//             let coeffs_slice = HostSlice::from_slice(&coeffs);
//             let mut poly = originalDensePolynomial::from_coeffs(coeffs_slice, *size, *size);
            
//             b.iter(|| {
//                 let mut test_poly = poly.clone();
//                 test_poly.resize(black_box(*size * 2), black_box(*size * 2));
//             });
//         });

//         group.bench_function(format!("optimized_resize_{}", size), |b| {
//             let coeffs = generate_random_polynomial(*size, *size);
//             let coeffs_slice = HostSlice::from_slice(&coeffs);
//             let mut poly = optimizedDensePolynomial::from_coeffs(coeffs_slice, *size, *size);
            
//             b.iter(|| {
//                 let mut test_poly = poly.clone();
//                 test_poly.resize(black_box(*size * 2), black_box(*size * 2));
//             });
//         });
//     }
    
//     group.finish();
// }

// fn benchmark_div_by_vanishing(c: &mut Criterion) {
//     let mut group = c.benchmark_group("div_by_vanishing");
    
//     for size in [32, 64, 128].iter() {
//         group.bench_function(format!("original_div_{}", size), |b| {
//             let coeffs = generate_random_polynomial(*size * 2, *size * 2);
//             let coeffs_slice = HostSlice::from_slice(&coeffs);
//             let poly = originalDensePolynomial::from_coeffs(coeffs_slice, *size * 2, *size * 2);
            
//             b.iter(|| {
//                 black_box(poly.div_by_vanishing(*size as i64, *size as i64));
//             });
//         });

//         group.bench_function(format!("optimized_div_{}", size), |b| {
//             let coeffs = generate_random_polynomial(*size * 2, *size * 2);
//             let coeffs_slice = HostSlice::from_slice(&coeffs);
//             let poly = optimizedDensePolynomial::from_coeffs(coeffs_slice, *size * 2, *size * 2);
            
//             b.iter(|| {
//                 black_box(poly.div_by_vanishing(*size as i64, *size as i64));
//             });
//         });
//     }
    
//     group.finish();
// }

// criterion_group!(
//     benches,
//     // benchmark_resize,
//     // benchmark_multiplication,
//     // benchmark_div_by_vanishing,
//     // benchmark_find_degree
// );
// criterion_main!(benches);

// #[cfg(test)]
// mod tests {
//     use super::*;

//     #[test]
//     fn test_multiplication_correctness() {
//         let size = 32;
//         let input_size = size;
//         let output_size = size * 2;
        
//         let coeffs1 = generate_random_polynomial(input_size, input_size);
//         let coeffs2 = generate_random_polynomial(input_size, input_size);
        
//         let coeffs1_slice = HostSlice::from_slice(&coeffs1);
//         let coeffs2_slice = HostSlice::from_slice(&coeffs2);
        
//         let original_poly1 = originalDensePolynomial::from_coeffs_fixed_size(coeffs1_slice.clone(), output_size, output_size);
//         let original_poly2 = originalDensePolynomial::from_coeffs_fixed_size(coeffs2_slice.clone(), output_size, output_size);
        
//         let optimized_poly1 = optimizedDensePolynomial::from_coeffs_fixed_size(coeffs1_slice, output_size, output_size);
//         let optimized_poly2 = optimizedDensePolynomial::from_coeffs_fixed_size(coeffs2_slice, output_size, output_size);
        
//         let original_result = &original_poly1 * &original_poly2;
//         let optimized_result = &optimized_poly1 * &optimized_poly2;
        
//         assert_eq!(original_result.x_degree, optimized_result.x_degree);
//         assert_eq!(original_result.y_degree, optimized_result.y_degree);
//     }

//     #[test]
//     fn test_resize_correctness() {
//         let size = 64;
//         let coeffs = generate_random_polynomial(size, size);
//         let coeffs_slice = HostSlice::from_slice(&coeffs);
        
//         let mut original_poly = originalDensePolynomial::from_coeffs(coeffs_slice.clone(), size, size);
//         let mut optimized_poly = optimizedDensePolynomial::from_coeffs(coeffs_slice, size, size);
        
//         original_poly.resize(size * 2, size * 2);
//         optimized_poly.resize(size * 2, size * 2);
        
//         assert_eq!(original_poly.x_size, optimized_poly.x_size);
//         assert_eq!(original_poly.y_size, optimized_poly.y_size);
//     }
// }


================================================
FILE: packages/backend/libs/benches/matrix_matrix_mul_bench.rs
================================================
use icicle_bls12_381::curve::{ScalarCfg, ScalarField};
use icicle_core::traits::FieldImpl;
use icicle_core::vec_ops::{VecOps, VecOpsConfig};
use icicle_runtime::memory::HostSlice;
use criterion::{criterion_group, criterion_main, Criterion};
use libs::vector_operations::{transpose_inplace, matrix_matrix_mul};
use libs::utils::check_device;

fn _repeat_extend(v: &mut Vec<ScalarField>, n: usize) {
    let original = v.clone();
    for _ in 1..n {
        v.extend(original.iter().cloned());
    }
}

fn matrix_matrix_mul_original(lhs_mat: &[ScalarField], rhs_mat: &[ScalarField], m: usize, n:usize, l:usize, res_mat: &mut [ScalarField]) {
    if lhs_mat.len() != m * n || rhs_mat.len() != n * l || res_mat.len() != m * l {
        panic!("Incorrect sizes for the matrix multiplication")
    }
    if lhs_mat.is_empty() || rhs_mat.is_empty() {
        res_mat.fill(ScalarField::zero());
        return;
    }
    // size of LHS: m-by-n
    // size of RHS: n-by-l
    // Extending LHS and RHS. E.g., say LHS = [r1; r2; r3] and RHS = [c1, c2] with m=3, l=2, where r_i are row vectors, and c_i are column vectors.
    // Extended_LHS = [r1, r1, r2, r2, r3, r3], and
    // Extended_RHS = [c1^T, c2^T c1^T, c2^T; c1^T, c2^T].
    // Then, LHS*RHS is a batched inner product of Extended_LHS and Extended_RHS.
    
    let mut ext_lhs_mat = lhs_mat.to_vec();
    transpose_inplace(&mut ext_lhs_mat, m, n);
    _repeat_extend(&mut ext_lhs_mat, l);
    transpose_inplace(&mut ext_lhs_mat, l*n, m);
    
    let mut ext_rhs_mat = rhs_mat.to_vec();
    transpose_inplace(&mut ext_rhs_mat, n, l);
    _repeat_extend(&mut ext_rhs_mat, m);

    let mut vec_ops_cfg = VecOpsConfig::default();
    let mut mul_res_vec = vec![ScalarField::zero(); m*n*l];
    let mul_res_buff = HostSlice::from_mut_slice(&mut mul_res_vec);
    ScalarCfg::mul(HostSlice::from_slice(&ext_lhs_mat), HostSlice::from_slice(&ext_rhs_mat), mul_res_buff, &vec_ops_cfg).unwrap();
    vec_ops_cfg.batch_size = (m * l) as i32;
    vec_ops_cfg.columns_batch = false;
    let res = HostSlice::from_mut_slice(res_mat);
    ScalarCfg::sum(mul_res_buff, res, &vec_ops_cfg).unwrap();
}

fn bench_matrix_matrix_mul(c: &mut Criterion) {
    check_device();
    let m = 1 << 10;
    let n = 1 << 10;
    let l = 5;
    let mut lhs_mat = vec![ScalarField::zero(); m * n];
    let mut rhs_mat = vec![ScalarField::zero(); n * l];
    let mut res_mat = vec![ScalarField::zero(); m * l];

    c.bench_function("matrix_matrix_mul_original", |b| {
        b.iter(|| {
            matrix_matrix_mul_original(&lhs_mat, &rhs_mat, m, n, l, &mut res_mat);
        })
    });

    c.bench_function("matrix_matrix_mul", |b| {
        b.iter(|| {
            matrix_matrix_mul(&lhs_mat, &rhs_mat, m, n, l, &mut res_mat);
        })
    });
}

criterion_group!(benches, bench_matrix_matrix_mul);
criterion_main!(benches);


================================================
FILE: packages/backend/libs/benches/outer_product_bench.rs
================================================
// benches/outer_product_bench.rs
use criterion::{criterion_group, criterion_main, Criterion};
use icicle_bls12_381::curve::{ScalarField};
use icicle_core::traits::FieldImpl;

use libs::vector_operations::{outer_product_two_vecs, outer_product_two_vecs_rayon};

fn bench_outer_products(c: &mut Criterion) {
    // í…ŒìŠ¤íŠ¸í•  ë²¡í„° í¬ê¸° ì„¤ì • (ì˜ˆ: 512 x 512)
    let col_len = 512;
    let row_len = 512;
    let total = col_len * row_len;

    // ì˜ˆì œ ì…ë ¥ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ì¼ì •í•œ ê·œì¹™ì„ ê°€ì§„ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    let col_vec: Box<[ScalarField]> = (0..col_len)
        .map(|i| ScalarField::from_u32((i % 100) as u32 + 1))
        .collect::<Vec<_>>()
        .into_boxed_slice();

    let row_vec: Box<[ScalarField]> = (0..row_len)
        .map(|i| ScalarField::from_u32(((i + 1) % 100) as u32 + 1))
        .collect::<Vec<_>>()
        .into_boxed_slice();

    // ê²°ê³¼ë¥¼ ì €ì¥í•  ë²„í¼ë“¤ì„ í• ë‹¹í•©ë‹ˆë‹¤.
    let mut res_seq: Box<[ScalarField]> = vec![ScalarField::zero(); total].into_boxed_slice();
    let mut res_par: Box<[ScalarField]> = vec![ScalarField::zero(); total].into_boxed_slice();

    // ìˆœì°¨ outer product í•¨ìˆ˜ ë²¤ì¹˜ë§ˆí¬
    c.bench_function("outer_product_two_vecs", |b| {
        b.iter(|| {
            outer_product_two_vecs(&col_vec, &row_vec, &mut res_seq);
        })
    });

    // Rayonì„ ì‚¬ìš©í•œ outer product í•¨ìˆ˜ ë²¤ì¹˜ë§ˆí¬
    c.bench_function("outer_product_two_vecs_rayon", |b| {
        b.iter(|| {
            outer_product_two_vecs_rayon(&col_vec, &row_vec, &mut res_par);
        })
    });
}

criterion_group!(benches, bench_outer_products);
criterion_main!(benches);



================================================
FILE: packages/backend/libs/src/lib.rs
================================================
#![allow(non_snake_case)]
pub mod bivariate_polynomial;
pub mod vector_operations;
pub mod iotools;
pub mod group_structures;
pub mod polynomial_structures;
pub mod field_structures;
pub mod utils;

#[doc(hidden)]
pub mod tests;



================================================
FILE: packages/backend/libs/src/tests.rs
================================================
use icicle_bls12_381::curve::{ScalarField, ScalarCfg};
use icicle_core::traits::{Arithmetic, FieldConfig, FieldImpl, GenerateRandom};
use icicle_runtime::memory::{HostOrDeviceSlice, HostSlice};
use std::cmp;
use std::time::Instant;

// Assuming the implementation of DensePolynomialExt and BivariatePolynomial is already available
// This mod tests can be placed in a separate file

#[cfg(test)]
mod msm_vs_rayon_tests {
    use super::*;
    use icicle_bls12_381::curve::{CurveCfg, G1Affine, G1Projective, ScalarCfg, ScalarField};
    use crate::iotools::{from_coef_vec_to_g1serde_vec, from_coef_vec_to_g1serde_vec_msm};
    use crate::group_structures::G1serde;
    use icicle_core::curve::Curve;

    #[test]
    fn cpu_and_gpu_versions_produce_same() {
        let n = 256;
        let coef_vec: Vec<ScalarField> = ScalarCfg::generate_random(n);
        // 3) generator point
        let gen = CurveCfg::generate_random_affine_points(n)[0];

        
        let mut res_cpu = vec![G1serde(G1Affine::zero()); n];
        let mut res_gpu = vec![G1serde(G1Affine::zero()); n];

        
        from_coef_vec_to_g1serde_vec(&coef_vec, &gen, &mut res_cpu);
        from_coef_vec_to_g1serde_vec_msm(&Box::from(coef_vec.clone().into_boxed_slice()), &gen, &mut res_gpu);

        assert_eq!(res_cpu.len(), res_gpu.len());
        for i in 0..n {
            assert_eq!(
                res_cpu[i].0, 
                res_gpu[i].0,
                "mismatch at index {}: cpu={:?}, gpu={:?}",
                i, res_cpu[i].0, res_gpu[i].0
            );
        }
    }
}


#[cfg(test)]
mod tests {
    use icicle_core::ntt;

    use super::*;
    use crate::vector_operations::{*};
    use crate::bivariate_polynomial::{DensePolynomialExt, BivariatePolynomial};

    // Helper function: Create a simple 2D polynomial
    fn create_simple_polynomial() -> DensePolynomialExt {
        // Simple 2x2 polynomial: 1 + 2x + 3y + 4xy (coefficient matrix: [[1, 3], [2, 4]])
        let coeffs = vec![
            ScalarField::from_u32(1),  // Constant term
            ScalarField::from_u32(3),  // x coefficient
            ScalarField::from_u32(2),  // y coefficient
            ScalarField::from_u32(4),  // xy coefficient
        ];
        DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs), 2, 2)
    }

    fn create_larger_polynomial() -> DensePolynomialExt {
        // Create a 4x4 polynomial with random coefficients
        let size = 16; // 4x4
        let coeffs = ScalarCfg::generate_random(size);
        DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs), 4, 4)
    }

    // Create a univariate polynomial in x
    fn create_univariate_x_polynomial() -> DensePolynomialExt {
        // Polynomial in x: 1 + 2x + 3x^2
        let coeffs = vec![
            ScalarField::from_u32(1),
            ScalarField::from_u32(2),
            ScalarField::from_u32(3),
            ScalarField::from_u32(0),
        ];
        DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs), 4, 1)
    }

    // Create a univariate polynomial in y
    fn create_univariate_y_polynomial() -> DensePolynomialExt {
        // Polynomial in y: 1 + 2y + 3y^2
        let mut coeffs = vec![ScalarField::from_u32(0); 16];
        coeffs[0] = ScalarField::from_u32(1);  // Constant
        coeffs[4] = ScalarField::from_u32(2);  // y
        coeffs[8] = ScalarField::from_u32(3);  // y^2
        
        DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs), 4, 4)
    }

    #[test]
    fn test_from_coeffs() { // pass
        let poly = create_simple_polynomial();
        assert_eq!(poly.x_degree, 1);
        assert_eq!(poly.y_degree, 1);
        assert_eq!(poly.x_size, 2);
        assert_eq!(poly.y_size, 2);

        // Verify coefficients
        assert_eq!(poly.get_coeff(0, 0), ScalarField::from_u32(1));
        assert_eq!(poly.get_coeff(1, 0), ScalarField::from_u32(2));
        assert_eq!(poly.get_coeff(0, 1), ScalarField::from_u32(3));
        assert_eq!(poly.get_coeff(1, 1), ScalarField::from_u32(4));
    }
    #[test]
    fn test_from_evals() {
        let x_size = 2048;
        let y_size = 1;
        let evals = ScalarCfg::generate_random(x_size * y_size);
        
        let poly = DensePolynomialExt::from_rou_evals(
            HostSlice::from_slice(&evals),
            x_size,
            y_size,
            None,
            None
        );
        let mut recoevered_evals = vec![ScalarField::zero(); x_size * y_size];
        let buff = HostSlice::from_mut_slice(&mut recoevered_evals);
        poly.to_rou_evals(None, None, buff);
        
        let mut flag = true;
        for i in 0..x_size * y_size {
            if !evals[i].eq(&recoevered_evals[i]) {
                flag = false;
            }
        }
        assert!(flag);
    }

    #[test]
    fn test_add() { // pass
        let poly1 = create_simple_polynomial();
        let poly2 = create_simple_polynomial();
        
        let result = &poly1 + &poly2;
        
        // Verify addition results
        assert_eq!(result.get_coeff(0, 0), ScalarField::from_u32(1) + ScalarField::from_u32(1));  // 1+1
        assert_eq!(result.get_coeff(1, 0), ScalarField::from_u32(2) + ScalarField::from_u32(2));  // 2+2
        assert_eq!(result.get_coeff(0, 1), ScalarField::from_u32(3) + ScalarField::from_u32(3));  // 3+3
        assert_eq!(result.get_coeff(1, 1), ScalarField::from_u32(4) + ScalarField::from_u32(4));  // 4+4
    }

    #[test]
    fn test_sub() { // pass
        let poly1 = create_simple_polynomial();
        // Create a polynomial with different coefficients
        let coeffs2 = vec![
            ScalarField::from_u32(5),  // Constant
            ScalarField::from_u32(2),  // x
            ScalarField::from_u32(1),  // y
            ScalarField::from_u32(3),  // xy
        ];
        let poly2 = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs2), 2, 2);
        
        let result = &poly1 - &poly2;
        
        // Verify subtraction results
        assert_eq!(result.get_coeff(0, 0), ScalarField::from_u32(1) - ScalarField::from_u32(5));
        assert_eq!(result.get_coeff(1, 0), ScalarField::from_u32(2) - ScalarField::from_u32(1));
        assert_eq!(result.get_coeff(0, 1), ScalarField::from_u32(3) - ScalarField::from_u32(2));
        assert_eq!(result.get_coeff(1, 1), ScalarField::from_u32(4) - ScalarField::from_u32(3));
    }

    #[test]
    fn test_mul_scalar() { // pass
        let x_size = 2usize.pow(10);
        let y_size = 2usize.pow(5);
        let poly = DensePolynomialExt::from_coeffs(
            HostSlice::from_slice(&ScalarCfg::generate_random(x_size * y_size)), 
            x_size, 
            y_size
        );
        let scalar = ScalarCfg::generate_random(1)[0];
        
        let result1 = &poly * &scalar;
        let result2 = &scalar * &poly;
        
        // Verify scalar multiplication results
        let x = ScalarCfg::generate_random(1)[0];
        let y = ScalarCfg::generate_random(1)[0];
        assert_eq!(result1.eval(&x, &y), poly.eval(&x, &y) * scalar);
        assert_eq!(result1.eval(&x, &y), result2.eval(&x, &y));
    }

    #[test]
    fn test_sub_scalar() { // pass
        let x_size = 2usize.pow(10);
        let y_size = 2usize.pow(5);
        let poly = DensePolynomialExt::from_coeffs(
            HostSlice::from_slice(&ScalarCfg::generate_random(x_size * y_size)), 
            x_size, 
            y_size
        );
        let scalar = ScalarCfg::generate_random(1)[0];
        let result1 = &poly - &scalar;
        let result2 = &scalar - &poly;

        // Verify scalar multiplication results
        let x = ScalarCfg::generate_random(1)[0];
        let y = ScalarCfg::generate_random(1)[0];
        assert_eq!(result1.eval(&x, &y), poly.eval(&x, &y) - scalar);
        assert_eq!(result2.eval(&x, &y), scalar - poly.eval(&x, &y));  
    }

    #[test]
    fn test_add_scalar() { // pass
        let x_size = 2usize.pow(10);
        let y_size = 2usize.pow(5);
        let poly = DensePolynomialExt::from_coeffs(
            HostSlice::from_slice(&ScalarCfg::generate_random(x_size * y_size)), 
            x_size, 
            y_size
        );
        let scalar = ScalarCfg::generate_random(1)[0];
        let result1 = &poly + &scalar;
        let result2 = &scalar + &poly;
        
        // Verify scalar multiplication results
        let x = ScalarCfg::generate_random(1)[0];
        let y = ScalarCfg::generate_random(1)[0];
        assert_eq!(result1.eval(&x, &y), poly.eval(&x, &y) + scalar);
        assert_eq!(result1.eval(&x, &y), result2.eval(&x, &y));
    }

    #[test]
    fn test_neg() { // pass
        let poly = create_simple_polynomial();
        let result = -&poly;
        
        // Verify negation results
        assert_eq!(result.get_coeff(0, 0), ScalarField::from_u32(0) - ScalarField::from_u32(1));
        assert_eq!(result.get_coeff(1, 0), ScalarField::from_u32(0) - ScalarField::from_u32(2));
        assert_eq!(result.get_coeff(0, 1), ScalarField::from_u32(0) - ScalarField::from_u32(3));
        assert_eq!(result.get_coeff(1, 1), ScalarField::from_u32(0) - ScalarField::from_u32(4));
    }


    #[test]
    fn test_get_univariate_polynomial() { // pass
        // Create a polynomial with predictable coefficients
        let mut coeffs = vec![ScalarField::from_u32(0); 16];
        for y in 0..4 {
            for x in 0..4 {
                let idx = y * 4 + x;
                coeffs[idx] = ScalarField::from_u32((x + y) as u32);
            }
        }
        let poly = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs), 4, 4);
        
        // Extract univariate polynomial in x at y = 2
        let x_poly = poly.get_univariate_polynomial_x(2);
        assert_eq!(x_poly.y_size, 1);
        assert_eq!(x_poly.x_size, 4);
        // Check coefficients: at y = 2, the coefficients should be [2, 3, 4, 5]
        for i in 0..4 {
            assert_eq!(x_poly.get_coeff(i, 0), ScalarField::from_u32((i + 2) as u32));
        }
        
        // Extract univariate polynomial in y at x = 1
        let y_poly = poly.get_univariate_polynomial_y(1);
        assert_eq!(y_poly.x_size, 1);
        assert_eq!(y_poly.y_size, 4);
        // Check coefficients: at x = 1, the coefficients should be [1, 2, 3, 4]
        for i in 0..4 {
            assert_eq!(y_poly.get_coeff(0, i), ScalarField::from_u32((1 + i) as u32));
        }
    }

    #[test]
    fn test_eval() { // pass
        let poly = create_simple_polynomial();
        let x = ScalarField::from_u32(2);
        let y = ScalarField::from_u32(3);
        
        // 1 + 2x + 3y + 4xy = 1 + 2*2 + 3*3 + 4*2*3 = 1 + 4 + 9 + 24 = 38
        let expected = ScalarField::from_u32(38);
        let result = poly.eval(&x, &y);
        
        assert_eq!(result, expected);
    }

    #[test]
    fn test_eval_x() { // pass
        let poly = create_simple_polynomial();
        let x = ScalarField::from_u32(2);
        
        // Polynomial (1 + 2x + 3y + 4xy) with x=2 becomes: (1 + 4) + (3 + 8)y = 5 + 11y
        let result = poly.eval_x(&x);
        
        assert_eq!(result.x_size, 1);
        assert_eq!(result.y_size, 2);
        
        // Verify coefficients
        assert_eq!(result.get_coeff(0, 0), ScalarField::from_u32(5));   // Constant: 1 + 2*2
        assert_eq!(result.get_coeff(0, 1), ScalarField::from_u32(11));  // y coeff: 3 + 4*2
    }

    #[test]
    fn test_eval_y() { // pass
        let poly = create_simple_polynomial();
        let y = ScalarField::from_u32(3);
        
        // Polynomial (1 + 2x + 3y + 4xy) with y=3 becomes: (1 + 9) + (2 + 12)x = 10 + 14x
        let result = poly.eval_y(&y);
        
        assert_eq!(result.x_size, 2);
        assert_eq!(result.y_size, 1);
        
        // Verify coefficients
        assert_eq!(result.get_coeff(0, 0), ScalarField::from_u32(10));  // Constant: 1 + 3*3
        assert_eq!(result.get_coeff(1, 0), ScalarField::from_u32(14));  // x coeff: 2 + 4*3
    }


    #[test]
    fn test_resize() { // pass
        let mut poly = create_simple_polynomial();
        
        // Resize to 4x4
        poly.resize(4, 4);
        
        // Verify size
        assert_eq!(poly.x_size, 4);
        assert_eq!(poly.y_size, 4);
        
        // Verify original coefficients are preserved
        assert_eq!(poly.get_coeff(0, 0), ScalarField::from_u32(1));
        assert_eq!(poly.get_coeff(1, 0), ScalarField::from_u32(2));
        assert_eq!(poly.get_coeff(0, 1), ScalarField::from_u32(3));
        assert_eq!(poly.get_coeff(1, 1), ScalarField::from_u32(4));
        
        // New parts are filled with zeros
        assert_eq!(poly.get_coeff(2, 0), ScalarField::from_u32(0));
        assert_eq!(poly.get_coeff(0, 2), ScalarField::from_u32(0));
        assert_eq!(poly.get_coeff(3, 3), ScalarField::from_u32(0));
    }

    #[test]
    fn test_optimize_size() { // pass
        // Create a larger polynomial (4x4) but with only 2x2 actually used
        let mut coeffs = vec![ScalarField::from_u32(0); 16];
        for y in 0..2 {
            for x in 0..2 {
                let idx = y * 4 + x;
                coeffs[idx] = ScalarField::from_u32(1);  // Set non-zero values only in 2x2 submatrix
            }
        }
        
        let mut poly = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs), 4, 4);
        // Manually adjust the degree to reflect the actual non-zero terms
        poly.x_degree = 1;
        poly.y_degree = 1;
        
        // Optimize size
        poly.optimize_size();
        
        // Size should be 2x2 (or the next power of 2 that can contain 2x2)
        assert!(poly.x_size <= 2);
        assert!(poly.y_size <= 2);
    }

    #[test]
    fn test_div_by_ruffini() {
        let x_size = 2usize.pow(10);
        let y_size = 2usize.pow(5);
        let p_coeffs_vec = ScalarCfg::generate_random(x_size * y_size);
        let p = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&p_coeffs_vec), x_size, y_size);
        let x = ScalarCfg::generate_random(1)[0];
        let y = ScalarCfg::generate_random(1)[0];
        
        let (q_x, q_y, r) = p.div_by_ruffini(&x, &y);
        let a = ScalarCfg::generate_random(1)[0];
        let b = ScalarCfg::generate_random(1)[0];
        let q_x_eval = q_x.eval(&a, &b);
        let q_y_eval = q_y.eval(&a, &b);
        let estimated_p_eval = (q_x_eval * (a - x)) + (q_y_eval * (b - y)) + r;
        let true_p_eval = p.eval(&a, &b);
        assert!(estimated_p_eval.eq(&true_p_eval));
    }

    #[test]
    fn test_divide_x() {
        let x_size = 2usize.pow(10);
        let y_size = 2usize.pow(5);
        let p_coeffs_vec = ScalarCfg::generate_random(x_size * y_size);
        let p = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&p_coeffs_vec), x_size, y_size);

        let denom_x_size = 2usize.pow(6);
        let denom_y_size = 1;
        let denom_coeffs_vec = ScalarCfg::generate_random(denom_x_size * denom_y_size);
        let denom = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&denom_coeffs_vec), denom_x_size, denom_y_size);
        
        let (q, r) = p.divide_x(&denom);
        let a = ScalarCfg::generate_random(1)[0];
        let b = ScalarCfg::generate_random(1)[0];
        let denom_eval = denom.eval(&a, &b);
        let q_eval = q.eval(&a, &b);
        let r_eval = r.eval(&a, &b);
        let estimated_p_eval = (q_eval * denom_eval) + r_eval;
        let true_p_eval = p.eval(&a, &b);
        assert!(estimated_p_eval.eq(&true_p_eval));
    }

    #[test]
    fn test_divide_y() {
        let x_size = 2usize.pow(5);
        let y_size = 2usize.pow(10);
        let p_coeffs_vec = ScalarCfg::generate_random(x_size * y_size);
        let p = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&p_coeffs_vec), x_size, y_size);

        let denom_y_size = 2usize.pow(6);
        let denom_x_size = 1;
        let denom_coeffs_vec = ScalarCfg::generate_random(denom_x_size * denom_y_size);
        let denom = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&denom_coeffs_vec), denom_x_size, denom_y_size);
        
        let (q, r) = p.divide_y(&denom);
        let a = ScalarCfg::generate_random(1)[0];
        let b = ScalarCfg::generate_random(1)[0];
        let denom_eval = denom.eval(&a, &b);
        let q_eval = q.eval(&a, &b);
        let r_eval = r.eval(&a, &b);
        let estimated_p_eval = (q_eval * denom_eval) + r_eval;
        let true_p_eval = p.eval(&a, &b);
        assert!(estimated_p_eval.eq(&true_p_eval));
    }

    #[test]
    fn test_mul_monomial() {
        // Create a simple 2x2 polynomial: 1 + 2x + 3y + 4xy
        let coeffs = vec![
            ScalarField::from_u32(1),  // Constant
            ScalarField::from_u32(3),  // x
            ScalarField::from_u32(2),  // y
            ScalarField::from_u32(4),  // xy
        ];
        let poly = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&coeffs), 2, 2);
        
        // Multiply by xy (shift each term by x^1 * y^1)
        let result = poly.mul_monomial(1, 1);
        
        // Verify result dimensions are powers of two
        assert_eq!(result.x_size.is_power_of_two(), true);
        assert_eq!(result.y_size.is_power_of_two(), true);
        
        // In the implemented code, the degrees are calculated as size-1, so we test against that
        assert_eq!(result.x_degree, result.x_size as i64 - 1);
        assert_eq!(result.y_degree, result.y_size as i64 - 1);
        
        // Original: 1 + 2x + 3y + 4xy
        // After multiplying by xy: xy + 2x^2y + 3xy^2 + 4x^2y^2
        assert_eq!(result.get_coeff(0, 0), ScalarField::from_u32(0));  // No constant term
        assert_eq!(result.get_coeff(1, 1), ScalarField::from_u32(1));  // xy coefficient
        assert_eq!(result.get_coeff(2, 1), ScalarField::from_u32(2));  // x^2y coefficient
        assert_eq!(result.get_coeff(1, 2), ScalarField::from_u32(3));  // xy^2 coefficient
        assert_eq!(result.get_coeff(2, 2), ScalarField::from_u32(4));  // x^2y^2 coefficient
    }

    #[test]
    fn test_mul_polynomial() {
        let m = 5;
        let n = 3;
        let p1_x_size = 2usize.pow(m);
        let p1_y_size = 2usize.pow(0);
        let p2_x_size = 2usize.pow(m);
        let p2_y_size = 2usize.pow(n);

        let p1_coeffs_vec = ScalarCfg::generate_random(p1_x_size * p1_y_size);
        let p2_coeffs_vec = ScalarCfg::generate_random(p2_x_size * p2_y_size);
        let p1 = DensePolynomialExt::from_coeffs(
            HostSlice::from_slice(&p1_coeffs_vec),
            p1_x_size,
            p1_y_size
        );
        let p2 = DensePolynomialExt::from_coeffs(
            HostSlice::from_slice(&p2_coeffs_vec),
            p2_x_size, 
            p2_y_size
        );

        let p3 = &p1 * &p2;
        
        let x = ScalarCfg::generate_random(1)[0];
        let y = ScalarCfg::generate_random(1)[0];

        let p1_eval = p1.eval(&x, &y);
        let p2_eval = p2.eval(&x, &y);
        let p3_eval = p3.eval(&x, &y);

        assert!( p3_eval.eq(&(p1_eval * p2_eval)));

        let omega_x = ntt::get_root_of_unity::<ScalarField>(2u64.pow(m));
        let omega_y = ntt::get_root_of_unity::<ScalarField>(2u64.pow(n));
        let mut flag = true;
        for i in 0..2usize.pow(m) {
            for j  in 0..2usize.pow(n) {
                let x = omega_x.pow(i);
                let y = omega_y.pow(j);
                if !p3.eval(&x, &y).eq(&(p1.eval(&x, &y) * p2.eval(&x, &y))) {
                    flag = false;
                }
            }
        }
        assert!(flag);

    }


    // Test for div_by_vanishing - requires specific conditions
    #[test]
    fn test_div_by_vanishing_basic() {
        // Case m=2 and n=2:

        let c = 2usize.pow(4);
        let d = 2usize.pow(3);
        let m = 2;
        let n = 2;
        let mut t_x_coeffs = vec![ScalarField::zero(); 2*c];
        let mut t_y_coeffs = vec![ScalarField::zero(); 2*d];
        t_x_coeffs[c] = ScalarField::one();
        t_x_coeffs[0] = ScalarField::zero() - ScalarField::one();
        t_y_coeffs[d] = ScalarField::one();
        t_y_coeffs[0] = ScalarField::zero() - ScalarField::one();
        let mut t_x = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&t_x_coeffs), 2*c, 1);
        let mut t_y = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&t_y_coeffs), 1, 2*d);
        t_x.optimize_size();
        t_y.optimize_size();
        println!("t_x_xdeg: {:?}", t_x.x_degree);
        println!("t_y_ydeg: {:?}", t_y.y_degree);

        let q_x_coeffs_opt = ScalarCfg::generate_random(((m-1)*c-2) * (n*d -2) );
        let q_y_coeffs_opt = ScalarCfg::generate_random((c-1) * ((n-1)*d-2));
        let q_x_coeffs = resize(
            &q_x_coeffs_opt.into_boxed_slice(), 
            (m-1)*c-2, 
            n*d -2,
            (m-1)*c, 
            n*d,
            ScalarField::zero()
        );
        let q_y_coeffs = resize(
            &q_y_coeffs_opt.into_boxed_slice(), 
            c-1, 
            (n-1)*d-2, 
            c, 
            (n-1)*d,
            ScalarField::zero()
        );
        let mut q_x = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&q_x_coeffs), (m-1)*c, n*d);
        let mut q_y = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&q_y_coeffs), c, (n-1)*d);
        q_x.optimize_size();
        q_y.optimize_size();
        let mut p = &(&q_x * &t_x) + &(&q_y * &t_y);
        p.optimize_size();
        println!("p_xsize: {:?}", p.x_size);
        println!("p_ysize: {:?}", p.y_size);
        
        let (mut q_x_found, mut q_y_found) = p.div_by_vanishing(c as i64, d as i64);
        q_x_found.optimize_size();
        q_y_found.optimize_size();
        let p_reconstruct = &(&q_x_found * &t_x) + &(&q_y_found * &t_y);

        let a = ScalarCfg::generate_random(1)[0];
        let b = ScalarCfg::generate_random(1)[0];
        
        let p_evaled = p.eval(&a, &b);
        let p_reconstruct_evaled = p_reconstruct.eval(&a, &b);
        assert!(p_evaled.eq(&p_reconstruct_evaled));
        assert_eq!(q_x.x_degree, q_x_found.x_degree);
        assert_eq!(q_x.y_degree, q_x_found.y_degree);
        assert_eq!(q_y.x_degree, q_y_found.x_degree);
        assert_eq!(q_y.y_degree, q_y_found.y_degree);
        println!("Case m=2 and n=2 passed");

        // Case m=4 and n=2:

        let m = 3;
        let n = 2;
        let c = 2usize.pow(4);
        let d = 2usize.pow(3);
        let mut t_x_coeffs = vec![ScalarField::zero(); 2*c];
        let mut t_y_coeffs = vec![ScalarField::zero(); 2*d];
        t_x_coeffs[c] = ScalarField::one();
        t_x_coeffs[0] = ScalarField::zero() - ScalarField::one();
        t_y_coeffs[d] = ScalarField::one();
        t_y_coeffs[0] = ScalarField::zero() - ScalarField::one();
        let mut t_x = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&t_x_coeffs), 2*c, 1);
        let mut t_y = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&t_y_coeffs), 1, 2*d);
        t_x.optimize_size();
        t_y.optimize_size();
        println!("t_x_xdeg: {:?}", t_x.x_degree);
        println!("t_y_ydeg: {:?}", t_y.y_degree);

        let q_x_coeffs_opt = ScalarCfg::generate_random(((m-1)*c-3) * (n*d -2) );
        let q_y_coeffs_opt = ScalarCfg::generate_random((c-1) * ((n-1)*d-2));
        let q_x_coeffs = resize(
            &q_x_coeffs_opt.into_boxed_slice(), 
            (m-1)*c-3, 
            n*d -2,
            (m-1)*c, 
            n*d,
            ScalarField::zero()
        );
        let q_y_coeffs = resize(
            &q_y_coeffs_opt.into_boxed_slice(), 
            c-1, 
            (n-1)*d-2, 
            c, 
            (n-1)*d,
            ScalarField::zero()
        );
        let mut q_x = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&q_x_coeffs), (m-1)*c, n*d);
        let mut q_y = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&q_y_coeffs), c, (n-1)*d);
        q_x.optimize_size();
        q_y.optimize_size();
        let mut p = &(&q_x * &t_x) + &(&q_y * &t_y);
        p.optimize_size();
        println!("p_xsize: {:?}", p.x_size);
        println!("p_ysize: {:?}", p.y_size);
        
        let (mut q_x_found, mut q_y_found) = p.div_by_vanishing(c as i64, d as i64);
        q_x_found.optimize_size();
        q_y_found.optimize_size();
        let p_reconstruct = &(&q_x_found * &t_x) + &(&q_y_found * &t_y);

        let a = ScalarCfg::generate_random(1)[0];
        let b = ScalarCfg::generate_random(1)[0];
        
        let p_evaled = p.eval(&a, &b);
        let p_reconstruct_evaled = p_reconstruct.eval(&a, &b);
        assert!(p_evaled.eq(&p_reconstruct_evaled));
        assert_eq!(q_x.x_degree, q_x_found.x_degree);
        assert_eq!(q_x.y_degree, q_x_found.y_degree);
        assert_eq!(q_y.x_degree, q_y_found.x_degree);
        assert_eq!(q_y.y_degree, q_y_found.y_degree);
        println!("Case m=4 and n=2 passed");

    }

    #[test]
    fn update_degree_general_case() {
        let x_size = 2usize.pow(12);
        let y_size = 2usize.pow(6);
        // let x_degree= 2i64.pow(9);
        // let y_degree = 0;
        let x_degree: i64 = (x_size - 1) as i64;
        let y_degree: i64 = (y_size - 1) as i64;

        let dense_coeffs = ScalarCfg::generate_random(((x_degree + 1) * (y_degree + 1)) as usize);
        let dense_coeffs_resized = resize(&dense_coeffs, 
            (x_degree + 1) as usize, 
            (y_degree + 1) as usize, 
            x_size, 
            y_size, 
            ScalarField::zero()
        );
        let mut _dense_p = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&dense_coeffs_resized), x_size, y_size);

        let time_parallel = Instant::now();
        let (x_deg, y_deg) = _dense_p.find_degree();
        assert_eq!(x_deg, x_degree);
        assert_eq!(y_deg, y_degree);
        println!("find_degree time: {:.6} seconds", time_parallel.elapsed().as_secs_f64());

        let x_degree= 0;
        let y_degree = 0;

        let sparse_coeffs = ScalarCfg::generate_random(((x_degree + 1) * (y_degree + 1)) as usize);
        let sparse_coeffs_resized = resize(&sparse_coeffs, 
            (x_degree + 1) as usize, 
            (y_degree + 1) as usize, 
            x_size, 
            y_size, 
            ScalarField::zero()
        );
        let mut _sparse_p = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&sparse_coeffs_resized), x_size, y_size);

        let time_parallel = Instant::now();
        let (x_deg, y_deg) = _sparse_p.find_degree();
        assert_eq!(x_deg, x_degree);
        assert_eq!(y_deg, y_degree);
        println!("find_degree time: {:.6} seconds", time_parallel.elapsed().as_secs_f64());


    }
    // More tests can be added as needed
}

#[cfg(test)]
mod tests_vectors {
    use icicle_bls12_381::curve::{ScalarField, ScalarCfg};
    use icicle_core::traits::{Arithmetic, FieldConfig, FieldImpl, GenerateRandom};
    use icicle_runtime::memory::{HostOrDeviceSlice, HostSlice};
    use std::cmp;
    
    use crate::vector_operations::{*};

    macro_rules! scalar_vec {
        ( $( $x:expr ),* ) => {
            vec![
                $( ScalarField::from_u32($x) ),*
            ].into_boxed_slice()
        };
    }

    #[test]
    fn test_point_mul_two_vecs() {
        let vec1 = scalar_vec![1, 2, 3];
        let vec2 = scalar_vec![4, 5];
        let vec3 = scalar_vec![2, 0, 2, 4];

        let mut res = vec![ScalarField::zero(); 6].into_boxed_slice();
        outer_product_two_vecs(&vec1, &vec2, &mut res);
        println!("res : {:?}", res);

        let mut res = vec![ScalarField::zero(); 6].into_boxed_slice();
        outer_product_two_vecs(&vec2, &vec1, &mut res);
        println!("res : {:?}", res);

        let mut res = vec![ScalarField::zero(); 12].into_boxed_slice();
        outer_product_two_vecs(&vec1, &vec3, &mut res);
        println!("res : {:?}", res);

        let mut res = vec![ScalarField::zero(); 8].into_boxed_slice();
        outer_product_two_vecs(&vec3, &vec2, &mut res);
        println!("res : {:?}", res);

    }

    #[test]
    fn test_scaled_outer_product() {
        let vec1 = scalar_vec![1, 2, 3];
        let vec2 = scalar_vec![4, 5];
        let vec3 = scalar_vec![2, 0, 2, 4];
        let scaler = ScalarField::from_u32(2);

        let mut res = vec![ScalarField::zero(); 6].into_boxed_slice();
        scaled_outer_product(&vec1, &vec2, Some(&scaler), &mut res);
        println!("res : {:?}", res);

        let mut res = vec![ScalarField::zero(); 8].into_boxed_slice();
        scaled_outer_product(&vec3, &vec2, None, &mut res);
        println!("res : {:?}", res);

    }

    #[test]
    fn test_matrix_matrix_mul_small() {
        // example size: 2x3 * 3x2 = 2x2
        // LHS: 2x3
        // [1 2 3]
        // [4 5 6]
        let lhs = vec![
            ScalarField::from_u32(1u32),
            ScalarField::from_u32(2u32),
            ScalarField::from_u32(3u32),
            ScalarField::from_u32(4u32),
            ScalarField::from_u32(5u32),
            ScalarField::from_u32(6u32),
        ]
        .into_boxed_slice();

        // RHS: 3x2
        // [7  8]
        // [9 10]
        // [11 12]
        let rhs = vec![
            ScalarField::from_u32(7u32),
            ScalarField::from_u32(8u32),
            ScalarField::from_u32(9u32),
            ScalarField::from_u32(10u32),
            ScalarField::from_u32(11u32),
            ScalarField::from_u32(12u32),
        ]
        .into_boxed_slice();

        // expected result: 2x2
        // [1*7+2*9+3*11, 1*8+2*10+3*12] = [58, 64]
        // [4*7+5*9+6*11, 4*8+5*10+6*12] = [139, 154]
        let expected = vec![
            ScalarField::from_u32(58u32),
            ScalarField::from_u32(64u32),
            ScalarField::from_u32(139u32),
            ScalarField::from_u32(154u32),
        ]
        .into_boxed_slice();

        let mut res = vec![ScalarField::zero(); 4].into_boxed_slice();
        matrix_matrix_mul(&lhs, &rhs, 2, 3, 2, &mut res);

        for i in 0..4 {
            assert_eq!(res[i], expected[i], "Mismatch at index {}", i);
        }
    }

    #[test]
    fn test_gen_evaled_lagrange_bases() {
        let x = ScalarCfg::generate_random(1)[0];
        let size = 2048;
        let mut res = vec![ScalarField::zero(); size].into_boxed_slice();
        gen_evaled_lagrange_bases(&x, size, &mut res);
        
    }
    #[test]
    fn test_resize() {
        let rW_X_coeffs = ScalarCfg::generate_random(3);
        let rW_X_coeffs_resized = resize(&rW_X_coeffs, 3, 1, 4, 1, ScalarField::zero());
        let rW_Y_coeffs = ScalarCfg::generate_random(3);
        let rW_Y_coeffs_resized = resize(&rW_Y_coeffs, 1, 3, 1, 4, ScalarField::zero());

        println!("X_orig: {:?}", rW_X_coeffs);
        println!("X_ext: {:?}", rW_X_coeffs_resized);
        println!("Y_orig: {:?}", rW_Y_coeffs);
        println!("Y_ext: {:?}", rW_Y_coeffs_resized);
    }


}

mod tests_iotools {
    use crate::iotools::{from_coef_vec_to_g1serde_vec, gen_g1serde_vec_of_xy_monomials, scaled_outer_product_1d};
    use crate::vector_operations::extend_monomial_vec;
    use icicle_bls12_381::curve::{ScalarField, ScalarCfg, G1Affine, CurveCfg};
    use icicle_core::traits::{FieldImpl, GenerateRandom};
    use crate::group_structures::{G1serde};
    use crate::bivariate_polynomial::{BivariatePolynomial, DensePolynomialExt};
    use icicle_core::curve::Curve;
    use std::time::Instant;

    #[test]
    fn test_scalar_to_G1_conversion() {
        let x_size = 2usize.pow(10);
        let y_size = 2usize.pow(4);
        let x = ScalarCfg::generate_random(1)[0];
        let y = ScalarCfg::generate_random(1)[0];
        let gen = CurveCfg::generate_random_affine_points(1)[0];
        let mut res = vec![G1serde::zero(); x_size * y_size];
        
        let time_rayon = Instant::now();
        let mut x_powers_vec = vec![ScalarField::zero(); x_size];
        let mut y_powers_vec = vec![ScalarField::zero(); y_size];
        extend_monomial_vec(&vec![ScalarField::one(), x], &mut x_powers_vec);
        extend_monomial_vec(&vec![ScalarField::one(), y], &mut y_powers_vec);
        scaled_outer_product_1d(&x_powers_vec, &y_powers_vec, &gen, None, &mut res);
        let duration_rayon = time_rayon.elapsed();
        println!("Scalar_to_G1 time with rayon: {:.6} seconds", duration_rayon.as_secs_f64());
        drop(res);

        let mut res = vec![G1serde::zero(); x_size * y_size];
        let time_msm = Instant::now();
        gen_g1serde_vec_of_xy_monomials(x, y, &gen, x_size, y_size, &mut res);
        let duration_msm = time_msm.elapsed();
        println!("Scalar_to_G1 time with hybrid of rayon and msm: {:.6} seconds", duration_msm.as_secs_f64());
    }
}


================================================
FILE: packages/backend/libs/src/bivariate_polynomial/mod.rs
================================================
use icicle_bls12_381::curve::{ScalarField, ScalarCfg};
use icicle_core::traits::{Arithmetic, FieldImpl, FieldConfig, GenerateRandom};
use icicle_core::polynomials::UnivariatePolynomial;
use icicle_core::ntt::{self, NTTDir};
use icicle_core::vec_ops::{VecOps, VecOpsConfig};
use icicle_bls12_381::polynomials::DensePolynomial;
use icicle_runtime::memory::{HostOrDeviceSlice, HostSlice, DeviceSlice, DeviceVec};
use std::{
    cmp,
    ops::{Add, AddAssign, Mul, Sub, Neg},
};
use super::vector_operations::{*};
use rayon::prelude::*;

fn _find_size_as_twopower(target_x_size: usize, target_y_size: usize) -> (usize, usize) {
    // Problem: find min{m: x_size*2^m >= target_x_size} and min{n: y_size*2^n >= target_y_size}
    if target_x_size == 0 || target_y_size == 0 {
        panic!("Invalid target sizes for resize")
    }
    let mut new_x_size = target_x_size;
    let mut new_y_size = target_y_size;
    if target_x_size.is_power_of_two() == false {
        new_x_size = 1 << (usize::BITS - target_x_size.leading_zeros());
    }
    if target_y_size.is_power_of_two() == false {
        new_y_size = 1 << (usize::BITS - target_y_size.leading_zeros());
    }
    (new_x_size, new_y_size)
}

pub struct DensePolynomialExt {
    pub poly: DensePolynomial,
    pub x_degree: i64,
    pub y_degree: i64,
    pub x_size: usize,
    pub y_size: usize,
}


impl DensePolynomialExt {
    // Inherit DensePolynomial
    pub fn print(&self) {
        unsafe {
            self.poly.print()
        }
    }
    // Inherit DensePolynomial
    pub fn coeffs_mut_slice(&mut self) -> &mut DeviceSlice<ScalarField> {
        unsafe {
            self.poly.coeffs_mut_slice()
        }
    }

    // Method to get the degree of the polynomial.
    pub fn degree(&self) -> (i64, i64) {
        (self.x_degree, self.y_degree)
    }
    pub fn is_zero(&self) -> bool {
        let (x_degree, y_degree) = self.find_degree();
        if x_degree == -1 && y_degree == -1 {
            return true
        }
        return false
    }
}

// impl Drop for DensePolynomialExt {
//     fn drop(&mut self) {
//         unsafe {
//             delete(self.poly);
//             delete(self.x_degree);
//             delete(self.y_degree);
//         }
//     }
// }

impl Clone for DensePolynomialExt {
    fn clone(&self) -> Self {
        Self {
            poly: self.poly.clone(),
            x_degree: self.x_degree.clone(),
            y_degree: self.y_degree.clone(),
            x_size: self.x_size.clone(),
            y_size: self.y_size.clone(),
        }
    }
}

impl Add for &DensePolynomialExt {
    type Output = DensePolynomialExt;
    fn add(self: Self, rhs: Self) -> Self::Output {
        let mut lhs_ext = self.clone();
        let mut rhs_ext = rhs.clone();
        if self.x_size != rhs.x_size || self.y_size != rhs.y_size {
            let target_x_size = cmp::max(self.x_size, rhs.x_size);
            let target_y_size = cmp::max(self.y_size, rhs.y_size);
            lhs_ext.resize(target_x_size, target_y_size);
            rhs_ext.resize(target_x_size, target_y_size);
        }
        let out_poly = &lhs_ext.poly + &rhs_ext.poly;
        let x_size = lhs_ext.x_size;
        let y_size = lhs_ext.y_size;
        //let (x_degree, y_degree) = DensePolynomialExt::find_degree(&out_poly, x_size, y_size);
        DensePolynomialExt {
            poly: out_poly,
            x_degree: x_size as i64 - 1,
            y_degree: y_size as i64 - 1,
            x_size,
            y_size,
        }
    }
}

impl AddAssign<&DensePolynomialExt> for DensePolynomialExt {
    fn add_assign(&mut self, rhs: &DensePolynomialExt) {
        let mut lhs_ext = self.clone();
        let mut rhs_ext = rhs.clone();
        if self.x_size != rhs.x_size || self.y_size != rhs.y_size {
            let target_x_size = cmp::max(self.x_size, rhs.x_size);
            let target_y_size = cmp::max(self.y_size, rhs.y_size);
            lhs_ext.resize(target_x_size, target_y_size);
            rhs_ext.resize(target_x_size, target_y_size);
        }
        self.poly = &lhs_ext.poly + &rhs_ext.poly;
        self.x_size = lhs_ext.x_size;
        self.y_size = lhs_ext.y_size;
        //let (x_degree, y_degree) = DensePolynomialExt::find_degree(&self.poly, self.x_size, self.y_size);
        self.x_degree = self.x_size as i64 - 1;
        self.y_degree = self.y_size as i64 - 1;
    }
}

impl Sub for &DensePolynomialExt {
    type Output = DensePolynomialExt;

    fn sub(self: Self, rhs: Self) -> Self::Output {
        let mut lhs_ext = self.clone();
        let mut rhs_ext = rhs.clone();
        if self.x_size != rhs.x_size || self.y_size != rhs.y_size {
            let target_x_size = cmp::max(self.x_size, rhs.x_size);
            let target_y_size = cmp::max(self.y_size, rhs.y_size);
            lhs_ext.resize(target_x_size, target_y_size);
            rhs_ext.resize(target_x_size, target_y_size);
        }
        let out_poly = &lhs_ext.poly - &rhs_ext.poly;
        let x_size = lhs_ext.x_size;
        let y_size = lhs_ext.y_size;
        //let (x_degree, y_degree) = DensePolynomialExt::find_degree(&out_poly, x_size, y_size);
        DensePolynomialExt {
            poly: out_poly,
            x_degree: x_size as i64 - 1,
            y_degree: y_size as i64 - 1,
            x_size,
            y_size,
        }
    }
}

impl Mul for &DensePolynomialExt {
    type Output = DensePolynomialExt;

    fn mul(self: Self, rhs: Self) -> Self::Output {
        self._mul(rhs)
    }
}

// poly * scalar
impl Mul<&ScalarField> for &DensePolynomialExt {
    type Output = DensePolynomialExt;

    fn mul(self: Self, rhs: &ScalarField) -> Self::Output {
        if rhs.eq(&ScalarField::one()) {
            return self.clone()
        }
        let mut coeffs = DeviceVec::<ScalarField>::device_malloc(self.x_size * self.y_size).unwrap();
        self.copy_coeffs(0, &mut coeffs);
        let vec_ops_cfg = VecOpsConfig::default();
        let scaler_vec = [*rhs];
        let scaler = HostSlice::from_slice(&scaler_vec);
        let mut res_coeffs = DeviceVec::<ScalarField>::device_malloc(self.x_size * self.y_size).unwrap();
        ScalarCfg::scalar_mul(scaler, &coeffs, &mut res_coeffs, &vec_ops_cfg).unwrap();
        DensePolynomialExt::from_coeffs(&res_coeffs, self.x_size, self.y_size)
    }
}

// scalar * poly
impl Mul<&DensePolynomialExt> for &ScalarField {
    type Output = DensePolynomialExt;

    fn mul(self: Self, rhs: &DensePolynomialExt) -> Self::Output {
        if self.eq(&ScalarField::one()) {
            return rhs.clone()
        }
        let mut coeffs = DeviceVec::<ScalarField>::device_malloc(rhs.x_size * rhs.y_size).unwrap();
        rhs.copy_coeffs(0, &mut coeffs);
        let vec_ops_cfg = VecOpsConfig::default();
        let scaler_vec = [*self];
        let scaler = HostSlice::from_slice(&scaler_vec);
        let mut res_coeffs = DeviceVec::<ScalarField>::device_malloc(rhs.x_size * rhs.y_size).unwrap();
        ScalarCfg::scalar_mul(scaler, &coeffs, &mut res_coeffs, &vec_ops_cfg).unwrap();
        DensePolynomialExt::from_coeffs(&res_coeffs, rhs.x_size, rhs.y_size)
    }
}

// scalar + poly
impl Add<&DensePolynomialExt> for &ScalarField {
    type Output = DensePolynomialExt;

    fn add(self: Self, rhs: &DensePolynomialExt) -> Self::Output {
        let mut coeffs_vec = vec![ScalarField::zero(); rhs.x_size * rhs.y_size];
        let coeffs = HostSlice::from_mut_slice(&mut coeffs_vec);
        rhs.copy_coeffs(0, coeffs);
        coeffs_vec[0] = coeffs_vec[0] + *self;
        let res_coeffs = coeffs_vec.clone();
        DensePolynomialExt::from_coeffs(HostSlice::from_slice(&res_coeffs), rhs.x_size, rhs.y_size)
    }
}

// poly + scalar
impl Add<&ScalarField> for &DensePolynomialExt {
    type Output = DensePolynomialExt;

    fn add(self: Self, rhs: &ScalarField) -> Self::Output {
        let mut coeffs_vec = vec![ScalarField::zero(); self.x_size * self.y_size];
        let coeffs = HostSlice::from_mut_slice(&mut coeffs_vec);
        self.copy_coeffs(0, coeffs);
        coeffs_vec[0] = coeffs_vec[0] + *rhs;
        let res_coeffs = coeffs_vec.clone();
        DensePolynomialExt::from_coeffs(HostSlice::from_slice(&res_coeffs), self.x_size, self.y_size)
    }
}

// scalar - poly
impl Sub<&DensePolynomialExt> for &ScalarField {
    type Output = DensePolynomialExt;

    fn sub(self: Self, rhs: &DensePolynomialExt) -> Self::Output {
        let neg_rhs = -rhs;
        let mut coeffs_vec = vec![ScalarField::zero(); rhs.x_size * rhs.y_size];
        let coeffs = HostSlice::from_mut_slice(&mut coeffs_vec);
        neg_rhs.copy_coeffs(0, coeffs);
        coeffs[0] = *self + coeffs[0];

        DensePolynomialExt::from_coeffs(coeffs, rhs.x_size, rhs.y_size)
    }
}

// poly - scalar
impl Sub<&ScalarField> for &DensePolynomialExt {
    type Output = DensePolynomialExt;

    fn sub(self: Self, rhs: &ScalarField) -> Self::Output {
        let mut coeffs_vec = vec![ScalarField::zero(); self.x_size * self.y_size];
        let coeffs = HostSlice::from_mut_slice(&mut coeffs_vec);
        self.copy_coeffs(0, coeffs);
        coeffs_vec[0] = coeffs_vec[0] - *rhs;
        let res_coeffs = coeffs_vec.clone();
        DensePolynomialExt::from_coeffs(HostSlice::from_slice(&res_coeffs), self.x_size, self.y_size)
    }
}

impl Neg for &DensePolynomialExt {
    type Output = DensePolynomialExt;

    fn neg(self: Self) -> Self::Output {
        self._neg()
    }
}


pub trait BivariatePolynomial
where
    Self::Field: FieldImpl,
    Self::FieldConfig: FieldConfig,
{
    type Field: FieldImpl;
    type FieldConfig: FieldConfig;

    fn _biNTT<In: HostOrDeviceSlice<Self::Field> + ?Sized, Out: HostOrDeviceSlice<Self::Field> + ?Sized>( in_mat: &In, x_size: usize, y_size: usize, dir: NTTDir, out_mat: &mut Out);

    // Methods to create polynomials from coefficients or roots-of-unity evaluations.
    fn zero() -> Self;
    fn from_coeffs<S: HostOrDeviceSlice<Self::Field> + ?Sized>(coeffs: &S, x_size: usize, y_size: usize) -> Self;
    fn from_rou_evals<S: HostOrDeviceSlice<Self::Field> + ?Sized>(evals: &S, x_size: usize, y_size: usize, coset_x: Option<&Self::Field>, coset_y: Option<&Self::Field>) -> Self;

    // Method to evaluate the polynomial over the roots-of-unity domain for power-of-two sized domain
    fn to_rou_evals<S: HostOrDeviceSlice<Self::Field> + ?Sized>(&self, coset_x: Option<&Self::Field>, coset_y: Option<&Self::Field>, evals: &mut S);

    fn find_degree(&self) -> (i64, i64);

    // Method to divide this polynomial by vanishing polynomials 'X^{x_degree}-1' and 'Y^{y_degree}-1'.
    fn div_by_vanishing(&mut self, x_degree: i64, y_degree: i64) -> (Self, Self) where Self: Sized;

    // Method to divide this polynomial by (X-x) and (Y-y)
    fn div_by_ruffini(&self, x: &Self::Field, y: &Self::Field) -> (Self, Self, Self::Field) where Self: Sized;

    // // Methods to add or subtract a monomial in-place.
    // fn add_monomial_inplace(&mut self, monomial_coeff: &Self::Field, monomial: u64);
    // fn sub_monomial_inplace(&mut self, monomial_coeff: &Self::Field, monomial: u64);

    // Method to shift coefficient indicies. The same effect as multiplying a monomial X^iY^j.
    fn mul_monomial(&self, x_exponent: usize, y_exponent: usize) -> Self;

    fn resize(&mut self, target_x_size: usize, target_y_size: usize);
    fn optimize_size(&mut self);

    // Method to slice the polynomial, creating a sub-polynomial.
    fn _slice_coeffs_into_blocks(&self, num_blocks_x: usize, num_blocks_y: usize, blocks_raw: &mut Vec<Vec<Self::Field>> );

    // // Methods to return new polynomials containing only the even or odd terms.
    // fn even_x(&self) -> Self;
    // fn even_y(&self) -> Self;
    // fn odd_y(&self) -> Self;
    // fn odd_y(&self) -> Self;

    // Method to evaluate the polynomial at a given domain point.
    fn eval_x(&self, x: &Self::Field) -> Self;

    // Method to evaluate the polynomial at a given domain point.
    fn eval_y(&self, y: &Self::Field) -> Self;

    fn eval(&self, x: &Self::Field, y: &Self::Field) -> Self::Field;

    // // Method to evaluate the polynomial over a domain and store the results.
    // fn eval_on_domain<D_x: HostOrDeviceSlice<Self::Field> + ?Sized, D_y: HostOrDeviceSlice<Self::Field> + ?Sized, E: HostOrDeviceSlice<Self::Field> + ?Sized>(
    //     &self,
    //     domain_x: &D_x,
    //     domain_y: &D_y,
    //     evals: &mut E,
    // );

    // Method to retrieve a coefficient at a specific index.
    fn get_coeff(&self, idx_x: u64, idx_y: u64) -> Self::Field;
    // fn get_nof_coeffs_x(&self) -> u64;
    // fn get_nof_coeffs_y(&self) -> u64;

    // Method to retrieve a univariate polynomial of x as the coefficient of the 'idx_y'-th power of y.
    fn get_univariate_polynomial_x(&self, idx_y:u64) -> Self;
    // Method to retrieve a univariate polynomial of y as the coefficient of the 'idx_x'-th power of x.
    fn get_univariate_polynomial_y(&self, idx_x:u64) -> Self;

    // Method to copy coefficients into a provided slice.
    fn copy_coeffs<S: HostOrDeviceSlice<Self::Field> + ?Sized>(&self, start_idx: u64, coeffs: &mut S);
    // Scale a polynomial's coefficients of X by powers of a scaler.
    fn scale_coeffs_x(&self, scaler: &Self::Field) -> Self;
    fn scale_coeffs_y(&self, scaler: &Self::Field) -> Self;
    fn _scale_coeffs<S: HostOrDeviceSlice<Self::Field> + ?Sized>(&self, scaler: &Self::Field, y_dir: bool, scaled_coeffs: &mut S);

    fn _mul(&self, rhs: &Self) -> Self;
    // Method to divide this polynomial by another, returning quotient and remainder.
    fn divide_x(&self, denominator: &Self) -> (Self, Self) where Self: Sized;

    // Method to divide this polynomial by another, returning quotient and remainder.
    fn divide_y(&self, denominator: &Self) -> (Self, Self) where Self: Sized;

    fn _divide_uni(&self, denom: &Self, y_dir: bool) -> (Self, Self) where Self: Sized;

    fn _neg(&self) -> Self;

    // Method to divide a univariate polynomial by (X-x)
    fn _div_uni_coeffs_by_ruffini(poly_coeffs_vec: &[Self::Field], x: &Self::Field) -> (Vec<Self::Field>, Self::Field);

}

impl BivariatePolynomial for DensePolynomialExt {
    type Field = ScalarField;
    type FieldConfig = ScalarCfg;

    // fn update_degree(&mut self) {
    //     // find X degree
    //     let mut x_deg: i64 = -1;
    //     let mut y_deg: i64 = -1;
    //     for i in (0..self.x_size).rev() {
    //         let sub_poly = self.get_univariate_polynomial_y(i as u64);
    //         if sub_poly.poly.degree() >= 0 {
    //             x_deg = i as i64;
    //             break;
    //         }
    //     }
    //     for i in (0..self.y_size).rev() {
    //         let sub_poly = self.get_univariate_polynomial_x(i as u64);
    //         if sub_poly.poly.degree() >= 0 {
    //             y_deg = i as i64;
    //             break;
    //         }
    //     }

    //     self.x_degree = x_deg;
    //     self.y_degree = y_deg;
    // }

    fn _biNTT<In, Out>(
        in_mat: &In, 
        x_size: usize, 
        y_size: usize, 
        dir: NTTDir, 
        out_mat: &mut Out
    ) where 
        In: HostOrDeviceSlice<Self::Field> + ?Sized,
        Out: HostOrDeviceSlice<Self::Field> + ?Sized 
    {
        let size = x_size * y_size;
        ntt::initialize_domain::<Self::Field>(
            ntt::get_root_of_unity::<Self::Field>(size.try_into().unwrap()),
            &ntt::NTTInitDomainConfig::default(),
        ).unwrap();

        let mut cfg = ntt::NTTConfig::<Self::Field>::default();
        let vec_ops_cfg = VecOpsConfig::default();

        {
            // IFFT along Y coeffs
            let mut out_y = DeviceVec::<Self::Field>::device_malloc(size).unwrap();
            cfg.batch_size = x_size as i32;
            cfg.columns_batch = false;
            ntt::ntt(in_mat, dir, &cfg, &mut out_y).unwrap();

            // IFFT along X coeffs (GPU does not work with columns_batch == true, so we manually transpose the matrix)
            let mut out_y_tr = DeviceVec::<Self::Field>::device_malloc(size).unwrap();
            Self::FieldConfig::transpose(
                &out_y,
                x_size as u32,
                y_size as u32,
                &mut out_y_tr,
                &vec_ops_cfg,
            ).unwrap();
            drop(out_y);

            cfg.batch_size = y_size as i32;
            cfg.columns_batch = false;
            let mut out_x_tr = DeviceVec::<Self::Field>::device_malloc(size).unwrap();
            ntt::ntt(&out_y_tr, dir, &cfg, &mut out_x_tr).unwrap();
            drop(out_y_tr);

            Self::FieldConfig::transpose(
                &out_x_tr,
                y_size as u32,
                x_size as u32,
                out_mat,
                &vec_ops_cfg,
            ).unwrap();
        }
        ntt::release_domain::<Self::Field>().unwrap();
    }

    fn find_degree(&self) -> (i64, i64) {
        let size = self.x_size * self.y_size;
        let mut buf = vec![ScalarField::zero(); size];
        {
            let slice = HostSlice::from_mut_slice(&mut buf);
            self.poly.copy_coeffs(0, slice);
        }
        let x_size = self.x_size;
        let y_size = self.y_size;

        let (x_deg, y_deg) = rayon::join(
            || {
                (0..x_size)
                    .into_par_iter()
                    .rev()
                    .find_first(|&i| {
                        let row = &buf[i * y_size .. (i+1) * y_size];
                        row.iter().any(|c| *c != ScalarField::zero())
                    })
                    .map(|i| i as i64)
                    .unwrap_or(-1)
            },
            || {
                (0..y_size)
                    .into_par_iter()
                    .rev()
                    .find_first(|&j| {
                        (0..x_size).any(|i| buf[i * y_size + j] != ScalarField::zero())
                    })
                    .map(|j| j as i64)
                    .unwrap_or(-1)
            },
        );

        (x_deg, y_deg)
    }

    fn zero() -> Self {
        let zero_vec = [Self::Field::zero()];
        Self {
            poly: DensePolynomial::from_coeffs(HostSlice::from_slice(&zero_vec), 1),
            x_degree: -1,
            y_degree: -1,
            x_size: 1,
            y_size: 1,
        }
    }
    fn from_coeffs<S: HostOrDeviceSlice<Self::Field> + ?Sized>(coeffs: &S, x_size: usize, y_size: usize) -> Self {
        
        if x_size * y_size != coeffs.len() {
            panic!("Mismatch between the coefficient vector and the polynomial size")
        }
        if x_size.is_power_of_two() == false || y_size.is_power_of_two() == false {
            panic!("The input sizes for from_coeffs must be powers of two.")
        }
        let size = x_size + y_size;
        if size == 0 {
            return Self::zero()
        }
        let poly = DensePolynomial::from_coeffs(coeffs, x_size * y_size);
        //let (x_degree, y_degree) = DensePolynomialExt::_find_degree(&poly, x_size, y_size);
        Self {
            poly,
            x_degree: x_size as i64 - 1,
            y_degree: y_size as i64 - 1,
            x_size,
            y_size,
        }
    }

    fn scale_coeffs_x(&self, x_factor: &Self::Field) -> Self {
        let mut scaled_coeffs_vec = vec![Self::Field::zero(); self.x_size * self.y_size];
        let scaled_coeffs = HostSlice::from_mut_slice(&mut scaled_coeffs_vec);
        self._scale_coeffs(x_factor, false, scaled_coeffs);
        return DensePolynomialExt::from_coeffs(
            scaled_coeffs,
            self.x_size,
            self.y_size
        )
    }

    fn scale_coeffs_y(&self, y_factor: &Self::Field) -> Self {
        let mut scaled_coeffs_vec = vec![Self::Field::zero(); self.x_size * self.y_size];
        let scaled_coeffs = HostSlice::from_mut_slice(&mut scaled_coeffs_vec);
        self._scale_coeffs(y_factor, true, scaled_coeffs);
        return DensePolynomialExt::from_coeffs(
            scaled_coeffs,
            self.x_size,
            self.y_size
        )
    }

    fn _scale_coeffs<S: HostOrDeviceSlice<Self::Field> + ?Sized>(&self, factor: &Self::Field, y_dir: bool, scaled_coeffs: &mut S) {
        let x_size = self.x_size;
        let y_size = self.y_size;
        let size = x_size * y_size;
        let mut coeffs_vec = vec![Self::Field::zero(); size];
        let coeffs = HostSlice::from_mut_slice(&mut coeffs_vec);
        self.copy_coeffs(0, coeffs);
        let vec_ops_cfg = VecOpsConfig::default();

        if !y_dir {
            let mut left_scale = DeviceVec::<Self::Field>::device_malloc(size).unwrap();
            let mut scaler = Self::Field::one();
            for ind in 0..x_size {
                left_scale[ind * y_size .. (ind+1) * y_size].copy_from_host(HostSlice::from_slice(&vec![scaler; y_size])).unwrap();
                scaler = scaler.mul(*factor);
            }
            Self::FieldConfig::mul(coeffs, &left_scale, scaled_coeffs, &vec_ops_cfg).unwrap();
        }

        if y_dir {
            let mut _right_scale = DeviceVec::<Self::Field>::device_malloc(size).unwrap();
            let mut scaler = Self::Field::one();
            for ind in 0..y_size {
                _right_scale[ind * x_size .. (ind+1) * x_size].copy_from_host(HostSlice::from_slice(&vec![scaler; x_size])).unwrap();
                scaler = scaler.mul(*factor);
            }
            let mut right_scale = DeviceVec::<Self::Field>::device_malloc(size).unwrap();
            Self::FieldConfig::transpose(&_right_scale, y_size as u32, x_size as u32, &mut right_scale, &vec_ops_cfg).unwrap();
            Self::FieldConfig::mul(coeffs, &right_scale, scaled_coeffs, &vec_ops_cfg).unwrap();
        }
    }

    fn from_rou_evals<S: HostOrDeviceSlice<Self::Field> + ?Sized>(
        evals: &S,
        x_size: usize,
        y_size: usize,
        coset_x: Option<&Self::Field>,
        coset_y: Option<&Self::Field>
    ) -> Self {
        if !x_size.is_power_of_two() || !y_size.is_power_of_two() {
            panic!("The input sizes for from_rou_evals must be powers of two.")
        }
    
        let size = x_size * y_size;
        if size == 0 {
            return Self::zero()
        }
        let mut coeffs = DeviceVec::<Self::Field>::device_malloc(size).unwrap();

        let ntt_dir = ntt::NTTDir::kInverse;
        Self::_biNTT(evals, x_size, y_size, ntt_dir, &mut coeffs);
    
        let mut poly = DensePolynomialExt::from_coeffs(
            &coeffs,
            x_size,
            y_size,
        );
    
        if let Some(_factor) = coset_x {
            let factor = _factor.inv();
            poly = poly.scale_coeffs_x(&factor);
        }
        if let Some(_factor) = coset_y {
            let factor = _factor.inv();
            poly = poly.scale_coeffs_y(&factor);
        }
        poly
    }
    
    fn to_rou_evals<S: HostOrDeviceSlice<Self::Field> + ?Sized>(
        &self,
        coset_x: Option<&Self::Field>,
        coset_y: Option<&Self::Field>,
        evals: &mut S,
    ) {
        let size = self.x_size * self.y_size;

        if evals.len() < size {
            panic!("Insufficient buffer length for to_rou_evals")
        }
        let mut scaled_coeffs_vec = vec![Self::Field::zero(); self.x_size * self.y_size];
        let scaled_coeffs = HostSlice::from_mut_slice(&mut scaled_coeffs_vec);
        {
            let mut scaled_poly = self.clone();

            if let Some(factor) = coset_x {
                scaled_poly = scaled_poly.scale_coeffs_x(factor);
            }

            if let Some(factor) = coset_y {
                scaled_poly = scaled_poly.scale_coeffs_y(factor);
            }

            
            scaled_poly.copy_coeffs(0, scaled_coeffs);
        }
        
        let mut in_mat = DeviceVec::<ScalarField>::device_malloc(size).unwrap();
        in_mat.copy_from_host(&scaled_coeffs).unwrap();

        let ntt_dir = ntt::NTTDir::kForward;
        Self::_biNTT(&in_mat, self.x_size, self.y_size, ntt_dir, evals);
    }

    fn copy_coeffs<S: HostOrDeviceSlice<Self::Field> + ?Sized>(&self, start_idx: u64, coeffs: &mut S) {
        self.poly.copy_coeffs(start_idx, coeffs);
    }

    fn _neg(&self) -> Self {
        let zero_poly = Self::zero();
        &zero_poly - self
    }

    fn _slice_coeffs_into_blocks(&self, num_blocks_x: usize, num_blocks_y: usize, blocks: &mut Vec<Vec<Self::Field>> ) {

        if self.x_size % num_blocks_x != 0 || self.y_size % num_blocks_y != 0 {
            panic!("Matrix size must be exactly divisible by the number of blocks.");
        }
        if blocks.len() != num_blocks_x * num_blocks_y {
            panic!("Incorrect length of the vector to store the result.")
        }
        let block_x_size = self.x_size / num_blocks_x;
        let block_y_size = self.y_size / num_blocks_y;

        let mut orig_coeffs_vec = vec![Self::Field::zero(); self.x_size * self.y_size];
        let orig_coeffs = HostSlice::from_mut_slice(&mut orig_coeffs_vec);
        self.poly.copy_coeffs(0, orig_coeffs);

        for row_idx in 0..self.x_size{
            let row_vec = &orig_coeffs_vec[row_idx * self.y_size .. (row_idx + 1) * self.y_size];
            for col_idx in 0..self.y_size {
                let block_idx = num_blocks_y * (row_idx / block_x_size) + (col_idx / block_y_size);
                let in_block_idx = block_y_size * (row_idx % block_x_size) + (col_idx % block_y_size) ;
                blocks[block_idx][in_block_idx] = row_vec[col_idx].clone();
            }
        }

    }

    fn eval_x(&self, x: &Self::Field) -> Self {
        let mut result_slice = vec![Self::Field::zero(); self.y_size];
        let result = HostSlice::from_mut_slice(&mut result_slice);

        for offset in 0..(self.y_degree + 1) as usize  {
            let sub_xpoly = self.get_univariate_polynomial_x(offset as u64);
            result[offset] = sub_xpoly.poly.eval(x);
        }

        DensePolynomialExt::from_coeffs(result, 1, self.y_size)
    }

    fn eval_y(&self, y: &Self::Field) -> Self {
        let mut result_slice = vec![Self::Field::zero(); self.x_size];
        let result = HostSlice::from_mut_slice(&mut result_slice);

        for offset in 0..(self.x_degree + 1) as usize {
            let sub_ypoly = self.get_univariate_polynomial_y(offset as u64);
            result[offset] = sub_ypoly.poly.eval(y);
        }
        DensePolynomialExt::from_coeffs(result, self.x_size, 1)
    }

    fn eval(&self, x: &Self::Field, y: &Self::Field) -> Self::Field {
        let res1 = self.eval_x(x);
        let res2 = res1.eval_y(y);
        if !(res2.x_degree == 0 && res2.y_degree == 0) {
            panic!("The evaluation is not a constant.");
        } else {
            res2.get_coeff(0,0)
        }
    }

    fn get_coeff(&self, idx_x: u64, idx_y: u64) -> Self::Field {
        if !(idx_x <= self.x_size as u64 && idx_y <= self.y_size as u64){
            panic!("The index at which to get a coefficient exceeds the coefficient size.");
        }
        let idx = idx_x * self.y_size as u64 + idx_y;
        self.poly.get_coeff(idx)
    }

    fn get_univariate_polynomial_x(&self, idx_y:u64) -> Self {
        Self {
            poly: self.poly.slice(idx_y, self.y_size as u64, self.x_size as u64),
            x_size: self.x_size.clone(),
            y_size: 1,
            x_degree: self.x_degree.clone(),
            y_degree: 0,
        }
    }

    fn get_univariate_polynomial_y(&self, idx_x:u64) -> Self {
        Self {
            poly: self.poly.slice(idx_x * self.y_size as u64, 1, self.y_size as u64),
            x_size: 1,
            y_size: self.y_size.clone(),
            x_degree: 0,
            y_degree: self.y_degree.clone(),
        }
    }


    fn resize(&mut self, target_x_size: usize, target_y_size: usize){
        let (new_x_size, new_y_size) = _find_size_as_twopower(target_x_size, target_y_size);
        if self.x_size == new_x_size && self.y_size == new_y_size {
            return
        }
        let new_size: usize = new_x_size * new_y_size;
        let mut orig_coeffs_vec = Vec::<Self::Field>::with_capacity(self.x_size * self.y_size);
        unsafe{orig_coeffs_vec.set_len(self.x_size * self.y_size);}
        let orig_coeffs = HostSlice::from_mut_slice(&mut orig_coeffs_vec);
        self.copy_coeffs(0, orig_coeffs);

        let mut res_coeffs_vec = vec![Self::Field::zero(); new_size];
        for i in 0 .. cmp::min(self.x_size, new_x_size) {
            let each_y_size = cmp::min(self.y_size, new_y_size);
            res_coeffs_vec[new_y_size * i .. new_y_size * i + each_y_size].copy_from_slice(
                &orig_coeffs_vec[self.y_size * i .. self.y_size * i + each_y_size]
            );
        }

        let res_coeffs = HostSlice::from_mut_slice(&mut res_coeffs_vec);

        self.poly = DensePolynomial::from_coeffs(res_coeffs, new_size);
        self.x_size = new_x_size;
        self.y_size = new_y_size;
    }

    fn optimize_size(&mut self) {
        let (updated_x_degree, updated_y_degree) = self.find_degree();
        self.x_degree = updated_x_degree;
        self.y_degree = updated_y_degree;
        let target_x_size = updated_x_degree + 1;
        let target_y_size = updated_y_degree + 1;
        if target_x_size == 0 || target_y_size == 0 {
            return
        }
        self.resize(target_x_size as usize, target_y_size as usize);
    }

    fn mul_monomial(&self, x_exponent: usize, y_exponent: usize) -> Self {
        if x_exponent == 0 && y_exponent == 0 {
            self.clone()
        } else {
            let mut orig_coeffs_vec = Vec::<Self::Field>::with_capacity(self.x_size * self.y_size);
            unsafe{orig_coeffs_vec.set_len(self.x_size * self.y_size);}
            let orig_coeffs = HostSlice::from_mut_slice(&mut orig_coeffs_vec);
            self.copy_coeffs(0, orig_coeffs);

            let target_x_size = (self.x_degree + 1) as usize + x_exponent;
            let target_y_size = (self.y_degree + 1) as usize + y_exponent;
            let (new_x_size, new_y_size) = _find_size_as_twopower(target_x_size, target_y_size);
            let new_size: usize = new_x_size * new_y_size;

            let mut res_coeffs_vec = vec![Self::Field::zero(); new_size];
            for i in 0 .. self.x_size {
                res_coeffs_vec[new_y_size * (i + x_exponent) + y_exponent .. new_y_size * (i + x_exponent) + self.y_size + y_exponent].copy_from_slice(
                    &orig_coeffs_vec[self.y_size * i .. self.y_size * (i+1)]
                );
            }

            let res_coeffs = HostSlice::from_slice(&res_coeffs_vec);

            DensePolynomialExt::from_coeffs(res_coeffs, new_x_size, new_y_size)
        }
    }

    fn _mul(&self, rhs: &Self) -> Self {
        let (lhs_x_degree, lhs_y_degree) = self.find_degree();
        let (rhs_x_degree, rhs_y_degree) = rhs.find_degree();
        if lhs_x_degree + lhs_y_degree == 0 && rhs_x_degree + rhs_y_degree > 0 {
            return &(rhs.clone()) * &(self.get_coeff(0, 0));
        }
        if rhs_x_degree + rhs_y_degree == 0 && lhs_x_degree + lhs_y_degree > 0 {
            return &(self.clone()) * &(rhs.get_coeff(0,0));
        }
        if rhs_x_degree + rhs_y_degree == 0 && lhs_x_degree + lhs_y_degree == 0 {
            let out_coeffs_vec = vec![self.get_coeff(0,0) * rhs.get_coeff(0,0); 1];
            let out_coeffs = HostSlice::from_slice(&out_coeffs_vec);
            return DensePolynomialExt::from_coeffs(out_coeffs, 1, 1);
        }
        let target_x_size = (lhs_x_degree + rhs_x_degree + 1) as usize;
        let target_y_size = (lhs_y_degree + rhs_y_degree + 1) as usize;
        let mut lhs_ext = self.clone();
        lhs_ext.resize(target_x_size, target_y_size);
        let x_size = lhs_ext.x_size;
        let y_size = lhs_ext.y_size;
        let extended_size = x_size * y_size;
        let mut lhs_evals = DeviceVec::<Self::Field>::device_malloc(extended_size).unwrap();
        lhs_ext.to_rou_evals(None, None, &mut lhs_evals);
        drop(lhs_ext);
        let mut rhs_ext = rhs.clone();
        rhs_ext.resize(target_x_size, target_y_size);
        let mut rhs_evals = DeviceVec::<Self::Field>::device_malloc(extended_size).unwrap();
        rhs_ext.to_rou_evals(None, None, &mut rhs_evals);
        drop(rhs_ext);
        let cfg_vec_ops = VecOpsConfig::default();
        // Element-wise mult. of evaluations
        let mut out_evals = DeviceVec::<Self::Field>::device_malloc(extended_size).unwrap();
        ScalarCfg::mul(&lhs_evals, &rhs_evals, &mut out_evals, &cfg_vec_ops).unwrap();
        drop(lhs_evals);
        drop(rhs_evals);

        let mut res = DensePolynomialExt::from_rou_evals(&out_evals, x_size, y_size, None, None);
        res.optimize_size();
        return res
    }

    fn divide_x(&self, denominator: &Self) -> (Self, Self) where Self: Sized {
        let (numer_x_degree, numer_y_degree) = self.degree();
        let (denom_x_degree, denom_y_degree) = denominator.degree();
        if denom_y_degree != 0 {
            panic!("Denominator for divide_x must be X-univariate");
        }
        if numer_x_degree < denom_x_degree{
            panic!("Numer.degree < Denom.degree for divide_x");
        }
        if denom_x_degree == 0 {
            if Self::Field::eq(&(denominator.get_coeff(0, 0).inv()), &Self::Field::zero()) {
                panic!("Divide by zero")
            }
            let rem_coeffs_vec = vec![Self::Field::zero(); 1];
            let rem_coeffs = HostSlice::from_slice(&rem_coeffs_vec);
            return (
                &(self.clone()) * &(denominator.get_coeff(0, 0).inv()),
                DensePolynomialExt::from_coeffs(rem_coeffs, 1, 1),
            );
        }

        return self._divide_uni(denominator, false)
    }

    fn divide_y(&self, denominator: &Self) -> (Self, Self) where Self: Sized {
        let (numer_x_degree, numer_y_degree) = self.degree();
        let (denom_x_degree, denom_y_degree) = denominator.degree();
        if denom_x_degree != 0 {
            panic!("Denominator for divide_y must be Y-univariate");
        }
        if numer_y_degree < denom_y_degree{
            panic!("Numer.degree < Denom.degree for divide_y");
        }
        if denom_y_degree == 0 {
            if Self::Field::eq(&(denominator.get_coeff(0, 0).inv()), &Self::Field::zero()) {
                panic!("Divide by zero")
            }
            let rem_coeffs_vec = vec![Self::Field::zero(); 1];
            let rem_coeffs = HostSlice::from_slice(&rem_coeffs_vec);
            return (
                &(self.clone()) * &(denominator.get_coeff(0, 0).inv()),
                DensePolynomialExt::from_coeffs(rem_coeffs, 1, 1),
            );
        }

        return self._divide_uni(denominator, true)
    }

    fn _divide_uni(&self, denominator: &Self, y_dir: bool) -> (Self, Self) where Self: Sized {
        // Division along Y (denom is assumed to be a polynomial of Y)
        let quo_size = if y_dir {
            self.y_size
        } else {
            self.x_size
        };
        let rem_size = quo_size;
        let sweep_dir = if y_dir {
            self.x_size
        } else {
            self.y_size
        };

        let mut quo_coeffs_vec = vec![ScalarField::zero(); self.x_size * self.y_size];
        let mut rem_coeffs_vec = vec![ScalarField::zero(); self.x_size * self.y_size];

        for offset in 0..sweep_dir {
            let sub_poly = if y_dir {
                self.get_univariate_polynomial_y(offset as u64)
            } else {
                self.get_univariate_polynomial_x(offset as u64)
            };
            let (sub_quo_poly, sub_rem_poly) = sub_poly.poly.divide(&denominator.poly);
            let mut sub_quo_coeffs_vec = vec![Self::Field::zero(); quo_size];
            let mut sub_rem_coeffs_vec = vec![Self::Field::zero(); rem_size];
            let sub_quo_coeffs = HostSlice::from_mut_slice(&mut sub_quo_coeffs_vec);
            let sub_rem_coeffs = HostSlice::from_mut_slice(&mut sub_rem_coeffs_vec);
            sub_quo_poly.copy_coeffs(0, sub_quo_coeffs);
            sub_rem_poly.copy_coeffs(0, sub_rem_coeffs);
            quo_coeffs_vec[offset * quo_size .. (offset + 1) * quo_size].copy_from_slice(&sub_quo_coeffs_vec);
            rem_coeffs_vec[offset * rem_size .. (offset + 1) * rem_size].copy_from_slice(&sub_rem_coeffs_vec);
        }

        if !y_dir {
            transpose_inplace(&mut quo_coeffs_vec, self.y_size, self.x_size);
            transpose_inplace(&mut rem_coeffs_vec, self.y_size, self.x_size);
        }

        let quo_coeffs = HostSlice::from_mut_slice(&mut quo_coeffs_vec);
        let rem_coeffs = HostSlice::from_mut_slice(&mut rem_coeffs_vec);
        return (
            DensePolynomialExt::from_coeffs(quo_coeffs, self.x_size, self.y_size),
            DensePolynomialExt::from_coeffs(rem_coeffs, self.x_size, self.y_size)
        )
    }

    fn div_by_vanishing(&mut self, denom_x_degree: i64, denom_y_degree: i64) -> (Self, Self) {
        if !( (denom_x_degree as usize).is_power_of_two() && (denom_y_degree as usize).is_power_of_two() ) {
            panic!("The denominators must have degress as powers of two.")
        }
        self.optimize_size();
        let numer_x_size = self.x_size;
        let numer_y_size = self.y_size;
        let numer_x_degree = self.x_degree;
        let numer_y_degree = self.y_degree;
        if numer_x_degree < denom_x_degree || numer_y_degree < denom_y_degree {
            panic!("The numerator must have grater degrees than denominators.")
        }
        let m = numer_x_size / denom_x_degree as usize;
        let n = numer_y_size / denom_y_degree as usize;
        let c = denom_x_degree as usize;
        let d = denom_y_degree as usize;

        let zeta = Self::FieldConfig::generate_random(1)[0];
        let xi = zeta;
        let vec_ops_cfg = VecOpsConfig::default();

        let mut acc_block_eval = DeviceVec::<Self::Field>::device_malloc(c * n*d).unwrap();
        {
            let mut acc_block_vec = vec![Self::Field::zero(); c * n*d];
            let acc_block = HostSlice::from_mut_slice(&mut acc_block_vec);
            {
                let block = vec![Self::Field::zero(); c * n*d];
                let mut blocks = vec![block; m];
                self._slice_coeffs_into_blocks(m,1, &mut blocks);
                // Computing A' (accumulation of blocks of the numerator)

                for i in 0..m {
                    Self::FieldConfig::accumulate(
                        acc_block,
                        HostSlice::from_slice(&blocks[i]),
                        &vec_ops_cfg
                    ).unwrap();
                }
            }
            let acc_block_poly = DensePolynomialExt::from_coeffs(acc_block, c, n*d);
            // Computing R_tilde (eval of A' on rou-X and coset-Y)

            acc_block_poly.to_rou_evals(None, Some(&xi), &mut acc_block_eval);
        }

        // Computing Q_Y_tilde (eval of quo_y on rou-X and coset-Y)
        let quo_y = {
            let mut quo_y_tilde = DeviceVec::<Self::Field>::device_malloc(c * n*d).unwrap();
            {
                let mut denom = DeviceVec::<Self::Field>::device_malloc(c * n*d).unwrap();
                {
                    let mut t_d_coeffs = vec![ScalarField::zero(); 2*d];
                    t_d_coeffs[0] = ScalarField::zero() - ScalarField::one();
                    t_d_coeffs[d] = ScalarField::one();
                    let mut t_d = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&t_d_coeffs), 1, 2*d);
                    t_d.resize(c, n*d);
                    t_d.to_rou_evals(None, Some(&xi), &mut denom);
                }
                Self::FieldConfig::div(&acc_block_eval, &denom, &mut quo_y_tilde, &vec_ops_cfg).unwrap();
            }
            // Computing Q_Y
            DensePolynomialExt::from_rou_evals(&quo_y_tilde, c, n*d, None, Some(&xi))
        };

        // Computing Q_X
        let quo_x = {
            // Computing Q_X_tilde (eval of quo_x on coset-X and extended-rou-Y)
            let mut quo_x_tilde = DeviceVec::<Self::Field>::device_malloc(m*c * n*d).unwrap();
            {
                let mut b_tilde = DeviceVec::<Self::Field>::device_malloc(m*c * n*d).unwrap();
                {
                    // Computing R = quo_y * t_d
                    let r = &quo_y.mul_monomial(0, d) - &quo_y;
                    // Computing B
                    let mut b = &*self - &r;
                    drop(r);
                    b.resize(m*c, n*d);
                    // Computinb B_tilde (eval of B on coset-X and extended-rou-Y)

                    b.to_rou_evals(Some(&zeta), None, &mut b_tilde);
                }
                let mut denom = DeviceVec::<Self::Field>::device_malloc(m*c * n*d).unwrap();
                {
                    let mut t_c_coeffs = vec![ScalarField::zero(); 2*c];
                    t_c_coeffs[0] = ScalarField::zero() - ScalarField::one();
                    t_c_coeffs[c] = ScalarField::one();
                    let mut t_c = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&t_c_coeffs), 2*c, 1);
                    t_c.resize(m*c, n*d);
                    t_c.to_rou_evals(Some(&zeta), None, &mut denom);
                }
                Self::FieldConfig::div(&b_tilde, &denom, &mut quo_x_tilde, &vec_ops_cfg).unwrap();
            }
            DensePolynomialExt::from_rou_evals(&quo_x_tilde, m*c, n*d, Some(&zeta), None)
        };
        return (quo_x, quo_y)

    }

    fn div_by_ruffini(&self, x: &Self::Field, y: &Self:: Field) -> (Self, Self, Self::Field) where Self: Sized {
        // P(X,Y) = Q_X(X,Y)(X-x) + R_X(Y)
        // R_X(Y) = Q_Y(Y)(Y-y) + R_Y

        // Lengths of coeffs of P
        let x_len = self.x_size;
        let y_len = self.y_size;

        // Step 1: Extract the coefficients of univariate polynomials in X for each Y-degree
        // P(X,Y) = Y^{deg-1} P_{deg-1}(X) + Y^{deg-2} P_{deg-2}(X) + ... + Y^{0} (P_{0}(X))
        let mut p_i_coeffs_iter = vec![vec![Self::Field::zero();x_len]; y_len];
        for i in 0..y_len as u64 {
            let mut temp_vec = vec![Self::Field::zero(); x_len];
            let temp_buf = HostSlice::from_mut_slice(&mut temp_vec);
            self.get_univariate_polynomial_x(i).copy_coeffs(0, temp_buf);
            p_i_coeffs_iter[i as usize].clone_from_slice(&temp_vec);
        }

        // Step 2: Divide each polynomial P_i(X) by (X-x).
        let (q_x_coeffs_vec, r_x_coeffs_vec): (Vec<Vec<_>>, Vec<_>) =  p_i_coeffs_iter
            .into_par_iter()
            .map(|coeffs| {
                let (q_i_x, r_i) = DensePolynomialExt::_div_uni_coeffs_by_ruffini(&coeffs, x);
                (q_i_x, r_i)
            })
            .unzip();

        // Q_X(X,Y) = Y^0 q_0_X(X) + Y^1 q_1_X(X) + ... + Y^{deg-1} q_{deg-1}_X(X)
        // Flatten q_x_coeffs_vec
        let mut q_x_coeffs_vec_flat: Vec<Self::Field> = q_x_coeffs_vec.into_par_iter().flatten().collect();
        transpose_inplace(&mut q_x_coeffs_vec_flat, y_len, x_len);
        let q_x = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&q_x_coeffs_vec_flat), x_len, y_len);

        // Divide R_X(Y) by (Y-y).
        let (q_y_coeffs_vec, r_y) = DensePolynomialExt::_div_uni_coeffs_by_ruffini(&r_x_coeffs_vec, y);
        let q_y = DensePolynomialExt::from_coeffs(HostSlice::from_slice(&q_y_coeffs_vec), 1, y_len);
        (q_x, q_y, r_y)
    }

    fn _div_uni_coeffs_by_ruffini(poly_coeffs_vec: &[Self::Field], x: &Self::Field) -> (Vec<Self::Field>, Self::Field) {
        if poly_coeffs_vec.len() < 2 {
            return (vec![ScalarField::zero()], poly_coeffs_vec[0])
        }
        let len = poly_coeffs_vec.len();
        let mut q_coeffs_vec = vec![Self::Field::zero(); len];
        let mut b = poly_coeffs_vec[len - 1];
        q_coeffs_vec[len - 2] = b;
        for i in 3.. len + 1 {
            b = poly_coeffs_vec[len - i + 1] + b * *x;
            q_coeffs_vec[len - i] = b;
        }
        let r = poly_coeffs_vec[0] + b * *x;
        (q_coeffs_vec, r)
    }

}


================================================
FILE: packages/backend/libs/src/field_structures/mod.rs
================================================
use icicle_bls12_381::curve::{ScalarCfg, ScalarField};
use icicle_core::traits::{Arithmetic, FieldImpl, GenerateRandom};
use crate::iotools::{HexString, PlacementVariables, SetupParams, SubcircuitInfo, SubcircuitR1CS};
use super::vector_operations::{*};
use rand::Rng;
use std::ops::{Add, Mul, Sub};
use icicle_runtime::memory::HostSlice;
use icicle_core::hash::HashConfig;
use icicle_hash::keccak::Keccak256;

pub fn hashing(seed: &Vec<u8>) -> ScalarField {
    let keccak_hasher = Keccak256::new(0 /* default input size */).unwrap();
    let mut res_bytes = vec![0u8; 32]; // 32-byte output buffer
    keccak_hasher
    .hash(
        HostSlice::from_slice(seed),  // Input data
        &HashConfig::default(),                       // Default configuration
        HostSlice::from_mut_slice(&mut res_bytes),       // Output buffer
    )
    .unwrap();
    res_bytes[31] &= 0b0011_1111;
    return ScalarField::from_bytes_le(&res_bytes)
}

macro_rules! impl_Tau_struct {
    ( $($ScalarField:ident),* ) => {
        pub struct Tau {
            $(pub $ScalarField: ScalarField),*
        }

        impl Tau {
            pub fn gen() -> Self {
                Self {
                    $($ScalarField: ScalarCfg::generate_random(1)[0]),*
                }
            }
        }
    };
}
impl_Tau_struct!(x, y, alpha, gamma, delta, eta);

impl Tau {
    pub fn gen_fixed() -> Self {
        Self {
            x: ScalarField::from_hex("0x7234cd9b97845e0125e84ae3ae81354e004558d8c82a83425652bc7b9ed49f7d"),
            y: ScalarField::from_hex("0x6ed0eea55cbeeebdc7a41033ebd196ffecc1806fdbc13a8d41b8f1aa273a4037"),
            alpha: ScalarField::from_hex("0x7234cd9b97845e0125e84ae3ae81354e004558d8c82a83425652bc7b9ed49f7d"),
            gamma: ScalarField::from_hex("0x088dfe3d1b76775ec267d6d0e27b753ec904c76e0bc32ca8223dc2ae1a0ac6b4"),
            delta: ScalarField::from_hex("0x04b8ce26374c547d8722ac51f5ed1e0f9cb891c332c69c865d96af150189a818"),
            eta: ScalarField::from_hex("0x52eb2aeb35b72b94a19ea232e984850f2cda5542fdc10368955d8ac6274f8579")
        }
    }
}

pub fn from_r1cs_to_evaled_qap_mixture(
    compact_R1CS: &SubcircuitR1CS,
    setup_params: &SetupParams, 
    subcircuit_info: &SubcircuitInfo, 
    tau: &Tau, 
    x_evaled_lagrange_vec: &Box<[ScalarField]>
) -> Box<[ScalarField]> {
    let compact_A_mat = &compact_R1CS.A_compact_col_mat;
    let compact_B_mat = &compact_R1CS.B_compact_col_mat;
    let compact_C_mat = &compact_R1CS.C_compact_col_mat;
    let active_wires_A = &compact_R1CS.A_active_wires;
    let active_wires_B = &compact_R1CS.B_active_wires;
    let active_wires_C = &compact_R1CS.C_active_wires;
    let u_len = active_wires_A.len();
    let v_len = active_wires_B.len();
    let w_len = active_wires_C.len();
    let n = setup_params.n;

    // Evaluate u,v,w polynomials at tau.x
    let mut evaled_u_compact_col_vec = vec![ScalarField::zero(); u_len].into_boxed_slice();
    let mut evaled_v_compact_col_vec = vec![ScalarField::zero(); v_len].into_boxed_slice();
    let mut evaled_w_compact_col_vec = vec![ScalarField::zero(); w_len].into_boxed_slice();

    matrix_matrix_mul(compact_A_mat, x_evaled_lagrange_vec, u_len, n, 1, &mut evaled_u_compact_col_vec);
    matrix_matrix_mul(compact_B_mat, x_evaled_lagrange_vec, v_len, n, 1, &mut evaled_v_compact_col_vec);
    matrix_matrix_mul(compact_C_mat, x_evaled_lagrange_vec, w_len, n, 1, &mut evaled_w_compact_col_vec);
    
    // // Collect all active wires to form o_i(x) := \alpha * u_i(x) + \alpha^2 * v_i(x) + \alpha^3 * w_i(x)
    // let mut active_wires_o = HashSet::new();
    // active_wires_o = active_wires_o.union(active_wires_A).copied().collect();
    // active_wires_o = active_wires_o.union(active_wires_B).copied().collect();
    // active_wires_o = active_wires_o.union(active_wires_C).copied().collect();
    // let o_len = active_wires_o.len();
    
    // Prepare vectors for final evaluation
    let o_len = subcircuit_info.Nwires;
    let mut evaled_u_vec = vec![ScalarField::zero(); o_len].into_boxed_slice();
    let mut evaled_v_vec = evaled_u_vec.clone();
    let mut evaled_w_vec = evaled_u_vec.clone();

    let mut ordered_active_wires_A: Vec<usize> = active_wires_A.iter().cloned().collect();
    ordered_active_wires_A.sort();
    for (idx_u, &idx_o) in ordered_active_wires_A.iter().enumerate() {
        evaled_u_vec[idx_o] = evaled_u_compact_col_vec[idx_u];
    }
    let mut ordered_active_wires_B: Vec<usize> = active_wires_B.iter().cloned().collect();
    ordered_active_wires_B.sort();
    for (idx_v, &idx_o) in ordered_active_wires_B.iter().enumerate() {
        evaled_v_vec[idx_o] = evaled_v_compact_col_vec[idx_v];
    }
    let mut ordered_active_wires_C: Vec<usize> = active_wires_C.iter().cloned().collect();
    ordered_active_wires_C.sort();
    for (idx_w, &idx_o) in ordered_active_wires_C.iter().enumerate() {
        evaled_w_vec[idx_o] = evaled_w_compact_col_vec[idx_w];
    }
    drop(evaled_u_compact_col_vec);
    drop(evaled_v_compact_col_vec);
    drop(evaled_w_compact_col_vec);
    
    let mut first_term_vec = vec![ScalarField::zero(); o_len].into_boxed_slice();
    let mut second_term_vec = vec![ScalarField::zero(); o_len].into_boxed_slice();
    scale_vec(tau.alpha, &evaled_u_vec, &mut first_term_vec);
    scale_vec(tau.alpha.pow(2), &evaled_v_vec, &mut second_term_vec);
    drop(evaled_u_vec);
    drop(evaled_v_vec);

    let mut third_term_vec = vec![ScalarField::zero(); o_len].into_boxed_slice();
    point_add_two_vecs(&first_term_vec, &second_term_vec, &mut third_term_vec);
    drop(first_term_vec);
    drop(second_term_vec);

    let mut fourth_term_vec = vec![ScalarField::zero(); o_len].into_boxed_slice();
    scale_vec(tau.alpha.pow(3), &evaled_w_vec, &mut fourth_term_vec);
    drop(evaled_w_vec);

    let mut evaled_o_vec = vec![ScalarField::zero(); o_len].into_boxed_slice();
    point_add_two_vecs(&third_term_vec, &fourth_term_vec, &mut evaled_o_vec);
    
    return evaled_o_vec
}

impl PlacementVariables {
    pub fn gen_dummy(setup_params: &SetupParams, subcircuit_infos: &[SubcircuitInfo]) -> Box<[Self]> {
        let dummy = Self { subcircuitId: 0, variables: vec![HexString('0'.to_string())].into_boxed_slice() };
        let mut placement_variables_dummy: Box<[Self]> = vec![ dummy; setup_params.s_max].into_boxed_slice();
        for i in 0..setup_params.s_max {
            let mut rng = rand::thread_rng();
            let subcircuit_id: usize = if i == 0 {
                1
            } else if i== 1 {
                0
            } else {
                rng.gen_range(2..setup_params.s_D)
            };

            let variables_val = ScalarCfg::generate_random(subcircuit_infos[subcircuit_id].Nwires);
            let variables_hex: Vec<HexString> = variables_val.iter().map(|x| HexString(x.to_string())).collect();
            placement_variables_dummy[i] = Self {
                subcircuitId: subcircuit_id,
                variables: variables_hex.into_boxed_slice()
            };
        }

        return  placement_variables_dummy
    }
}
#[derive(Clone, Debug, Copy, PartialEq)]
pub struct FieldSerde(pub ScalarField);
impl Add for FieldSerde {
    type Output = Self;

    fn add(self, other: Self) -> Self {
        FieldSerde(self.0 + other.0)
    }
}

impl Sub for FieldSerde {
    type Output = Self;

    fn sub(self, other: Self) -> Self {
        FieldSerde(self.0 - other.0)
    }
}

impl Mul for FieldSerde {
    type Output = Self;

    fn mul(self, other: Self) -> Self {
        FieldSerde(self.0 * other.0)
    }
}

// serde - original
impl Sub<ScalarField> for FieldSerde {
    type Output = Self;

    fn sub(self, other: ScalarField) -> Self {
        FieldSerde(self.0 - other)
    }
}

// original - serde
impl Sub<FieldSerde> for ScalarField {
    type Output = FieldSerde;

    fn sub(self, other: FieldSerde) -> Self::Output {
        FieldSerde(self - other.0)
    }
}

// serde + original
impl Add<ScalarField> for FieldSerde {
    type Output = Self;

    fn add(self, other: ScalarField) -> Self {
        FieldSerde(self.0 + other)
    }
}

// original + serde
impl Add<FieldSerde> for ScalarField {
    type Output = FieldSerde;

    fn add(self, other: FieldSerde) -> Self::Output {
        FieldSerde(self + other.0)
    }
}

// serde * original
impl Mul<ScalarField> for FieldSerde {
    type Output = Self;

    fn mul(self, other: ScalarField) -> Self {
        FieldSerde(self.0 * other)
    }
}

// original * serde
impl Mul<FieldSerde> for ScalarField {
    type Output = FieldSerde;

    fn mul(self, other: FieldSerde) -> Self::Output {
        FieldSerde(self * other.0)
    }
}




================================================
FILE: packages/backend/libs/src/group_structures/mod.rs
================================================
use ark_ec::pairing::PairingOutput;
use icicle_bls12_381::curve::{G1Affine, G1Projective, G2Affine, ScalarField};
use icicle_core::traits::{Arithmetic, FieldImpl};
use icicle_core::msm::{self, MSMConfig};
use ark_bls12_381::{Bls12_381, G1Affine as ArkG1Affine, G2Affine as ArkG2Affine};
use ark_ec::{pairing::Pairing};
use ark_ff::{Field};
use icicle_runtime::memory::HostSlice;
use crate::bivariate_polynomial::{DensePolynomialExt, BivariatePolynomial};
use crate::field_structures::{FieldSerde, Tau};
use crate::iotools::{from_coef_vec_to_g1serde_mat, from_coef_vec_to_g1serde_vec, from_coef_vec_to_g1serde_vec_msm, scaled_outer_product_1d, scaled_outer_product_2d, Permutation, PlacementVariables, SetupParams, SubcircuitInfo, SubcircuitR1CS};
use crate::vector_operations::{*};

use serde::{Deserialize, Serialize};
use std::{
    ops::{Add, Mul, Sub},
};

macro_rules! extend_monomial_vec {
    ($mono_vec: expr, $target_size: expr) => {
        {
            let mut res = vec![ScalarField::zero(); $target_size].into_boxed_slice();
            extend_monomial_vec($mono_vec, &mut res);
            res
        }
    };
}

macro_rules! type_scaled_outer_product_2d {
    ($col_vec: expr, $row_vec: expr, $g1_gen: expr, $scaler: expr) => {
        {
            let row_size = $col_vec.len();
            let col_size = $row_vec.len();
            let mut res = vec![vec![G1serde::zero(); col_size].into_boxed_slice(); row_size].into_boxed_slice(); 
            scaled_outer_product_2d($col_vec, $row_vec, $g1_gen, $scaler, &mut res);
            res
        }
    };  
}

macro_rules! type_scaled_outer_product_1d {
    ($col_vec: expr, $row_vec: expr, $g1_gen: expr, $scaler: expr) => {
        {
            let row_size = $col_vec.len();
            let col_size = $row_vec.len();
            let mut res = vec![G1serde::zero(); row_size * col_size].into_boxed_slice();
            scaled_outer_product_1d($col_vec, $row_vec, $g1_gen, $scaler, &mut res);
            res
        }
    };  
}

#[macro_export]
macro_rules! type_scaled_monomials_1d {
    ( $cached_x_vec: expr, $cached_y_vec: expr, $x_size: expr, $y_size: expr, $scaler: expr, $g1_gen: expr ) => {
        {
            let col_vec = extend_monomial_vec!($cached_x_vec, $x_size);
            let row_vec = extend_monomial_vec!($cached_y_vec, $y_size);
            let res = type_scaled_outer_product_1d!(&col_vec, &row_vec, $g1_gen, $scaler);
            res
        }
    };
}

macro_rules! impl_encode_poly {
    ($t:ty) => {
        impl $t {
            pub fn encode_poly(
                &self,
                poly: &mut DensePolynomialExt,
                params: &SetupParams
            ) -> G1serde {
                poly.optimize_size();
                let x_size = poly.x_size;
                let y_size = poly.y_size;
                let rs_x_size = std::cmp::max(2*params.n, 2*(params.l_D - params.l) );
                let rs_y_size = params.s_max*2;
                let target_x_size = (poly.x_degree + 1) as usize;
                let target_y_size = (poly.y_degree + 1) as usize;
                if target_x_size > rs_x_size || target_y_size > rs_y_size {
                    panic!("Insufficient length of sigma.sigma_1.xy_powers");
                }
                if target_x_size * target_y_size == 0 {
                    return G1serde::zero()
                }
                let poly_coeffs_vec_compact = {
                    let mut poly_coeffs_vec = vec![ScalarField::zero(); x_size * y_size];
                    let poly_coeffs = HostSlice::from_mut_slice(&mut poly_coeffs_vec);
                    poly.copy_coeffs(0, poly_coeffs);
                    resize(
                        &poly_coeffs_vec,
                        x_size,
                        y_size,
                        target_x_size,
                        target_y_size,
                        ScalarField::zero()
                    )
                };
                
                let rs_unpacked: Vec<G1Affine> = {
                    let rs_resized = resize(
                        &self.xy_powers, 
                        rs_x_size, 
                        rs_y_size, 
                        target_x_size, 
                        target_y_size,
                        G1serde::zero()
                    );
                    rs_resized.iter().map(|x| x.0).collect()
                };

                let mut msm_res = vec![G1Projective::zero(); 1];
                
                msm::msm(
                    HostSlice::from_slice(
                        &poly_coeffs_vec_compact
                    ),
                    HostSlice::from_slice(
                        &rs_unpacked
                    ),
                    &MSMConfig::default(),
                    HostSlice::from_mut_slice(&mut msm_res)
                ).unwrap();
                G1serde(G1Affine::from(msm_res[0]))
            }
        }
    };
}

pub fn pairing(lhs: &[G1serde], rhs: &[G2serde]) -> PairingOutput<Bls12_381> {
    let lhs_ark: Vec<ArkG1Affine> = lhs.iter().map(|x| icicle_g1_affine_to_ark(&x.0)).collect();
    let rhs_ark: Vec<ArkG2Affine> = rhs.iter().map(|x| icicle_g2_affine_to_ark(&x.0)).collect();
    Bls12_381::multi_pairing(
        lhs_ark, 
        rhs_ark
    )
}

/// CRS (Common Reference String) structure
/// This corresponds to Ïƒ = ([Ïƒ_1]_1, [Ïƒ_2]_2) defined in the paper
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Sigma {
    pub G: G1serde,
    pub H: G2serde,
    pub sigma_1: Sigma1,
    pub sigma_2: Sigma2,
    pub lagrange_KL: G1serde,
}

impl Sigma {
    /// Generate full CRS
    pub fn gen(
        params: &SetupParams,
        tau: &Tau,
        o_vec: &[ScalarField],
        l_vec: &[ScalarField],
        k_vec: &[ScalarField],
        m_vec: &[ScalarField],
        g1_gen: &G1Affine,
        g2_gen: &G2Affine
    ) -> Self {
        println!("Generating a sigma (Ïƒ)...");
        let lagrange_KL = (l_vec[params.s_max - 1] * k_vec[params.l_D - params.l - 1]) * G1serde(*g1_gen);
        let sigma_1 = Sigma1::gen(params, tau, o_vec, l_vec, k_vec, m_vec,g1_gen);
        let sigma_2 = Sigma2::gen(tau, g2_gen);
        Self {
            G: G1serde(*g1_gen),
            H: G2serde(*g2_gen),
            sigma_1,
            sigma_2,
            lagrange_KL,
        }
    }
}

/// CRS's AC component
/// This corresponds to Ïƒ_A,C in the mathematical formulation
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Sigma1 {
    // Elements of the form {x^h y^i}_{h=0,i=0}^{max(2n-2,3m_D-3),2*s_max-2}
    pub xy_powers: Box<[G1serde]>,
    pub x: G1serde,
    pub y: G1serde,
    pub delta: G1serde,
    pub eta: G1serde,
    pub gamma_inv_o_inst: Box<[G1serde]>,
    pub eta_inv_li_o_inter_alpha4_kj: Box<[Box<[G1serde]>]>, // {Î·^(-1)L_i(y)(o_{j+l}(x) + Î±^4 K_j(x))}_{i=0,j=0}^{s_max-1,m_I-1}
    pub delta_inv_li_o_prv: Box<[Box<[G1serde]>]>, // {Î´^(-1)L_i(y)o_j(x)}_{i=0,j=l+m_I}^{s_max-1,m_I-1}
    pub delta_inv_alphak_xh_tx: Box<[Box<[G1serde]>]>, // {Î´^(-1)Î±^k x^h t_n(x)}_{h=0,k=1}^{2,3}
    pub delta_inv_alpha4_xj_tx: Box<[G1serde]>, // {Î´^(-1)Î±^4 x^j t_{m_I}(x)}_{j=0}^{1}
    pub delta_inv_alphak_yi_ty: Box<[Box<[G1serde]>]>, // {Î´^(-1)Î±^k y^i t_{s_max}(y)}_{i=0,k=1}^{2,4}
}

impl_encode_poly!(Sigma1);

impl Sigma1 {
    pub fn gen(
        params: &SetupParams,
        tau: &Tau,
        o_vec: &[ScalarField],
        l_vec: &[ScalarField],
        k_vec: &[ScalarField],
        m_vec: &[ScalarField],
        g1_gen: &G1Affine
    ) -> Self {

        let n = params.n;
        let m_d = params.m_D;
        let l = params.l;
        let l_user = params.l_user;
        let l_block = params.l_block;
        let s_max = params.s_max;
        let m_block = params.l_block - l_user;
        let m_function = l - l_block;
        let m_i = params.l_D - l;
        
        println!("Generating Sigma1 components...");
        
        // Calculate max(2n-2, 3m_I-3) for h upper bound
        let h_max = std::cmp::max(2*n, 2*m_i);
        
        // Calculate elements of the form {x^h y^i}
        println!("Generating xy_powers of size {}...", h_max * (2*s_max));
        let x_pows_vec = extend_monomial_vec!(
            &vec![ScalarField::one(), tau.x].into_boxed_slice(), 
            h_max
        );
        let y_pows_vec = extend_monomial_vec!(
            &vec![ScalarField::one(), tau.y].into_boxed_slice(), 
            2*s_max
        );
        let xy_powers = type_scaled_monomials_1d!(&x_pows_vec, &y_pows_vec, h_max, 2*s_max, None, g1_gen);
        println!("");

        // Split output vector into input, output, intermediate, and private parts
        let o_inst_vec = &o_vec[0..l].to_vec().into_boxed_slice();
        let o_inter_vec = &o_vec[l..l+m_i].to_vec().into_boxed_slice();
        let o_prv_vec = &o_vec[l+m_i..m_d].to_vec().into_boxed_slice();
        
        // Generate delta = G1serde Â· Î´ and eta = G1serde Â· Î·
        let x = G1serde(G1Affine::from((*g1_gen).to_projective() * tau.x));
        let y = G1serde(G1Affine::from((*g1_gen).to_projective() * tau.y));
        let delta = G1serde(G1Affine::from((*g1_gen).to_projective() * tau.delta));
        let eta = G1serde(G1Affine::from((*g1_gen).to_projective() * tau.eta));
        
        // Generate Î³^(-1)(L_t(y)o_j(x) + M_j(x)) for public instance wires jâˆˆ[0,l_pub-1], t={0, 1} and Î³^(-1)L_t(y)o_j(x) for private instance wires jâˆˆ[l_pub,l-1], t={2, 3}
        println!("Generating gamma_inv_o_inst of size {}...", l);
        let mut gamma_inv_o_inst = vec![G1serde::zero(); l].into_boxed_slice();
        {
            // For the order of indices of l_vec, see BUFFER_LIST of tokamak-zk-evm/packages/frontend/synthesizer/src/interface/qapCompiler/configuredTypes.ts
            let user_vec = [
                vec![l_vec[0]; params.l_user_out], 
                vec![l_vec[1]; params.l_user - params.l_user_out],
                vec![l_vec[2]; m_block],
                vec![l_vec[3]; m_function],
            ].concat().into_boxed_slice();

            let mut l_o_inst_vec = vec![ScalarField::zero(); l].into_boxed_slice();
            point_mul_two_vecs(&user_vec, o_inst_vec, &mut l_o_inst_vec);
            drop(user_vec);

            let mut l_o_inst_mj_vec = vec![ScalarField::zero(); l].into_boxed_slice();
            point_add_two_vecs(&l_o_inst_vec, m_vec, &mut l_o_inst_mj_vec);
            drop(l_o_inst_vec);

            let mut gamma_inv_o_inst_vec = vec![ScalarField::zero(); l].into_boxed_slice();
            scale_vec(tau.gamma.inv(), &l_o_inst_mj_vec, &mut gamma_inv_o_inst_vec);
            drop(l_o_inst_mj_vec);

            from_coef_vec_to_g1serde_vec(&gamma_inv_o_inst_vec, g1_gen, &mut gamma_inv_o_inst);
        }
        
        // Generate Î·^(-1)L_i(y)(o_{j+l}(x) + Î±^k K_j(x)) for intermediate wires
        println!("Generating eta_inv_li_o_inter_alpha4_kj of size {}...", m_i * s_max);
        let mut alpha4_kj_vec = vec![ScalarField::zero(); m_i].into_boxed_slice();
        scale_vec(tau.alpha.pow(4), k_vec, &mut alpha4_kj_vec);
        let mut o_inter_alpha4_kj_vec = vec![ScalarField::zero(); m_i].into_boxed_slice();
        point_add_two_vecs(o_inter_vec, &alpha4_kj_vec, &mut o_inter_alpha4_kj_vec);
        let eta_inv_li_o_inter_alpha4_kj = type_scaled_outer_product_2d!(&o_inter_alpha4_kj_vec, l_vec, g1_gen, Some(&tau.eta.inv()));   
        drop(alpha4_kj_vec);
        drop(o_inter_alpha4_kj_vec);
        
        // Generate Î´^(-1)L_i(y)o_j(x) for private wires
        println!("Generating delta_inv_li_o_prv of size {}...", (m_d - (l + m_i)) * s_max );
        let delta_inv_li_o_prv = type_scaled_outer_product_2d!(o_prv_vec, l_vec, g1_gen, Some(&tau.delta.inv()));
        
        // Generate Î´^(-1)Î±^k x^h t_n(x) for a vanishing polynomial in x
        println!("Generating delta_inv_alphak_xh_tx...");
        let mut delta_inv_alphak_xh_tx = vec![vec![G1serde::zero(); 3].into_boxed_slice(); 3].into_boxed_slice(); // k âˆˆ [1,3], h âˆˆ [0,2]
        {
            let mut delta_inv_alphak_xh_tx_vec = vec![ScalarField::zero(); 9].into_boxed_slice(); // k âˆˆ [1,3], h âˆˆ [0,2]
            let t_x = tau.x.pow(n) - ScalarField::one(); // t_n(x) = x^n - 1
            for k in 1..=3 {
                for h in 0..=2 {
                    let idx = (k-1) * 3 + h;
                    delta_inv_alphak_xh_tx_vec[idx] = tau.delta.inv() * tau.alpha.pow(k) * tau.x.pow(h) * t_x;
                }
            }
            from_coef_vec_to_g1serde_mat(&delta_inv_alphak_xh_tx_vec, 3, 3, g1_gen, &mut delta_inv_alphak_xh_tx);
        }
        
        // Generate Î´^(-1)Î±^4 x^j t_{m_I}(x) for a vanishing polynomial in x
        println!("Generating delta_inv_alpha4_xj_tx...");
        let mut delta_inv_alpha4_xj_tx = vec![G1serde::zero(); 2].into_boxed_slice(); // Only j âˆˆ [0,1]
        {
            let mut delta_inv_alpha4_xj_tx_vec = vec![ScalarField::zero(); 2].into_boxed_slice();
            let t_x = tau.x.pow(m_i) - ScalarField::one(); // t_{m_I}(x) = x^{m_I} - 1
            for j in 0..=1 {
                delta_inv_alpha4_xj_tx_vec[j] = tau.delta.inv() * tau.alpha.pow(4) * tau.x.pow(j) * t_x;
            }
            from_coef_vec_to_g1serde_vec(&delta_inv_alpha4_xj_tx_vec, g1_gen, &mut delta_inv_alpha4_xj_tx);
        }
        
        // Generate Î´^(-1)Î±^k y^i t_{s_max}(y) for a vanishing polynomial in y
        println!("Generating delta_inv_alphak_yi_ty...");
        let mut delta_inv_alphak_yi_ty = vec![vec![G1serde::zero(); 3].into_boxed_slice(); 4].into_boxed_slice(); // k âˆˆ [1,4], i âˆˆ [0,2]
        {
            let mut delta_inv_alphak_yi_ty_vec = vec![ScalarField::zero(); 12].into_boxed_slice();
            let t_y = tau.y.pow(s_max) - ScalarField::one(); // t_{s_max}(y) = y^{s_max} - 1
            for k in 1..=4 {
                for i in 0..=2 {
                    let idx = (k-1) * 3 + i;
                    delta_inv_alphak_yi_ty_vec[idx] = tau.delta.inv() * tau.alpha.pow(k) * tau.y.pow(i) * t_y;
                }
            }
            from_coef_vec_to_g1serde_mat(&delta_inv_alphak_yi_ty_vec, 4, 3, g1_gen, &mut delta_inv_alphak_yi_ty);
        }

        Self {
            xy_powers,
            x,
            y,
            delta,
            eta,
            gamma_inv_o_inst,
            eta_inv_li_o_inter_alpha4_kj,
            delta_inv_li_o_prv,
            delta_inv_alphak_xh_tx,
            delta_inv_alpha4_xj_tx,
            delta_inv_alphak_yi_ty
        }
        
    }

    // pub fn encode_O_inst(
    //     &self,
    //     placement_variables: &[PlacementVariables],
    //     subcircuit_infos: &[SubcircuitInfo],
    //     setup_params: &SetupParams
    // ) -> G1serde {
    //     let mut aligned_rs = vec![G1Affine::zero(); setup_params.l];
    //     let mut aligned_wtns = vec![ScalarField::zero(); setup_params.l];
    //     let mut cnt: usize = 0;
    //     let gamma_inv_o_inst = [
    //         &self.gamma_inv_o_user_inst[..],
    //         &self.gamma2_inv_o_block_inst[..],
    //         &self.gamma2_inv_o_function_inst[..],
    //     ].concat().into_boxed_slice();
    //     for i in 0..4 {
    //         let subcircuit_id = placement_variables[i].subcircuitId;
    //         let variables = &placement_variables[i].variables;
    //         let subcircuit_info = &subcircuit_infos[subcircuit_id];
    //         let flatten_map = &subcircuit_info.flattenMap;
    //         let (start_idx, end_idx_exclusive) = if subcircuit_info.name == "bufferPubOut" {
    //             // PUBLIC_OUT
    //             (subcircuit_info.Out_idx[0], subcircuit_info.Out_idx[0] + subcircuit_info.Out_idx[1])
    //         } else if subcircuit_info.name == "bufferPubIn" {
    //             // PUBLIC_IN
    //             (subcircuit_info.In_idx[0], subcircuit_info.In_idx[0] + subcircuit_info.In_idx[1])
    //         } else if subcircuit_info.name == "bufferBlockIn" {
    //             // BLOCK_IN
    //             (subcircuit_info.In_idx[0], subcircuit_info.In_idx[0] + subcircuit_info.In_idx[1])
    //         } else if subcircuit_info.name == "bufferEVMIn" {
    //             // EVM_IN
    //             (subcircuit_info.In_idx[0], subcircuit_info.In_idx[0] + subcircuit_info.In_idx[1])
    //         } else {
    //             panic!("Target placement is not a buffer")
    //         };

    //         for j in start_idx..end_idx_exclusive {
    //             aligned_wtns[cnt] = ScalarField::from_hex(&variables[j]);
    //             let global_idx = flatten_map[j];
    //             let curve_point = gamma_inv_o_inst[global_idx].0;
    //             aligned_rs[cnt] = curve_point;
    //             cnt += 1;
    //         }
    //     }
    //     let mut msm_res = vec![G1Projective::zero(); 1];
    //     msm::msm(
    //         HostSlice::from_slice(&aligned_wtns),
    //         HostSlice::from_slice(&aligned_rs),
    //         &MSMConfig::default(),
    //         HostSlice::from_mut_slice(&mut msm_res)
    //     ).unwrap();

    //     G1serde(G1Affine::from(msm_res[0]))
    // }

    pub fn encode_O_inst(
        &self,
        placement_variables: &[PlacementVariables],
        subcircuit_infos: &[SubcircuitInfo],
        setup_params: &SetupParams
    ) -> G1serde {
        let mut aligned_rs = vec![G1Affine::zero(); setup_params.l];
        let mut aligned_wtns = vec![ScalarField::zero(); setup_params.l];
        let mut cnt: usize = 0;
        for i in 0..4 {
            let subcircuit_id = placement_variables[i].subcircuitId;
            let variables = &placement_variables[i].variables;
            let subcircuit_info = &subcircuit_infos[subcircuit_id];
            let flatten_map = &subcircuit_info.flattenMap;
            let (start_idx, end_idx_exclusive) = if subcircuit_info.name == "bufferPubOut" {
                // PUBLIC_OUT
                (subcircuit_info.Out_idx[0], subcircuit_info.Out_idx[0] + subcircuit_info.Out_idx[1])
            } else if subcircuit_info.name == "bufferPubIn" {
                // PUBLIC_IN
                (subcircuit_info.In_idx[0], subcircuit_info.In_idx[0] + subcircuit_info.In_idx[1])
            } else if subcircuit_info.name == "bufferEVMIn" {
                // EVM_IN
                (subcircuit_info.In_idx[0], subcircuit_info.In_idx[0] + subcircuit_info.In_idx[1])
            } else if subcircuit_info.name == "bufferBlockIn" {
                // BLOCK_IN
                (subcircuit_info.In_idx[0], subcircuit_info.In_idx[0] + subcircuit_info.In_idx[1])
            } else {
                panic!("Target placement is not a buffer")
            };

            for j in start_idx..end_idx_exclusive {
                aligned_wtns[cnt] = ScalarField::from_hex(&variables[j]);
                let global_idx = flatten_map[j];
                let curve_point = self.gamma_inv_o_inst[global_idx].0;
                aligned_rs[cnt] = curve_point;
                cnt += 1;
            }
        }
        let mut msm_res = vec![G1Projective::zero(); 1];
        msm::msm(
            HostSlice::from_slice(&aligned_wtns),
            HostSlice::from_slice(&aligned_rs),
            &MSMConfig::default(),
            HostSlice::from_mut_slice(&mut msm_res)
        ).unwrap();

        G1serde(G1Affine::from(msm_res[0]))
    }


    pub fn encode_O_mid_no_zk(
        &self,
        placement_variables: &[PlacementVariables],
        subcircuit_infos: &[SubcircuitInfo],
        setup_params: &SetupParams
    ) -> G1serde {
        let mut nVar: usize = 0;
        for i in 0..placement_variables.len() {
            let subcircuit_id = placement_variables[i].subcircuitId;
            let subcircuit_info = &subcircuit_infos[subcircuit_id];
            if subcircuit_info.name == "bufferPubOut" {
                // PUBLIC_OUT
                nVar = nVar + subcircuit_info.In_idx[1]; // Number of input wires
            } else if subcircuit_info.name == "bufferPubIn" {
                // PUBLIC_IN
                nVar = nVar + subcircuit_info.Out_idx[1]; // Number of output wires
            } else if subcircuit_info.name == "bufferBlockIn"{
                // BLOCK_IN
                nVar = nVar + subcircuit_info.Out_idx[1]; // Number of output wires
            } else if subcircuit_info.name == "bufferEVMIn"{
                // EVM_IN
                nVar = nVar + subcircuit_info.Out_idx[1]; // Number of output wires
            } else {
                nVar = nVar + subcircuit_info.Out_idx[1] + subcircuit_info.In_idx[1];
            }
            nVar += 1; // Adding 1 for constant wires
        }

        return self._encode_statement(
            setup_params.l,
            setup_params.l_D,
            nVar, 
            &self.eta_inv_li_o_inter_alpha4_kj, 
            placement_variables, 
            subcircuit_infos
        );

        // let mut aligned_rs = vec![G1Affine::zero(); nVar];
        // let mut aligned_wtns = vec![ScalarField::zero(); nVar];
        // let mut cnt: usize = 0;
        // for i in 0..placement_variables.len() {
        //     let subcircuit_id = placement_variables[i].subcircuitId;
        //     let variables = &placement_variables[i].variables;
        //     let subcircuit_info = &subcircuit_infos[subcircuit_id];
        //     let flatten_map = &subcircuit_info.flattenMap;
        //     // Filterling out interface wires
        //     let (start_idx, end_idx_exclusive) = if subcircuit_info.name == "bufferPubOut" {
        //         // PUBLIC_OUT
        //         (subcircuit_info.In_idx[0],  subcircuit_info.In_idx[0] + subcircuit_info.In_idx[1])
        //     } else if subcircuit_info.name == "bufferPubIn" {
        //         // PUBLIC_IN
        //         (subcircuit_info.Out_idx[0], subcircuit_info.Out_idx[0] + subcircuit_info.Out_idx[1])
        //     } else if subcircuit_info.name == "bufferBlockIn" {
        //         // BLOCK_IN
        //         (subcircuit_info.Out_idx[0], subcircuit_info.Out_idx[0] + subcircuit_info.Out_idx[1])
        //     } else if subcircuit_info.name == "bufferEVMIn" {
        //         // EVM_IN
        //         (subcircuit_info.Out_idx[0], subcircuit_info.Out_idx[0] + subcircuit_info.Out_idx[1])
        //     } else {
        //         (subcircuit_info.Out_idx[0], subcircuit_info.Out_idx[0] + subcircuit_info.Out_idx[1] + subcircuit_info.In_idx[1])
        //     };

        //     for j in start_idx..end_idx_exclusive {
        //         aligned_wtns[cnt] = ScalarField::from_hex(&variables[j]);
        //         let global_idx = flatten_map[j] - setup_params.l;
        //         let curve_point = self.eta_inv_li_o_inter_alpha4_kj[global_idx][i].0;
        //         aligned_rs[cnt] = curve_point;
        //         cnt += 1;
        //     }        
        // }
        // let mut msm_res = vec![G1Projective::zero(); 1];
        // msm::msm(
        //     HostSlice::from_slice(&aligned_wtns),
        //     HostSlice::from_slice(&aligned_rs),
        //     &MSMConfig::default(),
        //     HostSlice::from_mut_slice(&mut msm_res)
        // ).unwrap();
        // return G1serde(G1Affine::from(msm_res[0]))
    }

    pub fn encode_O_prv_no_zk(
        &self,
        placement_variables: &[PlacementVariables],
        subcircuit_infos: &[SubcircuitInfo],
        setup_params: &SetupParams
    ) -> G1serde {
        let mut nVar: usize = 0;
        for i in 0..placement_variables.len() {
            let subcircuit_id = placement_variables[i].subcircuitId;
            let subcircuit_info = &subcircuit_infos[subcircuit_id];
            nVar = nVar + ( subcircuit_info.Nwires - subcircuit_info.In_idx[1] - subcircuit_info.Out_idx[1] );
        }

        return self._encode_statement(
            setup_params.l_D,
            setup_params.m_D,
            nVar, 
            &self.delta_inv_li_o_prv, 
            placement_variables, 
            subcircuit_infos
        );

        // let mut aligned_rs = vec![G1Affine::zero(); nVar];
        // let mut aligned_wtns = vec![ScalarField::zero(); nVar];
        // let mut cnt: usize = 0;
        // for i in 0..placement_variables.len() {
        //     let subcircuit_id = placement_variables[i].subcircuitId;
        //     let variables = &placement_variables[i].variables;
        //     let subcircuit_info = &subcircuit_infos[subcircuit_id];
        //     let flatten_map = &subcircuit_info.flattenMap;
        //     for j in 0..subcircuit_info.Nwires {
        //         if flatten_map[j] >= setup_params.l_D {
        //             let global_idx = flatten_map[j] - setup_params.l_D;
        //             aligned_wtns[cnt] = ScalarField::from_hex(&variables[j]);
        //             let curve_point = self.delta_inv_li_o_prv[global_idx][i].0;
        //             aligned_rs[cnt] = curve_point;
        //             cnt += 1;
        //         }
        //     }        
        // }
        // let mut msm_res = vec![G1Projective::zero(); 1];
        // msm::msm(
        //     HostSlice::from_slice(&aligned_wtns),
        //     HostSlice::from_slice(&aligned_rs),
        //     &MSMConfig::default(),
        //     HostSlice::from_mut_slice(&mut msm_res)
        // ).unwrap();
        // return G1serde(G1Affine::from(msm_res[0]))
    }

    fn _encode_statement(
        &self,
        global_wire_index_offset: usize,
        global_wire_index_end: usize,
        nVar: usize,
        bases: &Box<[Box<[G1serde]>]>,
        placement_variables: &[PlacementVariables],
        subcircuit_infos: &[SubcircuitInfo],
    ) -> G1serde {
        let mut aligned_rs = vec![G1Affine::zero(); nVar];
        let mut aligned_variable = vec![ScalarField::zero(); nVar];
        let mut cnt: usize = 0;
        for i in 0..placement_variables.len() {
            let subcircuit_id = placement_variables[i].subcircuitId;
            let variables = &placement_variables[i].variables;
            let subcircuit_info = &subcircuit_infos[subcircuit_id];
            let flatten_map = &subcircuit_info.flattenMap;
            for j in 0..subcircuit_info.Nwires {
                if flatten_map[j] >= global_wire_index_offset && flatten_map[j] < global_wire_index_end {
                    let global_idx = flatten_map[j] - global_wire_index_offset;
                    aligned_variable[cnt] = ScalarField::from_hex(&variables[j]);
                    let curve_point = bases[global_idx][i].0;
                    aligned_rs[cnt] = curve_point;
                    cnt += 1;
                }
            }        
        }
        let mut msm_res = vec![G1Projective::zero(); 1];
        msm::msm(
            HostSlice::from_slice(&aligned_variable),
            HostSlice::from_slice(&aligned_rs),
            &MSMConfig::default(),
            HostSlice::from_mut_slice(&mut msm_res)
        ).unwrap();
        return G1serde(G1Affine::from(msm_res[0]))
    }
}
/// This corresponds to Ïƒ_2 in the paper:
/// Ïƒ_2 := (Î±, Î±^2, Î±^3, Î±^4, Î³, Î´, Î·, x, y)
#[derive(Debug, Clone, Deserialize, Serialize, Copy)]
pub struct Sigma2 {
    pub alpha: G2serde,
    pub alpha2: G2serde,
    pub alpha3: G2serde,
    pub alpha4: G2serde,
    pub gamma: G2serde,
    pub delta: G2serde,
    pub eta: G2serde,
    pub x: G2serde,
    pub y: G2serde,
}

impl Sigma2 {
    /// Generate CRS elements for trapdoor component
    pub fn gen(
        tau: &Tau,
        g2_gen: &G2Affine
    ) -> Self {
        println!("Generating Sigma2 components...");
        let alpha = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.alpha));
        let alpha2 = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.alpha.pow(2)));
        let alpha3 = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.alpha.pow(3)));
        let alpha4 = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.alpha.pow(4)));
        let gamma = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.gamma));
        let delta = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.delta));
        let eta = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.eta));
        let x = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.x));
        let y = G2serde(G2Affine::from((*g2_gen).to_projective() * tau.y));
        
        Self {
            alpha,
            alpha2,
            alpha3,
            alpha4,
            gamma,
            delta,
            eta,
            x,
            y
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SigmaPreprocess {
    pub sigma_1: PartialSigma1
}
#[derive(Debug, Serialize, Deserialize)]
pub struct PartialSigma1 {
    pub xy_powers: Box<[G1serde]>,
}
impl_encode_poly!(PartialSigma1);

// impl PartialSigma1 {
//     pub fn encode_O_function_inst(
//         &self,
//         a_pub_function: &[String],
//     ) -> G1serde {
//         let mut msm_res = vec![G1Projective::zero(); 1];
//         if a_pub_function.len() != self.gamma2_inv_o_function_inst.len() {
//             panic!("Public function instance length mismatch with corresponding CRS elements")
//         }
//         let scalars_field = a_pub_function.iter().map( |val| ScalarField::from_hex(val)).collect::<Vec<_>>().into_boxed_slice();
//         let bases_G1 = self.gamma2_inv_o_function_inst.iter().map(|serde| serde.0).collect::<Vec<_>>().into_boxed_slice();
//         msm::msm(
//             HostSlice::from_slice(&scalars_field),
//             HostSlice::from_slice(&bases_G1),
//             &MSMConfig::default(),
//             HostSlice::from_mut_slice(&mut msm_res)
//         ).unwrap();

//         G1serde(G1Affine::from(msm_res[0]))
//     }
    
//     pub fn encode_O_block_inst(
//         &self,
//         a_pub_block: &[String],
//     ) -> G1serde {
//         let mut msm_res = vec![G1Projective::zero(); 1];
//         if a_pub_block.len() != self.gamma2_inv_o_block_inst.len() {
//             panic!("Public block instance length mismatch with corresponding CRS elements")
//         }
//         let scalars_field = a_pub_block.iter().map( |val| ScalarField::from_hex(val)).collect::<Vec<_>>().into_boxed_slice();
//         let bases_G1 = self.gamma2_inv_o_block_inst.iter().map(|serde| serde.0).collect::<Vec<_>>().into_boxed_slice();
//         msm::msm(
//             HostSlice::from_slice(&scalars_field),
//             HostSlice::from_slice(&bases_G1),
//             &MSMConfig::default(),
//             HostSlice::from_mut_slice(&mut msm_res)
//         ).unwrap();

//         G1serde(G1Affine::from(msm_res[0]))
//     }
// }

#[derive(Debug, Serialize, Deserialize)]
pub struct PartialSigma1Verify {
    pub x: G1serde,
    pub y: G1serde,
}
#[derive(Debug, Serialize, Deserialize)]
pub struct SigmaVerify {
    pub G: G1serde,
    pub H: G2serde,
    pub sigma_1: PartialSigma1Verify,
    pub sigma_2: Sigma2,
    pub lagrange_KL: G1serde,
}

#[derive(Clone, Debug, Copy, PartialEq)]
pub struct G1serde(pub G1Affine);
impl G1serde {
    pub fn zero() -> Self {
        Self(G1Affine::zero())
    }
}
impl Add for G1serde {
    type Output = Self;

    fn add(self, other: Self) -> Self {
        G1serde(G1Affine::from(self.0.to_projective() + other.0.to_projective()))
    }
}

impl Sub for G1serde {
    type Output = Self;

    fn sub(self, other: Self) -> Self {
        G1serde(G1Affine::from(self.0.to_projective() - other.0.to_projective()))
    }
}

// G1serde * original field
impl Mul<ScalarField> for G1serde {
    type Output = Self;

    fn mul(self, other: ScalarField) -> Self {
        G1serde(G1Affine::from(self.0.to_projective() * other))
    }
}
// original field * G1serde
impl Mul<G1serde> for ScalarField {
    type Output = G1serde;

    fn mul(self, other: G1serde) -> Self::Output {
        G1serde(G1Affine::from(other.0.to_projective() * self))
    }
}

// G1serde * FieldSerde
impl Mul<FieldSerde> for G1serde {
    type Output = Self;

    fn mul(self, other: FieldSerde) -> Self {
        G1serde(G1Affine::from(self.0.to_projective() * other.0))
    }
}
// original field * G1serde
impl Mul<G1serde> for FieldSerde {
    type Output = G1serde;

    fn mul(self, other: G1serde) -> Self::Output {
        G1serde(G1Affine::from(other.0.to_projective() * self.0))
    }
}


#[derive(Clone, Debug, Copy, PartialEq)]
pub struct G2serde(pub G2Affine);
impl G2serde {
    pub fn zero() -> Self {
        Self(G2Affine::zero())
    }
}
//new added for G2Serde
impl Add for G2serde {
    type Output = Self;

    fn add(self, other: Self) -> Self {
        G2serde(G2Affine::from(self.0.to_projective() + other.0.to_projective()))
    }
}

impl Sub for G2serde {
    type Output = Self;

    fn sub(self, other: Self) -> Self {
        G2serde(G2Affine::from(self.0.to_projective() - other.0.to_projective()))
    }
}

// G2serde * original field
impl Mul<ScalarField> for G2serde {
    type Output = Self;

    fn mul(self, other: ScalarField) -> Self {
        G2serde(G2Affine::from(self.0.to_projective() * other))
    }
}
// original field * G2serde
impl Mul<G2serde> for ScalarField {
    type Output = G2serde;

    fn mul(self, other: G2serde) -> Self::Output {
        G2serde(G2Affine::from(other.0.to_projective() * self))
    }
}

// G2serde * FieldSerde
impl Mul<FieldSerde> for G2serde {
    type Output = Self;

    fn mul(self, other: FieldSerde) -> Self {
        G2serde(G2Affine::from(self.0.to_projective() * other.0))
    }
}
// original field * G2serde
impl Mul<G2serde> for FieldSerde {
    type Output = G2serde;

    fn mul(self, other: G2serde) -> Self::Output {
        G2serde(G2Affine::from(other.0.to_projective() * self.0))
    }
}

pub fn icicle_g1_affine_to_ark(g: &G1Affine) -> ArkG1Affine {
    let x_bytes = g.x.to_bytes_le();
    let y_bytes = g.y.to_bytes_le();
    let x = ark_bls12_381::Fq::from_random_bytes(&x_bytes)
        .expect("failed to convert x from icicle to ark");
    let y = ark_bls12_381::Fq::from_random_bytes(&y_bytes)
        .expect("failed to convert y from icicle to ark");
    ArkG1Affine::new_unchecked(x, y)
}

pub fn icicle_g2_affine_to_ark(g: &G2Affine) -> ArkG2Affine {
    let x_bytes = g.x.to_bytes_le();
    let y_bytes = g.y.to_bytes_le();
    
    let x = ark_bls12_381::Fq2::from_random_bytes(&x_bytes)
        .expect("failed to convert x from icicle to ark");
    let y = ark_bls12_381::Fq2::from_random_bytes(&y_bytes)
        .expect("failed to convert y from icicle to ark");
    
    ArkG2Affine::new_unchecked(x, y)
}



================================================
FILE: packages/backend/libs/src/iotools/mod.rs
================================================
use icicle_bls12_381::curve::{ScalarField, G1Affine, G2Affine, BaseField, G2BaseField, G1Projective};
use icicle_core::ntt;
use icicle_core::traits::{Arithmetic, FieldImpl};
use icicle_core::msm::{self, MSMConfig};
use icicle_runtime::{self, memory::{HostSlice, DeviceVec}};
use rayon::iter::{IndexedParallelIterator, IntoParallelIterator, IntoParallelRefMutIterator, ParallelIterator};
use crate::field_structures::FieldSerde;
use crate::group_structures::{G1serde, G2serde, PartialSigma1, PartialSigma1Verify, Sigma, Sigma1, Sigma2, SigmaPreprocess, SigmaVerify};
use crate::bivariate_polynomial::{BivariatePolynomial, DensePolynomialExt};
use crate::polynomial_structures::{from_subcircuit_to_QAP, QAP};
use crate::utils::{check_gpu};
use crate::vector_operations::transpose_inplace;

use super::vector_operations::{*};

use std::fs::{self, File};
use std::io::{self, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::{env, fmt};
use std::collections::{HashMap, HashSet};
use std::time::Instant;
use std::ops::Deref;
use num_bigint::BigUint;
use serde::{Deserialize, Serialize};
use serde::ser::{SerializeStruct};
use serde::de::{Deserializer, Visitor, Error};
use serde_json::{to_writer_pretty};
use hex::decode_to_slice;

#[macro_export]
macro_rules! impl_read_from_json {
    ($t:ty) => {
        impl $t {
            pub fn read_from_json(path: PathBuf) -> std::io::Result<Self> {
                use std::io::BufReader;
                use std::fs::File;
                use serde_json::from_reader;
                // let abs_path = env::current_dir()?.join(path);
                let file = File::open(path)?;
                let reader = BufReader::new(file);
                let res: Self = from_reader(reader)?;
                Ok(res)
            }
        }
    };
}

#[macro_export]
macro_rules! impl_read_from_bincode {
    ($t:ty) => {
        impl $t {
            pub fn read_from_bincode(path: PathBuf) -> std::io::Result<Self> {
                let data = std::fs::read(path)?;
                bincode::deserialize(&data).map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))
            }
        }
    }
}

#[macro_export]
macro_rules! impl_read_box_from_json {
    ($t:ty) => {
        impl $t {
            pub fn read_box_from_json(path: PathBuf) -> io::Result<Box<[Self]>> {
                use std::io::BufReader;
                use std::fs::File;
                use serde_json::from_reader;
                // let abs_path = env::current_dir()?.join(path);
                let file = File::open(path)?;
                let reader = BufReader::new(file);
                let box_data: Box<[Self]> = from_reader(reader)?;
                Ok(box_data)
            }
        }
    };
}

#[macro_export]
macro_rules! impl_write_into_json {
    ($t:ty) => {
        impl $t {
            pub fn write_into_json(&self, path: PathBuf) -> std::io::Result<()> {
                use std::io::BufWriter;
                use std::env;
                use std::fs::{self, File};
                use serde_json::to_writer_pretty;
                if let Some(parent) = path.parent() {
                    fs::create_dir_all(parent)?;
                }
                let file = File::create(&path)?;
                let writer = BufWriter::new(file);
                to_writer_pretty(writer, self)?;
                Ok(())
            }
        }
    };
}

fn g1_vec_to_code(v: &Box<[G1serde]>) -> String {
    let inner = v.iter()
        .map(|g| g.to_rust_code())
        .collect::<Vec<_>>()
        .join(",\n        ");
    format!("Box::new([\n        {}\n    ])", inner)
}

fn g1_mat_to_code(vv: &Box<[Box<[G1serde]>]>) -> String {
    let rows = vv.iter()
        .map(|row| g1_vec_to_code(row))
        .collect::<Vec<_>>()
        .join(",\n    ");
    format!("Box::new([\n    {}\n])", rows)
}

fn byte_slice_to_literal(bytes: &[u8]) -> String {
    bytes.iter()
        .map(|b| format!("{}", b))
        .collect::<Vec<_>>()
        .join(", ")
}

// A wrapping structure to make sure hex strings read from JSON to be even-length. For compatibility with ICICLE Core.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct HexString(pub String);
impl<'de> Deserialize<'de> for HexString {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        let mut s = String::deserialize(deserializer)?;

        if let Some(hex) = s.strip_prefix("0x") {
            if hex.len() % 2 == 1 {
                s = format!("0x0{}", hex);
            }
        } else if s.len() % 2 == 1 {
            s = format!("0{}", s);
        }

        Ok(HexString(s))
    }
}
impl Deref for HexString {
    type Target = str;
    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl AsRef<str> for HexString {
    fn as_ref(&self) -> &str { &self.0 }
}

#[derive(Debug, Deserialize)]
pub struct PublicWireInfo {
    pub subcircuitId: usize,
    pub numPubWires: usize,
}

#[derive(Debug, Deserialize)]
pub struct SetupParams {
    pub l: usize,
    pub l_user_out: usize,
    pub l_user: usize,
    pub l_block: usize,
    pub l_D: usize, //m_I = l_D - 1
    pub m_D: usize,
    pub n: usize,
    pub s_D: usize,
    pub s_max: usize,
}

impl_read_from_json!(SetupParams);
impl_read_from_json!(Sigma);
impl_read_from_json!(SigmaPreprocess);
impl_read_from_json!(SigmaVerify);
impl_write_into_json!(Sigma);
impl_read_from_bincode!(Sigma);

impl Sigma {
    /// Write verifier CRS into JSON
    pub fn write_into_json_for_verify(&self, abs_path: PathBuf) -> io::Result<()> {
        if let Some(parent) = abs_path.parent() {
            fs::create_dir_all(parent)?;
        }
        let file = File::create(&abs_path)?;
        let writer = BufWriter::new(file);
        let partial_sigma1_verify: PartialSigma1Verify = PartialSigma1Verify { x: self.sigma_1.x, y: self.sigma_1.y };
        let sigma_verify: SigmaVerify = SigmaVerify {
            G: self.G,
            H: self.H,
            sigma_1: partial_sigma1_verify,
            sigma_2: self.sigma_2,
            lagrange_KL: self.lagrange_KL,
        };
        to_writer_pretty(writer, &sigma_verify)?;
        Ok(())
    }

    /// Write preprocess CRS into JSON
    pub fn write_into_json_for_preprocess(&self, abs_path: PathBuf) -> io::Result<()> {
        if let Some(parent) = abs_path.parent() {
            fs::create_dir_all(parent)?;
        }
        let file = File::create(&abs_path)?;
        let writer = BufWriter::new(file);
        let partial_sigma_1: PartialSigma1 = PartialSigma1 {
            xy_powers: self.sigma_1.xy_powers.clone(),
            // gamma2_inv_o_env_inst: self.sigma_1.gamma2_inv_o_env_inst.clone(),
        };
        let sigma_preprocess: SigmaPreprocess = SigmaPreprocess {
            sigma_1: partial_sigma_1
        };
        to_writer_pretty(writer, &sigma_preprocess)?;
        Ok(())
    }

    pub fn write_into_rust_code(&self, path: &str) -> io::Result<()> {
        let abs_path = env::current_dir()?.join(path);

        if let Some(parent) = abs_path.parent() {
            fs::create_dir_all(parent)?;
        }

        let mut f = File::create(&abs_path)?;

        writeln!(
            f,
            "// This file is auto-generated by setup.\n\npub static SIGMA: Sigma = {};\n",
            self.to_rust_code()
        )?;

        println!("Sigma Rust code saved at {:?}", abs_path);
        Ok(())
    }

    pub fn to_rust_code(&self) -> String {
        format!(
            "Sigma {{
                G: {},
                H: {},
                sigma_1: {},
                sigma_2: {}
            }}",
            self.G.to_rust_code(),
            self.H.to_rust_code(),
            self.sigma_1.to_rust_code(),
            self.sigma_2.to_rust_code()
        )
    }
}

impl Sigma1 {
    pub fn to_rust_code(&self) -> String {
        format!(
            "Sigma1 {{
                xy_powers: {},
                delta: {},
                eta: {},
                gamma_inv_o_inst: {},
                eta_inv_li_o_inter_alpha4_kj: {},
                delta_inv_li_o_prv: {},
                delta_inv_alphak_xh_tx: {},
                delta_inv_alpha4_xj_tx: {},
                delta_inv_alphak_yi_ty: {}
            }}",
            g1_vec_to_code(&self.xy_powers),
            self.delta.to_rust_code(),
            self.eta.to_rust_code(),
            g1_vec_to_code(&self.gamma_inv_o_inst),
            g1_mat_to_code(&self.eta_inv_li_o_inter_alpha4_kj),
            g1_mat_to_code(&self.delta_inv_li_o_prv),
            g1_mat_to_code(&self.delta_inv_alphak_xh_tx),
            g1_vec_to_code(&self.delta_inv_alpha4_xj_tx),
            g1_mat_to_code(&self.delta_inv_alphak_yi_ty),
        )
    }
}

impl Sigma2 {
    pub fn to_rust_code(&self) -> String {
        format!(
            "Sigma2 {{
                alpha: {},
                alpha2: {},
                alpha3: {},
                alpha4: {},
                gamma: {},
                delta: {},
                eta: {},
                x: {},
                y: {}
            }}",
            self.alpha.to_rust_code(),
            self.alpha2.to_rust_code(),
            self.alpha3.to_rust_code(),
            self.alpha4.to_rust_code(),
            self.gamma.to_rust_code(),
            self.delta.to_rust_code(),
            self.eta.to_rust_code(),
            self.x.to_rust_code(),
            self.y.to_rust_code(),
        )
    }
}


#[derive(Debug, Deserialize, Clone)]
pub struct PlacementVariables {
    pub subcircuitId: usize,
    pub variables: Box<[HexString]>,
}

impl_read_box_from_json!(PlacementVariables);

#[derive(Debug, Deserialize)]
pub struct OutPts {
    pub extDest: String,
    pub key: String,
    pub offset: usize,
    pub valueHex: String,
}

#[derive(Debug, Deserialize)]
pub struct PublicOutputBuffer {
    pub outPts: Box<[OutPts]>
}

#[derive(Debug, Deserialize)]
pub struct InPts {
    pub extSource: String,
    pub key: String,
    pub valueHex: String,
}

#[derive(Debug, Deserialize)]
pub struct PublicInputBuffer {
    pub inPts: Box<[InPts]>
}

#[derive(Debug, Deserialize)]
pub struct Instance {
    pub a_pub_user: Box<[HexString]>,
    pub a_pub_block: Box<[HexString]>,
    pub a_pub_function: Box<[HexString]>,
}

impl_read_from_json!(Instance);

#[derive(Debug, Deserialize)]
pub struct Permutation {
    pub row: usize,
    pub col: usize,
    pub X: usize,
    pub Y: usize,
}

impl_read_box_from_json!(Permutation);

impl Permutation {
    pub fn to_poly(perm_raw: &[Self], m_i: usize, s_max: usize) -> (DensePolynomialExt, DensePolynomialExt) {
        let omega_m_i = ntt::get_root_of_unity::<ScalarField>(m_i as u64);
        let omega_s_max = ntt::get_root_of_unity::<ScalarField>(s_max as u64);
        let mut s0_evals_vec = vec![ScalarField::zero(); m_i * s_max];
        let mut s1_evals_vec = vec![ScalarField::zero(); m_i * s_max];
        // Initialization
        for row_idx in 0..m_i {
            for col_idx in 0..s_max {
                s0_evals_vec[row_idx * s_max + col_idx] = omega_m_i.pow(row_idx);
                s1_evals_vec[row_idx * s_max + col_idx] = omega_s_max.pow(col_idx);
            }
        }
        // Reflecting the actual values
        for i in 0..perm_raw.len() {
            let idx = perm_raw[i].row * s_max + perm_raw[i].col;
            s0_evals_vec[idx] = omega_m_i.pow(perm_raw[i].X);
            s1_evals_vec[idx] = omega_s_max.pow(perm_raw[i].Y);            
        }
        let s0_evals = HostSlice::from_slice(&s0_evals_vec);
        let s1_evals = HostSlice::from_slice(&s1_evals_vec);
        return (
            DensePolynomialExt::from_rou_evals(s0_evals, m_i, s_max, None, None),
            DensePolynomialExt::from_rou_evals(s1_evals, m_i, s_max, None, None),
        ) 
    }
}

#[derive(Debug, Deserialize)]
pub struct SubcircuitInfo {
    pub id: usize,
    pub name: String,
    pub Nwires: usize,
    pub Nconsts: usize,
    pub Out_idx: Box<[usize]>,
    pub In_idx: Box<[usize]>,
    pub flattenMap: Box<[usize]>,
}

impl_read_box_from_json!(SubcircuitInfo);

pub fn read_global_wire_list_as_boxed_boxed_numbers(path: PathBuf) -> io::Result<Box<[Box<[usize]>]>> {
    // let abs_path = env::current_dir()?.join(QAP_COMPILER_PATH_PREFIX).join(path);
    let file = File::open(path)?;
    let reader = BufReader::new(file);

    let vec_of_vecs:Vec<Vec<i32>> = serde_json::from_reader(reader)?;
    let boxed_matrix: Box<[Box<[usize]>]> = vec_of_vecs
        .into_iter()
        .map(|row| row.into_iter().map(|x| x as usize).collect::<Vec<_>>().into_boxed_slice())
        .collect::<Vec<_>>()
        .into_boxed_slice();
    Ok(boxed_matrix)
}

#[derive(Debug, Deserialize)]
struct Constraints {
    constraints: Vec<Vec<HashMap<usize, String>>>,
}

impl_read_from_json!(Constraints);

impl Constraints {

    fn convert_values_to_hex(constraints: &mut Self) {
        for constraint_group in constraints.constraints.iter_mut() {
            for hashmap in constraint_group.iter_mut() {
                for (_, value) in hashmap.iter_mut() {
                    if let Ok(num) = value.parse::<BigUint>() {
                        let hex = format!("{:X}", num);
                        *value = if hex.len() % 2 == 1 {
                            let mut s = String::with_capacity(hex.len() + 1);
                            s.push('0');
                            s.push_str(&hex);
                            s
                        } else {
                            hex
                        };
                    }
                }
            }
        }
    }
}

pub struct SubcircuitR1CS{
    pub A_compact_col_mat: Vec<ScalarField>,
    pub B_compact_col_mat: Vec<ScalarField>,
    pub C_compact_col_mat: Vec<ScalarField>,
    pub A_active_wires: Vec<usize>,
    pub B_active_wires: Vec<usize>,
    pub C_active_wires: Vec<usize>,
}

impl SubcircuitR1CS{
    pub fn from_path(path: PathBuf, setup_params: &SetupParams, subcircuit_info: &SubcircuitInfo) -> io::Result<Self> {
        let mut constraints = Constraints::read_from_json(path)?;
        Constraints::convert_values_to_hex(&mut constraints);

        let mut A_active_wire_indices_set = HashSet::<usize>::new();
        let mut B_active_wire_indices_set = HashSet::<usize>::new();
        let mut C_active_wire_indices_set = HashSet::<usize>::new();

        for const_idx in 0..subcircuit_info.Nconsts {
            let constraint = &constraints.constraints[const_idx];
            // ê° constraintì˜ í‚¤ë“¤ì„ active setì— í™•ì¥(ì¬í• ë‹¹ ì—†ì´)
            A_active_wire_indices_set.extend(constraint[0].keys().copied());
            B_active_wire_indices_set.extend(constraint[1].keys().copied());
            C_active_wire_indices_set.extend(constraint[2].keys().copied());
        }

        let mut A_active_wire_indices: Vec<usize> = A_active_wire_indices_set.iter().map(|&idx| idx).collect();
        A_active_wire_indices.sort();
        let mut B_active_wire_indices: Vec<usize> = B_active_wire_indices_set.iter().map(|&idx| idx).collect();
        B_active_wire_indices.sort();
        let mut C_active_wire_indices: Vec<usize> = C_active_wire_indices_set.iter().map(|&idx| idx).collect();
        C_active_wire_indices.sort();     

        let n = setup_params.n; // used as the number of rows.
        if n < subcircuit_info.Nconsts {
            panic!("n is smaller than the actual number of constraints.");
        }
        // used as the numbers of columns (different by the R1CS matrices).
        let A_len = A_active_wire_indices.len();
        let B_len = B_active_wire_indices.len();
        let C_len = C_active_wire_indices.len();
        if A_len > subcircuit_info.Nwires || B_len > subcircuit_info.Nwires || C_len > subcircuit_info.Nwires{
            panic!("Incorrectly counted number of wires.");
        }
        
        // Each of a_mat_vec, b_mat_vec, and c_mat_vec is, respectively, not a matrix but just a vector of vectors (of irregular lengths).
        let mut A_compact_col_mat = vec![ScalarField::zero(); n * A_len];
        let mut B_compact_col_mat = vec![ScalarField::zero(); n * B_len];
        let mut C_compact_col_mat = vec![ScalarField::zero(); n * C_len];
        
        for row_idx in 0..subcircuit_info.Nconsts {
            let constraint = &constraints.constraints[row_idx];
            let a_constraint = &constraint[0];
            let b_constraint = &constraint[1];
            let c_constraint = &constraint[2];
        
            for col_idx in 0..A_len {
                if let Some(hex_val) = a_constraint.get(&A_active_wire_indices[col_idx]) {
                    let idx = A_len * row_idx + col_idx;
                    A_compact_col_mat[idx] = ScalarField::from_hex(hex_val);
                }
            }

            for col_idx in 0..B_len {
                if let Some(hex_val) = b_constraint.get(&B_active_wire_indices[col_idx]) {
                    let idx = B_len * row_idx + col_idx;
                    B_compact_col_mat[idx] = ScalarField::from_hex(hex_val);
                }
            }

            for col_idx in 0..C_len {
                if let Some(hex_val) = c_constraint.get(&C_active_wire_indices[col_idx]) {
                    let idx = C_len * row_idx + col_idx;
                    C_compact_col_mat[idx] = ScalarField::from_hex(hex_val);
                }
            }
        }
        // IMPORTANT: A, B, C matrices are of size A_len-by-n, B_len-by-n, and C_len-by-n, respectively.
        // They must be transposed before being converted into bivariate polynomials.
        transpose_inplace(&mut A_compact_col_mat, n, A_len);
        transpose_inplace(&mut B_compact_col_mat, n, B_len);
        transpose_inplace(&mut C_compact_col_mat, n, C_len);
        
        Ok(Self {
            A_compact_col_mat,
            B_compact_col_mat,
            C_compact_col_mat,
            A_active_wires: A_active_wire_indices,
            B_active_wires: B_active_wire_indices,
            C_active_wires: C_active_wire_indices,
        })
    }
    
}

impl QAP{
    pub fn gen_from_R1CS(
        qap_path: &PathBuf,
        subcircuit_infos: &Box<[SubcircuitInfo]>,
        setup_params: &SetupParams,
    ) -> Self {
        let m_d = setup_params.m_D;
        let s_d = setup_params.s_D;

        let global_wire_list_path = qap_path.join("globalWireList.json");
        let global_wire_list = read_global_wire_list_as_boxed_boxed_numbers(global_wire_list_path).unwrap();

        let zero_poly = DensePolynomialExt::zero();
        let mut u_j_X = vec![zero_poly.clone(); m_d];
        let mut v_j_X = vec![zero_poly.clone(); m_d];
        let mut w_j_X = vec![zero_poly.clone(); m_d];

        for i in 0..s_d {
            println!("Processing subcircuit id {}", i);
            
            let r1cs_path = qap_path.join(format!("json/subcircuit{i}.json"));
            let compact_r1cs = SubcircuitR1CS::from_path(r1cs_path, &setup_params, &subcircuit_infos[i]).unwrap();
            let (u_j_X_local, v_j_X_local, w_j_X_local) = from_subcircuit_to_QAP(
                &compact_r1cs,
                &setup_params,
                &subcircuit_infos[i]
            );
            
            // Map local wire indices to global wire indices
            let flatten_map = &subcircuit_infos[i].flattenMap;

            for local_idx in 0..subcircuit_infos[i].Nwires {
                let global_idx = flatten_map[local_idx];

                // Verify global wire list consistency with flatten map
                if (global_wire_list[global_idx][0] != subcircuit_infos[i].id) || 
                   (global_wire_list[global_idx][1] != local_idx) {
                    panic!("GlobalWireList is not the inverse of flattenMap.");
                }

                u_j_X[global_idx] = u_j_X_local[local_idx].clone();
                v_j_X[global_idx] = v_j_X_local[local_idx].clone();
                w_j_X[global_idx] = w_j_X_local[local_idx].clone();
            }
        }
        return Self {u_j_X, v_j_X, w_j_X}
    }

    
}

impl Serialize for FieldSerde {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where S: serde::Serializer {
        let string = self.0.to_string();
        serializer.serialize_str(&string)
    }
}

impl<'de> Deserialize<'de> for FieldSerde {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where D: Deserializer<'de>
    {
        struct FieldVisitor;

        impl<'de> Visitor<'de> for FieldVisitor {
            type Value = FieldSerde;

            fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                formatter.write_str("a hex string representing ScalarField")
            }

            fn visit_str<E>(self, v: &str) -> Result<Self::Value, E>
            where
                E: Error,
            {
                let scalar = ScalarField::from_hex(v);
                Ok(FieldSerde(scalar))
            }
        }

        deserializer.deserialize_str(FieldVisitor)
    }
}

impl Serialize for G1serde {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where S: serde::Serializer {
        let mut s = serializer.serialize_struct("G1serde", 2)?;
        let x_coord = &self.0.x.to_string();
        let y_coord = &self.0.y.to_string();
        s.serialize_field("x", x_coord)?;
        s.serialize_field("y", y_coord)?;
        s.end()
    }
}
impl<'de> Deserialize<'de> for G1serde {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where D: serde::Deserializer<'de> {
        #[derive(Deserialize)]
        struct G1Coords {
            x: String,
            y: String,
        }
        let G1Coords { x, y } = G1Coords::deserialize(deserializer)?;
        let x_field = BaseField::from_hex(&x).into();
        let y_field = BaseField::from_hex(&y).into();
        let point = G1Affine::from_limbs(x_field, y_field);
        Ok(G1serde(point))
    }
}
impl G1serde {
    pub fn to_rust_code(&self) -> String {
        let x_bytes = self.0.x.to_bytes_le();
        let y_bytes = self.0.y.to_bytes_le();
        
        format!(
            "G1serde(G1Affine::from_limbs(BaseField::from_bytes_le(&[{}]).into(),BaseField::from_bytes_le(&[{}]).into()))",
            byte_slice_to_literal(&x_bytes),
            byte_slice_to_literal(&y_bytes),
        )
    }
}

impl Serialize for G2serde {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where S: serde::Serializer {
        let mut s = serializer.serialize_struct("G2", 2)?;
        let x_coord = &self.0.x.to_string();
        let y_coord = &self.0.y.to_string();
        s.serialize_field("x", x_coord)?;
        s.serialize_field("y", y_coord)?;
        s.end()
    }
}
impl<'de> Deserialize<'de> for G2serde {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where D: serde::Deserializer<'de> {
        #[derive(Deserialize)]
        struct G2Coords {
            x: String,
            y: String,
        }
        let G2Coords { x, y } = G2Coords::deserialize(deserializer)?;
        let x_field = G2BaseField::from_hex(&x).into();
        let y_field = G2BaseField::from_hex(&y).into();
        let point = G2Affine::from_limbs(x_field, y_field);
        Ok(G2serde(point))
    }
}
impl G2serde {
    pub fn to_rust_code(&self) -> String {
        let x_bytes = self.0.x.to_bytes_le();
        let y_bytes = self.0.y.to_bytes_le();
        
        format!(
            "G2serde(G2Affine::from_limbs(G2BaseField::from_bytes_le(&[{}]).into(),G2BaseField::from_bytes_le(&[{}]).into()))",
            byte_slice_to_literal(&x_bytes),
            byte_slice_to_literal(&y_bytes),
        )
    }
}


pub fn scaled_outer_product_2d(
    col_vec: &[ScalarField], 
    row_vec: &[ScalarField], 
    g1_gen: &G1Affine, 
    scaler: Option<&ScalarField>, 
    res: &mut Box<[Box<[G1serde]>]>
) {
    let row_size = col_vec.len();
    let col_size = row_vec.len();
    let size = col_size * row_size;
    if res.len() > 0 {
        if res.len() * res[0].len() != size {
            panic!("Insufficient buffer length");
        }
    } else {
        panic!("Empty buffer");
    }
    
    let mut res_coef = vec![ScalarField::zero(); size].into_boxed_slice();
    scaled_outer_product(col_vec, row_vec, scaler, &mut res_coef);
    from_coef_vec_to_g1serde_mat(
        &res_coef,
        row_size,
        col_size,
        g1_gen,
        res,
    );
}

pub fn scaled_outer_product_1d(
    col_vec: &[ScalarField], 
    row_vec: &[ScalarField],
    g1_gen: &G1Affine, 
    scaler: Option<&ScalarField>, 
    res: &mut [G1serde]
) {
    let col_size = col_vec.len();
    let row_size = row_vec.len();
    let size = col_size * row_size;
    if res.len() != size {
        panic!("Insufficient buffer length");
    }
    let mut res_coef = vec![ScalarField::zero(); size].into_boxed_slice();
    scaled_outer_product(col_vec, row_vec, scaler, &mut res_coef);
    from_coef_vec_to_g1serde_vec(
        &res_coef,
        g1_gen,
        res,
    );
}

pub fn from_coef_vec_to_g1serde_vec_msm(
    coef: &[ScalarField],
    gen: &G1Affine,
    res: &mut [G1serde],
) {
    println!("msm");
    let n = coef.len();

    let t_start = Instant::now();

    let scalars_host = HostSlice::from_slice(coef.as_ref());

    let mut pts = Vec::with_capacity(n);
    pts.resize(n, *gen);

    let points_host = HostSlice::from_slice(&pts);

    let mut result_dev = DeviceVec::<G1Projective>::device_malloc(n)
        .expect("device_malloc failed");

    let cfg = msm::MSMConfig::default();

    msm::msm(
        scalars_host,       // &[ScalarField]
        points_host,        // &[G1Affine]
        &cfg,
        &mut result_dev[..] // &mut DeviceSlice<G1Projective>
    ).expect("msm failed");

    let mut host_out = vec![G1Projective::zero(); n];
    result_dev
        .copy_to_host(HostSlice::from_mut_slice(&mut host_out))
        .expect("copy_to_host failed");
  
    drop(result_dev);
    // drop(points_host);
    // drop(scalars_host);
    
    host_out
        .into_par_iter()
        .zip(res.par_iter_mut())
        .for_each(|(proj, slot)| {
            *slot = G1serde(G1Affine::from(proj));
        });

    println!("Total elapsed: {:?}", t_start.elapsed());
}

pub fn from_coef_vec_to_g1serde_vec(coef: &[ScalarField], gen: &G1Affine, res: &mut [G1serde]) {
    if check_gpu() {
        from_coef_vec_to_g1serde_vec_msm(&coef.to_vec().into_boxed_slice(), gen, res);
    } else {
        use std::sync::atomic::{AtomicU32, Ordering};
        use rayon::prelude::*;
        use std::io::{stdout, Write};

        let t_start = Instant::now();

        if res.len() != coef.len() {
            panic!("Not enough buffer length.")
        }
        if coef.len() == 0 {
            return
        }

        let gen_proj = gen.to_projective(); 

        let cnt = AtomicU32::new(1);
        let progress = AtomicU32::new(0);
        let _tick: u32 = std::cmp::max(coef.len() as u32 / 10, 1);
        let tick = AtomicU32::new(_tick);
        res.par_iter_mut()
            .zip(coef.par_iter())
            .for_each(|(r, &c)| {
                *r = G1serde(G1Affine::from(gen_proj * c));
                let current_cnt = cnt.fetch_add(1, Ordering::Relaxed);
                let target_tick = tick.load(Ordering::Relaxed);
                if current_cnt >= target_tick {
                    if tick
                    .compare_exchange(target_tick, target_tick + _tick, Ordering::Relaxed, Ordering::Relaxed)
                    .is_ok()
                    {
                        progress.fetch_add(10, Ordering::Relaxed);
                        let new_progress = progress.load(Ordering::Relaxed);
                        print!("\rProgress: {}%, {} elements out of {}.", new_progress, current_cnt, coef.len());
                        stdout().flush().unwrap();
                    }
                }
            });
        println!("\n");
        println!("Total elapsed: {:?}", t_start.elapsed());
        print!("\r");
    }
}

pub fn gen_g1serde_vec_of_xy_monomials(
    x: ScalarField,
    y: ScalarField,
    gen: &G1Affine,
    x_size: usize,
    y_size: usize,
    res: &mut [G1serde],
) {
    use rayon::prelude::*;
    if res.len() != x_size * y_size {
        panic!("Not enough buffer length.")
    }
    if x_size * y_size == 0 {
        return
    }

    let gen_proj = G1Projective::from(*gen);

    let is_row_base = if x_size <= y_size {
        true
    } else {
        false
    };
    
    let outer_loop_len = if is_row_base { x_size } else { y_size };
    let inner_loop_len = if is_row_base { y_size } else { x_size };
    let mut res_projective = vec![G1Projective::zero(); x_size * y_size]; 

    let mut base_vec = vec![G1Projective::zero(); inner_loop_len];
    let base_multiplier = if is_row_base {y} else {x};
    base_vec
        .par_iter_mut()
        .enumerate()
        .for_each(|(i, b)| {
            *b = gen_proj * base_multiplier.pow(i);
        });

    res_projective[0..inner_loop_len].clone_from_slice(&base_vec);
    drop(base_vec);

    let acc_multiplier = if is_row_base {x} else {y};
    let mut msm_cfg = MSMConfig::default();
    msm_cfg.batch_size = inner_loop_len.try_into().unwrap();
    for i in 1..outer_loop_len {
        let (head, tail) = res_projective.split_at_mut(i * inner_loop_len);
        let prev_vec = &head[(i - 1) * inner_loop_len .. i * inner_loop_len];
        let prev_vec_affine: Vec<G1Affine> = prev_vec.iter().map(|&x| G1Affine::from(x)).collect();
        let curr_vec = &mut tail[0..inner_loop_len];
        msm::msm(
            HostSlice::from_slice(&vec![acc_multiplier; inner_loop_len]),
            HostSlice::from_slice(&prev_vec_affine),
            &msm_cfg,
            HostSlice::from_mut_slice(curr_vec)
        ).unwrap();
    }
    for i in 0..x_size {
        for j in 0..y_size {
            if is_row_base {
                res[i * y_size + j] = G1serde(G1Affine::from(res_projective[i * inner_loop_len + j]));
            }
            else {
                res[i * y_size + j] = G1serde(G1Affine::from(res_projective[j * inner_loop_len + i]));
            }
        }
    }
}


pub fn from_coef_vec_to_g1serde_mat(coef: &[ScalarField], r_size: usize, c_size: usize, gen: &G1Affine, res: &mut Box<[Box<[G1serde]>]>) {
    if res.len() != r_size || res.len() == 0 {
        panic!("Not enough buffer row length.")
    }
    let mut temp_vec = vec![G1serde::zero(); r_size * c_size].into_boxed_slice();
    from_coef_vec_to_g1serde_vec(coef, gen, &mut temp_vec);
    for i in 0..r_size {
        if res[i].len() != c_size {
            panic!("Not enough buffer column length.")
        }
        res[i].copy_from_slice(&temp_vec[i * c_size .. (i + 1) * c_size]);
    }
}

pub fn read_R1CS_gen_uvwXY(
    qap_path: &str,
    placement_variables: &Box<[PlacementVariables]>,
    subcircuit_infos: &Box<[SubcircuitInfo]>,
    setup_params: &SetupParams,
) -> (DensePolynomialExt, DensePolynomialExt, DensePolynomialExt)  {
    let n = setup_params.n;
    let s_max = setup_params.s_max;

    let mut u_eval = vec![ScalarField::zero(); s_max * n];
    let mut v_eval = vec![ScalarField::zero(); s_max * n];
    let mut w_eval = vec![ScalarField::zero(); s_max * n];
    
    // Avoiding loading the same subcircuit multiple times
    let mut r1cs_cache: HashMap<usize, SubcircuitR1CS> = HashMap::new();
    let mut cache_stats: HashMap<usize, usize> = HashMap::new(); // Track cache hits
    
    // Cache for hex string -> ScalarField conversions (massive speedup for repeated variables)
    let mut hex_cache: HashMap<HexString, ScalarField> = HashMap::new();
    let mut hex_cache_hits = 0usize;
    let mut hex_cache_misses = 0usize;
        
    let time_start = Instant::now();
    for i in 0..placement_variables.len() {
        let subcircuit_id = placement_variables[i].subcircuitId;
        
        // Check cache first
        let compact_r1cs = if let Some(cached) = r1cs_cache.get(&subcircuit_id) {
            // Cache hit == no file I/O needed!
            *cache_stats.entry(subcircuit_id).or_insert(0) += 1;
            cached
        } else {
            // Cache miss - load and cache the subcircuit
            let r1cs_path = PathBuf::from(qap_path).join(format!("json/subcircuit{subcircuit_id}.json")); // TODO: use bincode instead.
            let t = Instant::now();
            let loaded_r1cs = SubcircuitR1CS::from_path(r1cs_path, &setup_params, &subcircuit_infos[subcircuit_id]).unwrap();
            println!("    ğŸ”„ Loading r1cs {} took {:?} (first time)", subcircuit_id, t.elapsed());
            r1cs_cache.insert(subcircuit_id, loaded_r1cs);
            cache_stats.insert(subcircuit_id, 0);
            r1cs_cache.get(&subcircuit_id).unwrap()
        };
        let variables = &placement_variables[i].variables;
        
        let t = Instant::now();
        _from_r1cs_to_eval_cached(
            &variables, 
            &compact_r1cs.A_compact_col_mat, 
            &compact_r1cs.A_active_wires, 
            i, 
            n, 
            &mut u_eval,
            &mut hex_cache,
            &mut hex_cache_hits,
            &mut hex_cache_misses
        );
        _from_r1cs_to_eval_cached(
            &variables, 
            &compact_r1cs.B_compact_col_mat, 
            &compact_r1cs.B_active_wires, 
            i, 
            n, 
            &mut v_eval,
            &mut hex_cache,
            &mut hex_cache_hits,
            &mut hex_cache_misses
        );
        _from_r1cs_to_eval_cached(
            &variables, 
            &compact_r1cs.C_compact_col_mat, 
            &compact_r1cs.C_active_wires, 
            i, 
            n, 
            &mut w_eval,
            &mut hex_cache,
            &mut hex_cache_hits,
            &mut hex_cache_misses
        );
        println!("    âš¡ Processing r1cs A,B,C {} took {:?}", subcircuit_id, t.elapsed());
    }
    println!("ğŸ”„ Loading r1cs took {:?}", time_start.elapsed());
    
    // Report cache statistics
    let total_cache_hits: usize = cache_stats.values().sum();
    let unique_subcircuits = r1cs_cache.len();
    let total_subcircuit_uses = placement_variables.len();
    println!("ğŸ“Š Cache stats: {} unique subcircuits, {} total uses, {} cache hits ({:.1}% hit rate)", 
        unique_subcircuits, total_subcircuit_uses, total_cache_hits, 
        (total_cache_hits as f64 / total_subcircuit_uses.max(1) as f64) * 100.0);
    for (&subcircuit_id, &hits) in cache_stats.iter() {
        if hits > 0 {
            println!("  ğŸ“‹ Subcircuit {} used {} times (cached)", subcircuit_id, hits + 1);
        }
    }
    
    // Report hex parsing cache statistics
    let total_hex_ops = hex_cache_hits + hex_cache_misses;
    if total_hex_ops > 0 {
        println!("ğŸ”¢ Hex cache stats: {} unique values, {} total conversions, {} hits ({:.1}% hit rate)",
            hex_cache.len(), total_hex_ops, hex_cache_hits,
            (hex_cache_hits as f64 / total_hex_ops as f64) * 100.0);
    }

    let time_start = Instant::now();
    transpose_inplace(&mut u_eval, s_max, n);
    transpose_inplace(&mut v_eval, s_max, n);
    transpose_inplace(&mut w_eval, s_max, n);
    println!("ğŸ”„ Transposing r1cs took {:?}", time_start.elapsed());


    return (
        DensePolynomialExt::from_rou_evals(
            HostSlice::from_slice(&u_eval),
            n,
            s_max,
            None,
            None
        ),
        DensePolynomialExt::from_rou_evals(
            HostSlice::from_slice(&v_eval),
            n,
            s_max,
            None,
            None
        ),
        DensePolynomialExt::from_rou_evals(
            HostSlice::from_slice(&w_eval),
            n,
            s_max,
            None,
            None
        )
    )
}

fn _from_r1cs_to_eval(variables: &Box<[String]>, compact_mat: &Vec<ScalarField>, active_wires: &Vec<usize>, i: usize, n: usize, eval: &mut Vec<ScalarField>)  {
    let d_len_A = active_wires.len();
    if d_len_A > 0 {
        let mut d_vec = vec![ScalarField::zero(); d_len_A].into_boxed_slice();
        for (compact_idx, &local_idx) in active_wires.iter().enumerate() {
            d_vec[compact_idx] = ScalarField::from_hex(&variables[local_idx]);
        }
        let mut frag_eval = vec![ScalarField::zero(); n].into_boxed_slice();
        matrix_matrix_mul(&d_vec, compact_mat, 1, d_len_A, n, &mut frag_eval);
        eval[i*n .. (i+1)*n].clone_from_slice(&frag_eval);
    }
}

// with hex caching
fn _from_r1cs_to_eval_cached(
    variables: &Box<[HexString]>, 
    compact_mat: &Vec<ScalarField>, 
    active_wires: &Vec<usize>, 
    i: usize, 
    n: usize, 
    eval: &mut Vec<ScalarField>,
    hex_cache: &mut HashMap<HexString, ScalarField>,
    hex_cache_hits: &mut usize,
    hex_cache_misses: &mut usize
) {
    let d_len_A = active_wires.len();
    if d_len_A > 0 {
        let mut d_vec = Vec::with_capacity(d_len_A);

        for &local_idx in active_wires.iter() {
            let hex_str = &variables[local_idx];
            let scalar_field = if let Some(&cached_value) = hex_cache.get(hex_str) {
                // Cache hit
                *hex_cache_hits += 1;
                cached_value
            } else {
                // Cache miss - parse and cache
                let parsed_value = ScalarField::from_hex(&hex_str.0);
                hex_cache.insert(hex_str.clone(), parsed_value);
                *hex_cache_misses += 1;
                parsed_value
            };
            d_vec.push(scalar_field);
        }
        
        // Direct write to the output slice - avoid temporary allocation
        let eval_slice = &mut eval[i*n .. (i+1)*n];
        matrix_matrix_mul(&d_vec, compact_mat, 1, d_len_A, n, eval_slice);
    }
}

// More generic helper function for any FieldImpl
pub fn any_field_to_hex<T: FieldImpl>(field: &T) -> String {
    let bytes = field.to_bytes_le();
    format!("0x{}", hex_encode(&bytes))
}

// Helper function to encode bytes as hex string
pub fn hex_encode(bytes: &[u8]) -> String {
    bytes.iter()
        .map(|byte| format!("{:02x}", byte))
        .collect::<String>()
}

// Helper function to split a G1 point into part1 (16 bytes) and part2 (32 bytes)
pub fn split_g1(point: &G1serde) -> (String, String, String, String) {
    // Get X coordinate bytes in little-endian and convert to big-endian
    let mut x_bytes = point.0.x.to_bytes_le();
    x_bytes.reverse(); // Convert to big-endian

    // Get Y coordinate bytes in little-endian and convert to big-endian
    let mut y_bytes = point.0.y.to_bytes_le();
    y_bytes.reverse(); // Convert to big-endian

    // For BLS12-381 Fp elements, we have 48 bytes
    // For X: first 16 bytes go to part1, last 32 bytes to part2
    let x_part1 = format!("0x{}", hex_encode(&x_bytes[0..16]));
    let x_part2 = format!("0x{}", hex_encode(&x_bytes[16..48]));

    // For Y: first 16 bytes go to part1, last 32 bytes to part2
    let y_part1 = format!("0x{}", hex_encode(&y_bytes[0..16]));
    let y_part2 = format!("0x{}", hex_encode(&y_bytes[16..48]));

    (x_part1, x_part2, y_part1, y_part2)
}

// Helper function to format scalar field as 256-bit hex
pub fn scalar_to_hex(scalar: &ScalarField) -> String {
    let mut bytes = scalar.to_bytes_le();
    bytes.reverse(); // Convert to big-endian
    // Pad to 32 bytes if necessary
    while bytes.len() < 32 {
        bytes.push(0);
    }
    format!("0x{}", hex_encode(&bytes))
}

#[macro_export]
macro_rules! split_push {
    ($part1: ident, $part2: ident, $( $point:expr ),+ $(,)?) => {
        $(
            {
                let (x_p1, x_p2, y_p1, y_p2) = split_g1($point);
                $part1.push(x_p1);
                $part2.push(x_p2);
                $part1.push(y_p1);
                $part2.push(y_p2);
            }
        )+
    };
}

// Helper function to recover a BaseField from part1 (16 bytes) and part2 (32 bytes)
fn recover_basefield(part1: &String, part2: &String) -> BaseField {
    let mut bytes = [0u8; 48];

    decode_to_slice(
        part1.trim_start_matches("0x"),
        &mut bytes[0..16],
    )
    .expect("Invalid format");

    decode_to_slice(
        part2.trim_start_matches("0x"),
        &mut bytes[16..48],
    )
    .expect("Invalid format");
    bytes.reverse();            // to little Edian

    return BaseField::from_bytes_le(&bytes);
}

pub fn next_point(idx: usize, part1: &Vec<String>, part2: &Vec<String>) -> G1serde {
    let bx = recover_basefield(&part1[idx], &part2[idx]);       
    let by = recover_basefield(&part1[idx + 1], &part2[idx + 1]);

    return G1serde(G1Affine {
        x: bx,
        y: by,
    });
}

#[macro_export]
macro_rules! pop_recover {
    ($idx: ident, $part1: expr, $part2: expr, $( $point: ident),+ $(,)?) => {
        $(
            let $point = next_point($idx, $part1, $part2);
            $idx += 2;
        )+
    };
}



================================================
FILE: packages/backend/libs/src/polynomial_structures/mod.rs
================================================
use icicle_bls12_381::curve::{ScalarField};
use icicle_core::traits::FieldImpl;
use icicle_runtime::memory::HostSlice;
use crate::iotools::{PlacementVariables, Instance, SetupParams, SubcircuitInfo, SubcircuitR1CS};
use crate::bivariate_polynomial::{DensePolynomialExt, BivariatePolynomial};
use crate::vector_operations::{*};

pub struct QAP{
    pub u_j_X: Vec<DensePolynomialExt>,
    pub v_j_X: Vec<DensePolynomialExt>,
    pub w_j_X: Vec<DensePolynomialExt>
}

pub fn from_subcircuit_to_QAP(
    compact_R1CS: &SubcircuitR1CS,
    setup_params: &SetupParams, 
    subcircuit_info: &SubcircuitInfo, 
) -> (Vec<DensePolynomialExt>, Vec<DensePolynomialExt>, Vec<DensePolynomialExt>) {
    let compact_A_mat = &compact_R1CS.A_compact_col_mat;
    let compact_B_mat = &compact_R1CS.B_compact_col_mat;
    let compact_C_mat = &compact_R1CS.C_compact_col_mat;
    let active_wires_A = &compact_R1CS.A_active_wires;
    let active_wires_B = &compact_R1CS.B_active_wires;
    let active_wires_C = &compact_R1CS.C_active_wires;
    let n = setup_params.n;

    // Reconstruct local u,v,w polynomials
    let zero_poly = DensePolynomialExt::zero();
    let mut u_j_X = vec![zero_poly.clone(); subcircuit_info.Nwires];
    let mut v_j_X = vec![zero_poly.clone(); subcircuit_info.Nwires];
    let mut w_j_X = vec![zero_poly.clone(); subcircuit_info.Nwires];

    let mut ordered_active_wires_A: Vec<usize> = active_wires_A.iter().cloned().collect();
    ordered_active_wires_A.sort();
    for (idx_u, &idx_o) in ordered_active_wires_A.iter().enumerate() {
        let u_j_eval_vec = &compact_A_mat[idx_u * n .. (idx_u+1) * n];
        let u_j_eval = HostSlice::from_slice(&u_j_eval_vec);
        let u_j_poly = DensePolynomialExt::from_rou_evals(u_j_eval, n, 1, None, None);
        u_j_X[idx_o] = u_j_poly;
    }
    let mut ordered_active_wires_B: Vec<usize> = active_wires_B.iter().cloned().collect();
    ordered_active_wires_B.sort();
    for (idx_v, &idx_o) in ordered_active_wires_B.iter().enumerate() {
        let v_j_eval_vec = &compact_B_mat[idx_v * n .. (idx_v+1) * n];
        let v_j_eval = HostSlice::from_slice(&v_j_eval_vec);
        let v_j_poly = DensePolynomialExt::from_rou_evals(v_j_eval, n, 1, None, None);
        v_j_X[idx_o] = v_j_poly;
    }
    let mut ordered_active_wires_C: Vec<usize> = active_wires_C.iter().cloned().collect();
    ordered_active_wires_C.sort();
    for (idx_w, &idx_o) in ordered_active_wires_C.iter().enumerate() {
        let w_j_eval_vec = &compact_C_mat[idx_w * n .. (idx_w+1) * n];
        let w_j_eval = HostSlice::from_slice(&w_j_eval_vec);
        let w_j_poly = DensePolynomialExt::from_rou_evals(w_j_eval, n, 1, None, None);
        w_j_X[idx_o] = w_j_poly;
    }

    return (u_j_X, v_j_X, w_j_X)
}

// pub struct GlobalVariables {
//     pub placementId: usize,
//     pub globalIdx: Box<[usize]>,
//     pub variables: Box<[String]>,
//     pub instance: Box<[ScalarField]>,
//     pub interface_wtns: Box<[Box<[ScalarField]>]>,
//     pub interface_wtns: Box<[Box<[ScalarField]>]>,
// }

// impl GlobalVariables {
//     pub fn from_placement_variables(local_var: PlacementVariables) -> io::Result<Self> {
//         let abs_path = env::current_dir()?.join(path);
//         let file = File::open(abs_path)?;
//         let reader = BufReader::new(file);
//         let data = from_reader(reader)?;
//         Ok(data)
//     }

    
// // }

macro_rules! define_gen_qapXY {
    ($func_name:ident, $mat_field:ident, $wires_field:ident) => {
        pub fn $func_name(
            placement_variables: &Box<[PlacementVariables]>,
            compact_library_R1CS: &Vec<SubcircuitR1CS>,
            setup_params: &SetupParams,
        ) -> DensePolynomialExt {
            let s_d = setup_params.s_D;
            let n = setup_params.n;
            let s_max = setup_params.s_max;
            if compact_library_R1CS.len() != s_d {
                panic!("Invalid subcircuit library composition");
            }

            let mut eval = vec![ScalarField::zero(); s_max * n];
            for i in 0..placement_variables.len() {
                let subcircuit_id = placement_variables[i].subcircuitId;
                let compact_mat = &compact_library_R1CS[subcircuit_id].$mat_field;
                let active_wires = &compact_library_R1CS[subcircuit_id].$wires_field;
                let variables = &placement_variables[i].variables;
                let d_len = active_wires.len();
                if d_len > 0 {
                    let mut d_vec = vec![ScalarField::zero(); d_len].into_boxed_slice();
                    for (compact_idx, &local_idx) in active_wires.iter().enumerate() {
                        d_vec[compact_idx] = ScalarField::from_hex(&variables[local_idx]);
                    }
                    let mut frag_eval = vec![ScalarField::zero(); n].into_boxed_slice();
                    matrix_matrix_mul(&d_vec, compact_mat, 1, d_len, n, &mut frag_eval);
                    eval[i*n .. (i+1)*n].clone_from_slice(&frag_eval);
                }
            }

            transpose_inplace(&mut eval, s_max, n);

            DensePolynomialExt::from_rou_evals(
                HostSlice::from_slice(&eval),
                n,
                s_max,
                None,
                None
            )
        }
    };
}

impl Instance {
    // pub fn gen_a_pub_user_X(&self, setup_params: &SetupParams) -> DensePolynomialExt {
    //     let l_user = setup_params.l_user;
    //     let mut public_instance = vec![ScalarField::zero(); l_user];
    //     for i in 0..l_user {
    //         public_instance[i] = ScalarField::from_hex(&self.a_pub_user[i]);
    //     }
    //     return DensePolynomialExt::from_rou_evals(
    //         HostSlice::from_slice(&public_instance),
    //         l_user,
    //         1,
    //         None,
    //         None
    //     )
    // }

    pub fn gen_a_pub_X(&self, setup_params: &SetupParams) -> DensePolynomialExt {
        let l = setup_params.l;
        let l_user = setup_params.l_user;
        let m_block = setup_params.l_block - l_user;
        let m_function = setup_params.l - setup_params.l_block;

        let mut user_instance = vec![ScalarField::zero(); l_user];
        for i in 0..l_user {
            user_instance[i] = ScalarField::from_hex(&self.a_pub_user[i]);
        }

        let mut block_instance = vec![ScalarField::zero(); m_block];
        for i in 0..m_block {
            block_instance[i] = ScalarField::from_hex(&self.a_pub_block[i]);
        }

        let mut function_instance = vec![ScalarField::zero(); m_function];
        for i in 0..m_function {
            function_instance[i] = ScalarField::from_hex(&self.a_pub_function[i]);
        }

        let public_instance = [
            user_instance,
            block_instance,
            function_instance,
        ].concat().into_boxed_slice();

        return DensePolynomialExt::from_rou_evals(
            HostSlice::from_slice(&public_instance),
            l,
            1,
            None,
            None
        )
    }

    // pub fn gen_a_pub_env_X(&self, setup_params: &SetupParams) -> DensePolynomialExt {
    //     let m_block = setup_params.l_block - setup_params.l_user;
    //     let m_function = setup_params.l - setup_params.l_block;
    //     let m_env = m_block + m_function;

    //     let mut block_instance = vec![ScalarField::zero(); m_block];
    //     for i in 0..m_block {
    //         block_instance[i] = ScalarField::from_hex(&self.a_pub_block[i]);
    //     }

    //     let mut function_instance = vec![ScalarField::zero(); m_function];
    //     for i in 0..m_function {
    //         function_instance[i] = ScalarField::from_hex(&self.a_pub_function[i]);
    //     }

    //     let public_instance = [
    //         block_instance,
    //         function_instance,
    //     ].concat().into_boxed_slice();

    //     return DensePolynomialExt::from_rou_evals(
    //         HostSlice::from_slice(&public_instance),
    //         m_env,
    //         1,
    //         None,
    //         None
    //     )
    // }

    // pub fn gen_a_pub_function_X(&self, setup_params: &SetupParams) -> DensePolynomialExt {
    //     let m_function = setup_params.l - setup_params.l_block;
    //     let mut public_instance = vec![ScalarField::zero(); m_function];
    //     for i in 0..m_function {
    //         public_instance[i] = ScalarField::from_hex(&self.a_pub_function[i]);
    //     }
    //     return DensePolynomialExt::from_rou_evals(
    //         HostSlice::from_slice(&public_instance),
    //         m_function,
    //         1,
    //         None,
    //         None
    //     )
    // }
}

pub fn gen_bXY(placement_variables: &Box<[PlacementVariables]>, subcircuit_infos: &Box<[SubcircuitInfo]>, setup_params: &SetupParams) -> DensePolynomialExt {
    let l = setup_params.l;
    let l_d = setup_params.l_D;
    let s_max = setup_params.s_max;
    let m_i = l_d - l;
    let mut interface_witness  = vec![ScalarField::zero(); m_i * s_max].into_boxed_slice();
    for i in 0..placement_variables.len() {
        let local_variables = &placement_variables[i].variables;
        let global_idx_set = &subcircuit_infos[placement_variables[i].subcircuitId].flattenMap;
        if local_variables.len() != global_idx_set.len(){
            panic!("Corrupted placement variables.")
        }
        for j in 0..global_idx_set.len() {
            let global_idx = global_idx_set[j];
            let val_str: &str = &local_variables[j];
            if global_idx >= l && global_idx < l_d && val_str != "0x0" {
                interface_witness[(global_idx-l) * s_max + i] = ScalarField::from_hex(val_str);
            } 
        }
    }
    return DensePolynomialExt::from_rou_evals(
        HostSlice::from_slice(&interface_witness),
        m_i,
        s_max,
        None,
        None
    )
}

define_gen_qapXY!(gen_uXY, A_compact_col_mat, A_active_wires);
define_gen_qapXY!(gen_vXY, B_compact_col_mat, B_active_wires);
define_gen_qapXY!(gen_wXY, C_compact_col_mat, C_active_wires);


================================================
FILE: packages/backend/libs/src/utils/mod.rs
================================================
use icicle_runtime::{self, Device};

/// Returns true if CUDA or METAL GPU is available.
pub fn check_gpu() -> bool {
    let device_cuda = Device::new("CUDA", 0);
    // "METAL" is not working yet.
    let device_metal = Device::new("CUDA", 0);

    icicle_runtime::is_device_available(&device_cuda)
        || icicle_runtime::is_device_available(&device_metal)
}

/// Sets the best available device and returns the selected device name ("CUDA", "METAL", or "CPU").
pub fn check_device() -> &'static str {

    let _ = icicle_runtime::load_backend_from_env_or_default();
    let device_cpu = Device::new("CPU", 0);
    let device_cuda = Device::new("CUDA", 0);
    let device_metal = Device::new("METAL", 0);

    if icicle_runtime::is_device_available(&device_cuda) {
        println!("CUDA is available");
        icicle_runtime::set_device(&device_cuda).expect("Failed to set CUDA device");
        "CUDA"
    } else if icicle_runtime::is_device_available(&device_metal) {
        println!("METAL is available");
        // icicle_runtime::set_device(&device_metal).expect("Failed to set METAL device");
        // "METAL"
        println!( "METAL is not working properly in the ICICLE version 3.8.0, so falling back to CPU only.");
        icicle_runtime::set_device(&device_cpu).expect("Failed to set CPU device");
        "CPU"
    } else {
        println!("GPU is not available, falling back to CPU only");
        icicle_runtime::set_device(&device_cpu).expect("Failed to set CPU device");
        "CPU"
    }
}



================================================
FILE: packages/backend/libs/src/vector_operations/mod.rs
================================================
use super::bivariate_polynomial::{DensePolynomialExt, BivariatePolynomial};
use icicle_core::vec_ops::{VecOps, VecOpsConfig};
use icicle_bls12_381::curve::{ScalarCfg, ScalarField};
use icicle_core::traits::FieldImpl;
use icicle_runtime::memory::{HostSlice, DeviceVec, DeviceSlice, HostOrDeviceSlice};
use std::cell::RefCell;

pub fn gen_evaled_lagrange_bases(val: &ScalarField, size: usize, res: &mut [ScalarField]) {
    let mut val_pows = vec![ScalarField::one(); size];
    for i in 1..size {
        val_pows[i] = val_pows[i-1] * *val;
    }
    let temp_evals = HostSlice::from_slice(&val_pows);
    let temp_poly = DensePolynomialExt::from_rou_evals(temp_evals, size, 1, None, None);
    let cached_val_pows = HostSlice::from_mut_slice(res);
    temp_poly.copy_coeffs(0, cached_val_pows);
}

pub fn point_mul_two_vecs(lhs: &[ScalarField], rhs: &[ScalarField], res: &mut [ScalarField]){
    if lhs.len() != rhs.len() || lhs.len() != res.len() {
        panic!("Mismatch of sizes of vectors to be pointwise-multiplied");
    }
    let mut vec_ops_cfg = VecOpsConfig::default();
    // Optimize for GPU memory coalescing
    vec_ops_cfg.is_a_on_device = false;
    vec_ops_cfg.is_b_on_device = false;
    vec_ops_cfg.is_result_on_device = false;
    
    let lhs_buff = HostSlice::from_slice(lhs);
    let rhs_buff = HostSlice::from_slice(rhs);
    let res_buff = HostSlice::from_mut_slice(res);
    ScalarCfg::mul(lhs_buff, rhs_buff, res_buff, &vec_ops_cfg).unwrap();
}


pub fn point_div_two_vecs(numer: &[ScalarField], denom: &[ScalarField], res: &mut [ScalarField]){
    if numer.len() != denom.len() || numer.len() != res.len() {
        panic!("Mismatch of sizes of vectors to be pointwise-multiplied");
    }
    let vec_ops_cfg = VecOpsConfig::default();
    let lhs_buff = HostSlice::from_slice(numer);
    let rhs_buff = HostSlice::from_slice(denom);
    let res_buff = HostSlice::from_mut_slice(res);
    ScalarCfg::div(lhs_buff, rhs_buff, res_buff, &vec_ops_cfg).unwrap();
}

pub fn point_add_two_vecs(lhs: &[ScalarField], rhs: &[ScalarField], res: &mut [ScalarField]){
    if lhs.len() != rhs.len() || lhs.len() != res.len() {
        panic!("Mismatch of sizes of vectors to be pointwise-multiplied");
    }
    let vec_ops_cfg = VecOpsConfig::default();
    let lhs_buff = HostSlice::from_slice(lhs);
    let rhs_buff = HostSlice::from_slice(rhs);
    let res_buff = HostSlice::from_mut_slice(res);
    ScalarCfg::add(lhs_buff, rhs_buff, res_buff, &vec_ops_cfg).unwrap();
}

pub fn scale_vec(scaler: ScalarField, vec: &[ScalarField], res: &mut [ScalarField]){
    if vec.len() != res.len() {
        panic!("Incorrect output buffer length");
    }
    let vec_ops_cfg = VecOpsConfig::default();
    let lhs_v = vec![scaler];
    let lhs_buff = HostSlice::from_slice(&lhs_v);
    let rhs_buff = HostSlice::from_slice(vec);
    // let scaler = vec![lhs; rhs.len()];
    // point_mul_two_vecs(&scaler, rhs, res);
    let res_buff = HostSlice::from_mut_slice(res);
    ScalarCfg::scalar_mul(lhs_buff, rhs_buff, res_buff, &vec_ops_cfg).unwrap();
}

pub fn scalar_vec_sub(lhs: ScalarField, rhs: &[ScalarField], res: &mut [ScalarField]){
    if rhs.len() != res.len() {
        panic!("Incorrect output buffer length");
    }
    let vec_ops_cfg = VecOpsConfig::default();
    let lhs_v = vec![lhs];
    let lhs_buff = HostSlice::from_slice(&lhs_v);
    let rhs_buff = HostSlice::from_slice(rhs);
    let res_buff = HostSlice::from_mut_slice(res);
    ScalarCfg::scalar_sub(lhs_buff, rhs_buff, res_buff, &vec_ops_cfg).unwrap();
}

pub fn scalar_vec_add(scalar: ScalarField, vec: &[ScalarField], res: &mut [ScalarField]){
    if vec.len() != res.len() {
        panic!("Incorrect output buffer length");
    }
    let vec_ops_cfg = VecOpsConfig::default();
    let lhs_v = vec![scalar];
    let lhs_buff = HostSlice::from_slice(&lhs_v);
    let rhs_buff = HostSlice::from_slice(vec);
    let res_buff = HostSlice::from_mut_slice(res);
    ScalarCfg::scalar_add(lhs_buff, rhs_buff, res_buff, &vec_ops_cfg).unwrap();
}

pub fn inner_product_two_vecs(lhs_vec: &[ScalarField], rhs_vec: &[ScalarField]) -> ScalarField {
    if lhs_vec.len() != rhs_vec.len() {
        panic!("Mismatch of sizes of vectors to be inner-producted");
    }

    let len = lhs_vec.len();
    let vec_ops_cfg = VecOpsConfig::default();
    let mut mul_res_vec = vec![ScalarField::zero(); len];
    let mul_res_buff = HostSlice::from_mut_slice(&mut mul_res_vec);
    ScalarCfg::mul(HostSlice::from_slice(&lhs_vec), HostSlice::from_slice(&rhs_vec), mul_res_buff, &vec_ops_cfg).unwrap();
    let mut res_vec = vec![ScalarField::zero()];
    let res = HostSlice::from_mut_slice(&mut res_vec);
    ScalarCfg::sum(mul_res_buff, res, &vec_ops_cfg).unwrap();
    return res_vec[0]
}
fn _repeat_extend(v: &mut Vec<ScalarField>, n: usize) {
    let original = v.clone();
    for _ in 1..n {
        v.extend(original.iter().cloned());
    }
}
pub fn transpose_inplace (a_vec: &mut [ScalarField], row_size: usize, col_size:usize) {
    if a_vec.len() != row_size * col_size {
        panic!("Error in transpose")
    } 
    if row_size * col_size == 0 {
        return
    }
    let vec_ops_cfg = VecOpsConfig::default();
    let a = HostSlice::from_slice(&a_vec);
    let mut res_vec = vec![ScalarField::zero(); row_size * col_size];
    let res = HostSlice::from_mut_slice(&mut res_vec);
    ScalarCfg::transpose(a, row_size as u32, col_size as u32, res, &vec_ops_cfg).unwrap();
    a_vec.clone_from_slice(&res_vec);
}

pub fn transpose_device_inplace (a_vec: &mut DeviceSlice<ScalarField>, row_size: usize, col_size:usize) {
    if a_vec.len() != row_size * col_size {
        panic!("Error in transpose")
    }
    if row_size * col_size == 0 {
        return
    }
    let mut vec_ops_cfg = VecOpsConfig::default();
    vec_ops_cfg.is_a_on_device = true;
    vec_ops_cfg.is_result_on_device = true;

    let mut res_vec = DeviceVec::device_malloc(row_size * col_size).expect("Failed to allocate device memory");

    ScalarCfg::transpose(a_vec, row_size as u32, col_size as u32, &mut res_vec, &vec_ops_cfg).unwrap();
    a_vec.copy(&res_vec).unwrap();
}

// TODO: benchmark this with a naive approach
fn _repeat_extend_device(v: &DeviceSlice<ScalarField>, n: usize) -> DeviceVec<ScalarField> {
    let original_len = v.len();
    
    if n == 0 {
        return DeviceVec::device_malloc(0).unwrap();
    }

    if n == 1 {
        // Direct copy
        let mut result = DeviceVec::device_malloc(original_len).unwrap();
        result.copy(v).unwrap();
        return result;
    }
    
    let target_len = original_len * n;
    let mut extended_vec = DeviceVec::device_malloc(target_len).unwrap();
    
    // Optimized approach: Use exponential doubling to minimize kernel launches
    // This reduces the number of copy operations from O(n) to O(log n)
    
    // First copy: original vector -> position 0
    extended_vec[0..original_len].copy(v).unwrap();
    
    // Exponential doubling: each iteration doubles the amount of valid data
    let mut current_len = original_len;
    while current_len < target_len {
        let copy_len = std::cmp::min(current_len, target_len - current_len);
        
        // Use a temporary buffer to avoid borrowing issues
        let mut temp_buffer = DeviceVec::device_malloc(copy_len).unwrap();
   